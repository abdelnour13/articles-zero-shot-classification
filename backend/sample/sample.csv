id,abstract
20149,"  Let $n$ and $k$ be natural numbers such that $2^k < n$. We study the
restriction to $\mathfrak{S}_{n-2^k}$ of odd-degree irreducible characters of
the symmetric group $\mathfrak{S}_n$. This analysis completes the study begun
in [Ayyer A., Prasad A., Spallone S., Sem. Lothar. Combin. 75 (2015), Art.
B75g, 13 pages] and recently developed in [Isaacs I.M., Navarro G., Olsson
J.B., Tiep P.H., J. Algebra 478 (2017), 271-282].
"
2099,"  Predicting Arctic sea ice extent is a notoriously difficult forecasting
problem, even for lead times as short as one month. Motivated by Arctic
intraannual variability phenomena such as reemergence of sea surface
temperature and sea ice anomalies, we use a prediction approach for sea ice
anomalies based on analog forecasting. Traditional analog forecasting relies on
identifying a single analog in a historical record, usually by minimizing
Euclidean distance, and forming a forecast from the analog's historical
trajectory. Here an ensemble of analogs are used to make forecasts, where the
ensemble weights are determined by a dynamics-adapted similarity kernel, which
takes into account the nonlinear geometry on the underlying data manifold. We
apply this method for forecasting pan-Arctic and regional sea ice area and
volume anomalies from multi-century climate model data, and in many cases find
improvement over the benchmark damped persistence forecast. Examples of success
include the 3--6 month lead time prediction of pan-Arctic area, the winter sea
ice area prediction of some marginal ice zone seas, and the 3--12 month lead
time prediction of sea ice volume anomalies in many central Arctic basins. We
discuss possible connections between KAF success and sea ice reemergence, and
find KAF to be successful in regions and seasons exhibiting high interannual
variability.
"
7811,"  Network coding based peer-to-peer streaming represents an effective solution
to aggregate user capacities and to increase system throughput in live
multimedia streaming. Nonetheless, such systems are vulnerable to pollution
attacks where a handful of malicious peers can disrupt the communication by
transmitting just a few bogus packets which are then recombined and relayed by
unaware honest nodes, further spreading the pollution over the network. Whereas
previous research focused on malicious nodes identification schemes and
pollution-resilient coding, in this paper we show pollution countermeasures
which make a standard network coding scheme resilient to pollution attacks.
Thanks to a simple yet effective analytical model of a reference node
collecting packets by malicious and honest neighbors, we demonstrate that i)
packets received earlier are less likely to be polluted and ii) short
generations increase the likelihood to recover a clean generation. Therefore,
we propose a recombination scheme where nodes draw packets to be recombined
according to their age in the input queue, paired with a decoding scheme able
to detect the reception of polluted packets early in the decoding process and
short generations. The effectiveness of our approach is experimentally
evaluated in a real system we developed and deployed on hundreds to thousands
peers. Experimental evidence shows that, thanks to our simple countermeasures,
the effect of a pollution attack is almost canceled and the video quality
experienced by the peers is comparable to pre-attack levels.
"
19856,"  The problem of the estimation of relevance to a set of histograms generated
by samples of a discrete time process is discussed on the base of the
variational principles proposed in the previous paper [1]. Some conditions for
dimension reduction of corresponding linear programming problems are presented
also.
"
13057,"  We study the problem of cooperative inference where a group of agents
interact over a network and seek to estimate a joint parameter that best
explains a set of observations. Agents do not know the network topology or the
observations of other agents. We explore a variational interpretation of the
Bayesian posterior density, and its relation to the stochastic mirror descent
algorithm, to propose a new distributed learning algorithm. We show that, under
appropriate assumptions, the beliefs generated by the proposed algorithm
concentrate around the true parameter exponentially fast. We provide explicit
non-asymptotic bounds for the convergence rate. Moreover, we develop explicit
and computationally efficient algorithms for observation models belonging to
exponential families.
"
13016,"  We present absolute frequency measurement of the unperturbed P7 P7 O$_2$
B-band transition with relative standard uncertainty of $2\times10^{-11}$. We
reached the level of accuracy typical for Doppler-free techniques, with
Doppler-limited spectroscopy. The Doppler-limited shapes of the P7 P7 spectral
line were measured with a frequency-stabilized cavity ring-down spectrometer
referenced to an $^{88}$Sr optical atomic clock by an optical frequency comb.
"
7232,"  Recently a certain $q$-Painlevé type system has been obtained from a
reduction of the $q$-Garnier system. In this paper it is shown that the
$q$-Painlevé type system is associated with another realization of the affine
Weyl group symmetry of type $E_7^{(1)}$ and is different from the well-known
$q$-Painlevé system of type $E_7^{(1)}$ from the point of view of evolution
directions. We also study a connection between the $q$-Painlevé type system
and the $q$-Painlevé system of type $E_7^{(1)}$. Furthermore determinant
formulas of particular solutions for the $q$-Painlevé type system are
constructed in terms of the terminating $q$-hypergeometric function.
"
18320,"  We show that any $d$-colored set of points in general position in
$\mathbb{R}^d$ can be partitioned into $n$ subsets with disjoint convex hulls
such that the set of points and all color classes are partitioned as evenly as
possible. This extends results by Holmsen, Kynčl & Valculescu (2017) and
establishes a special case of their general conjecture. Our proof utilizes a
result obtained independently by Soberón and by Karasev in 2010, on
simultaneous equipartitions of $d$ continuous measures in $\mathbb{R}^d$ by $n$
convex regions. This gives a convex partition of $\mathbb{R}^d$ with the
desired properties, except that points may lie on the boundaries of the
regions. In order to resolve the ambiguous assignment of these points, we set
up a network flow problem. The equipartition of the continuous measures gives a
fractional flow. The existence of an integer flow then yields the desired
partition of the point set.
"
16550,"  We prove an inverse theorem for the Gowers $U^2$-norm for maps $G\to\mathcal
M$ from an countable, discrete, amenable group $G$ into a von Neumann algebra
$\mathcal M$ equipped with an ultraweakly lower semi-continuous, unitarily
invariant (semi-)norm $\Vert\cdot\Vert$. We use this result to prove a
stability result for unitary-valued $\varepsilon$-representations $G\to\mathcal
U(\mathcal M)$ with respect to $\Vert\cdot \Vert$.
"
11117,"  The stochastic variance-reduced gradient method (SVRG) and its accelerated
variant (Katyusha) have attracted enormous attention in the machine learning
community in the last few years due to their superior theoretical properties
and empirical behaviour on training supervised machine learning models via the
empirical risk minimization paradigm. A key structural element in both of these
methods is the inclusion of an outer loop at the beginning of which a full pass
over the training data is made in order to compute the exact gradient, which is
then used to construct a variance-reduced estimator of the gradient. In this
work we design {\em loopless variants} of both of these methods. In particular,
we remove the outer loop and replace its function by a coin flip performed in
each iteration designed to trigger, with a small probability, the computation
of the gradient. We prove that the new methods enjoy the same superior
theoretical convergence properties as the original methods. However, we
demonstrate through numerical experiments that our methods have substantially
superior practical behavior.
"
12417,"  It is argued that many of the problems and ambiguities of standard cosmology
derive from a single one: violation of conservation of energy in the standard
paradigm. Standard cosmology satisfies conservation of local energy, however
disregards the inherent global aspect of energy. We therefore explore
conservation of the quasi-local Misner-Sharp energy within the causal horizon,
which, as we argue, is necessarily an apparent horizon. Misner-Sharp energy
assumes the presence of arbitrary mass-energy. Its conservation, however,
yields ""empty"" de Sitter (open, flat, closed) as single cosmological solution,
where Misner-Sharp total energy acts as cosmological constant and where the
source of curvature energy is unidentified. It is argued that de Sitter is only
apparently empty of matter. That is, total matter energy scales as curvature
energy in open de Sitter, which causes evolution of the cosmic potential and
induces gravitational time dilation. Curvature of time accounts completely for
the extrinsic curvature, i.e., renders open de Sitter spatially flat. This
explains the well known, surprising, spatial flatness of Misner-Sharp energy,
even if extrinsic curvature is non-zero. The general relativistic derivation
from Misner-Sharp energy is confirmed by a Machian equation of recessional and
peculiar energy, which explicitly assumes the presence of matter. This
relational model enhances interpretation. Time-dilated open de Sitter is
spatially flat, dynamically close to $\Lambda$CDM, and is shown to be without
the conceptual problems of concordance cosmology.
"
18845,"  Measuring domain relevance of data and identifying or selecting well-fit
domain data for machine translation (MT) is a well-studied topic, but denoising
is not yet. Denoising is concerned with a different type of data quality and
tries to reduce the negative impact of data noise on MT training, in
particular, neural MT (NMT) training. This paper generalizes methods for
measuring and selecting data for domain MT and applies them to denoising NMT
training. The proposed approach uses trusted data and a denoising curriculum
realized by online data selection. Intrinsic and extrinsic evaluations of the
approach show its significant effectiveness for NMT to train on data with
severe noise.
"
9883,"  We derive the Markov process equivalent to She-Leveque scaling in homogeneous
and isotropic turbulence. The Markov process is a jump process for velocity
increments $u(r)$ in scale $r$ in which the jumps occur randomly but with
deterministic width in $u$. From its master equation we establish a
prescription to simulate the She-Leveque process and compare it with Kolmogorov
scaling. To put the She-Leveque process into the context of other established
turbulence models on the Markov level, we derive a diffusion process for $u(r)$
from two properties of the Navier-Stokes equation. This diffusion process
already includes Kolmogorov scaling, extended self-similarity and a class of
random cascade models. The fluctuation theorem of this Markov process implies a
""second law"" that puts a loose bound on the multipliers of the random cascade
models. This bound explicitly allows for inverse cascades, which are necessary
to satisfy the fluctuation theorem. By adding a jump process to the diffusion
process, we go beyond Kolmogorov scaling and formulate the most general scaling
law for the class of Markov processes having both diffusion and jump parts.
This Markov scaling law includes She-Leveque scaling and a scaling law derived
by Yakhot.
"
10737,"  Formal verification techniques are widely used for detecting design flaws in
software systems. Formal verification can be done by transforming an already
implemented source code to a formal model and attempting to prove certain
properties of the model (e.g. that no erroneous state can occur during
execution). Unfortunately, transformations from source code to a formal model
often yield large and complex models, making the verification process
inefficient and costly. In order to reduce the size of the resulting model,
optimization transformations can be used. Such optimizations include common
algorithms known from compiler design and different program slicing techniques.
Our paper describes a framework for transforming C programs to a formal model,
enhanced by various optimizations for size reduction. We evaluate and compare
several optimization algorithms regarding their effect on the size of the model
and the efficiency of the verification. Results show that different
optimizations are more suitable for certain models, justifying the need for a
framework that includes several algorithms.
"
14217,"  For $d\geq1$, we study the simplicial structure of the chain complex
associated to the higher order Hochschild homology over the $d$-sphere. We
discuss $H_\bullet^{S^d}(A,M)$ by way of a bar-like resolution
$\mathcal{B}^d(A)$ in the context of simplicial modules. Besides the general
case, we give explicit detail corresponding to $S^3$. We also present a
description of what can replace these bar-like resolutions in order to aid with
computation. The cohomology version can be done following a similar
construction, of which we make mention.
"
2611,"  We propose new smoothed median and the Wilcoxon's rank sum test. As is
pointed out by Maesono et al.(2016), some nonparametric discrete tests have a
problem with their significance probability. Because of this problem, the
selection of the median and the Wilcoxon's test can be biased too, however, we
show new smoothed tests are free from the problem. Significance probabilities
and local asymptotic powers of the new tests are studied, and we show that they
inherit good properties of the discrete tests.
"
2257,"  Many applied settings in empirical economics involve simultaneous estimation
of a large number of parameters. In particular, applied economists are often
interested in estimating the effects of many-valued treatments (like teacher
effects or location effects), treatment effects for many groups, and prediction
models with many regressors. In these settings, machine learning methods that
combine regularized estimation and data-driven choices of regularization
parameters are useful to avoid over-fitting. In this article, we analyze the
performance of a class of machine learning estimators that includes ridge,
lasso and pretest in contexts that require simultaneous estimation of many
parameters. Our analysis aims to provide guidance to applied researchers on (i)
the choice between regularized estimators in practice and (ii) data-driven
selection of regularization parameters. To address (i), we characterize the
risk (mean squared error) of regularized estimators and derive their relative
performance as a function of simple features of the data generating process. To
address (ii), we show that data-driven choices of regularization parameters,
based on Stein's unbiased risk estimate or on cross-validation, yield
estimators with risk uniformly close to the risk attained under the optimal
(unfeasible) choice of regularization parameters. We use data from recent
examples in the empirical economics literature to illustrate the practical
applicability of our results.
"
10643,"  We introduce a new shape-constrained class of distribution functions on R,
the bi-$s^*$-concave class. In parallel to results of Dümbgen, Kolesnyk, and
Wilke (2017) for what they called the class of bi-log-concave distribution
functions, we show that every s-concave density f has a bi-$s^*$-concave
distribution function $F$ and that every bi-$s^*$-concave distribution function
satisfies $\gamma (F) \le 1/(1+s)$ where finiteness of $$ \gamma (F) \equiv
\sup_{x} F(x) (1-F(x)) \frac{| f' (x)|}{f^2 (x)}, $$ the Csörgő -
Révész constant of F, plays an important role in the theory of quantile
processes on $R$.
"
1045,"  We present the calibrated-projection MATLAB package implementing the method
to construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).
This manual provides details on how to use the package for inference on
projections of partially identified parameters. It also explains how to use the
MATLAB functions we developed to compute confidence intervals on solutions of
nonlinear optimization problems with estimated constraints.
"
6149,"  We establish a new connection between value and policy based reinforcement
learning (RL) based on a relationship between softmax temporal value
consistency and policy optimality under entropy regularization. Specifically,
we show that softmax consistent action values correspond to optimal entropy
regularized policy probabilities along any action sequence, regardless of
provenance. From this observation, we develop a new RL algorithm, Path
Consistency Learning (PCL), that minimizes a notion of soft consistency error
along multi-step action sequences extracted from both on- and off-policy
traces. We examine the behavior of PCL in different scenarios and show that PCL
can be interpreted as generalizing both actor-critic and Q-learning algorithms.
We subsequently deepen the relationship by showing how a single model can be
used to represent both a policy and the corresponding softmax state values,
eliminating the need for a separate critic. The experimental evaluation
demonstrates that PCL significantly outperforms strong actor-critic and
Q-learning baselines across several benchmarks.
"
6917,"  In this paper, we derive a Bayesian model order selection rule by using the
exponentially embedded family method, termed Bayesian EEF. Unlike many other
Bayesian model selection methods, the Bayesian EEF can use vague proper priors
and improper noninformative priors to be objective in the elicitation of
parameter priors. Moreover, the penalty term of the rule is shown to be the sum
of half of the parameter dimension and the estimated mutual information between
parameter and observed data. This helps to reveal the EEF mechanism in
selecting model orders and may provide new insights into the open problems of
choosing an optimal penalty term for model order selection and choosing a good
prior from information theoretic viewpoints. The important example of linear
model order selection is given to illustrate the algorithms and arguments.
Lastly, the Bayesian EEF that uses Jeffreys prior coincides with the EEF rule
derived by frequentist strategies. This shows another interesting relationship
between the frequentist and Bayesian philosophies for model selection.
"
7712,"  We give a new bound on the number of collinear triples for two arbitrary
subsets of a finite field. This improves on existing results which rely on the
Cauchy inequality. We then us this to provide a new bound on trilinear and
quadrilinear exponential sums.
"
14847,"  We study radiative neutrino pair emission in deexcitation process of atoms
taking into account coherence effect in a macroscopic target system. In the
course of preparing the coherent initial state to enhance the rate, a spatial
phase factor is imprinted in the macroscopic target. It is shown that this
initial spatial phase changes the kinematics of the radiative neutrino pair
emission. We investigate effects of the initial spatial phase in the photon
spectrum of the process. It turns out that the initial spatial phase provides
us significant improvements in exploring neutrino physics such as the
Dirac-Majorana distinction and the cosmic neutrino background.
"
3362,"  We study the indices of the geodesic central configurations on $\H^2$. We
then show that central configurations are bounded away from the singularity
set. With Morse's inequality, we get a lower bound for the number of central
configurations on $\H^2$.
"
16928,"  The abundance of metals in galaxies is a key parameter which permits to
distinguish between different galaxy formation and evolution models. Most of
the metallicity determinations are based on optical line ratios. However, the
optical spectral range is subject to dust extinction and, for high-z objects (z
> 3), some of the lines used in optical metallicity diagnostics are shifted to
wavelengths not accessible to ground based observatories. For this reason, we
explore metallicity diagnostics using far-infrared (IR) line ratios which can
provide a suitable alternative in such situations. To investigate these far-IR
line ratios, we modeled the emission of a starburst with the photoionization
code CLOUDY. The most sensitive far-IR ratios to measure metallicities are the
[OIII]52$\mu$m and 88$\mu$m to [NIII]57$\mu$m ratios. We show that this ratio
produces robust metallicities in the presence of an AGN and is insensitive to
changes in the age of the ionizing stellar. Another metallicity sensitive ratio
is the [OIII]88$\mu$m/[NII]122$\mu$m ratio, although it depends on the
ionization parameter. We propose various mid- and far-IR line ratios to break
this dependency. Finally, we apply these far-IR diagnostics to a sample of 19
local ultraluminous IR galaxies (ULIRGs) observed with Herschel and Spitzer. We
find that the gas-phase metallicity in these local ULIRGs is in the range 0.7 <
Z_gas/Z_sun < 1.5, which corresponds to 8.5 < 12 + log (O/H) < 8.9. The
inferred metallicities agree well with previous estimates for local ULIRGs and
this confirms that they lie below the local mass-metallicity relation.
"
16261,"  Topological metrics of graphs provide a natural way to describe the prominent
features of various types of networks. Graph metrics describe the structure and
interplay of graph edges and have found applications in many scientific fields.
In this work, graph metrics are used in network estimation by developing
optimisation methods that incorporate prior knowledge of a network's topology.
The derivatives of graph metrics are used in gradient descent schemes for
weighted undirected network denoising, network completion, and network
decomposition. The successful performance of our methodology is shown in a
number of toy examples and real-world datasets. Most notably, our work
establishes a new link between graph theory, network science and optimisation.
"
16160,"  We classify certain subcategories in quotients of exact categories. In
particular, we classify the triangulated and thick subcategories of an
algebraic triangulated category, i.e. the stable category of a Frobenius
category.
"
6977,"  In this article we develop a new sequential Monte Carlo (SMC) method for
multilevel (ML) Monte Carlo estimation. In particular, the method can be used
to estimate expectations with respect to a target probability distribution over
an infinite-dimensional and non-compact space as given, for example, by a
Bayesian inverse problem with Gaussian random field prior. Under suitable
assumptions the MLSMC method has the optimal $O(\epsilon^{-2})$ bound on the
cost to obtain a mean-square error of $O(\epsilon^2)$. The algorithm is
accelerated by dimension-independent likelihood-informed (DILI) proposals
designed for Gaussian priors, leveraging a novel variation which uses empirical
sample covariance information in lieu of Hessian information, hence eliminating
the requirement for gradient evaluations. The efficiency of the algorithm is
illustrated on two examples: inversion of noisy pressure measurements in a PDE
model of Darcy flow to recover the posterior distribution of the permeability
field, and inversion of noisy measurements of the solution of an SDE to recover
the posterior path measure.
"
13197,"  The goal of the paper is to investigate the dynamics of the eigenvalues of
the Sturm-Liouville operator with summable PT-symmetric potential on the finite
interval. It turns out that the case of a complex Airy operator presents an
exactly solvable model which allows us to trace the dynamics of the movement of
the eigenvalues in all details and to find explicitly the critical parameter
values, in particular, to specify precisely the number $\varepsilon_1$ such
that for $0<\varepsilon<\varepsilon_1$ the operator has a real spectrum and is
similar to a self-adjoint operator.
"
14903,"  We propose a multi-layer approach to simulate hyperpycnal and hypopycnal
plumes in flows with free surface. The model allows to compute the vertical
profile of the horizontal and the vertical components of the velocity of the
fluid flow. The model can describe as well the vertical profile of the sediment
concentration and the velocity components of each one of the sediment species
that form the turbidity current. To do so, it takes into account the settling
velocity of the particles and their interaction with the fluid. This allows to
better describe the phenomena than a single layer approach. It is in better
agreement with the physics of the problem and gives promising results. The
numerical simulation is carried out by rewriting the multi-layer approach in a
compact formulation, which corresponds to a system with non-conservative
products, and using path-conservative numerical scheme. Numerical results are
presented in order to show the potential of the model.
"
14090,"  In this short paper we generalise a theorem due to Kani and Rosen on
decomposition of Jacobian varieties of Riemann surfaces with group action. This
generalisation extends the set of Jacobians for which it is possible to obtain
an isogeny decomposition where all the factors are Jacobians.
"
8768,"  We determine which of the modular curves $X_\Delta(N)$, that is, curves lying
between $X_0(N)$ and $X_1(N)$, are bielliptic. Somewhat surprisingly, we find
that one of these curves has exceptional automorphisms. Finally we find all
$X_\Delta(N)$ that have infinitely many quadratic points over $\mathbb{Q}$.
"
16606,"  Magnetic domain wall (DW) motion induced by a localized Gaussian temperature
profile is studied in a Permalloy nanostrip within the framework of the
stochastic Landau-Lifshitz-Bloch equation. The different contributions to
thermally induced DW motion, entropic torque and magnonic spin transfer torque,
are isolated and compared. The analysis of magnonic spin transfer torque
includes a description of thermally excited magnons in the sample. A third
driving force due to a thermally induced dipolar field is found and described.
Finally, thermally induced DW motion is studied under realistic conditions by
taking into account the edge roughness. The results give quantitative insights
into the different mechanisms responsible for domain wall motion in temperature
gradients and allow for comparison with experimental results.
"
9013,"  In two recent publications ( Int. J. Quant. Chem. 114, 1645 (2014) and Molec.
Phys. 114, 227 (2016)) it was shown that the Born -Hwang (BH) treatment of a
molecular system perturbed by an external field yields a set of decoupled
vectorial Wave Equations, just like in Electromagnetism. This finding led us to
declare on the existence of a new type of Fields, which were termed Molecular
Fields. The fact that such fields exist implies that at the vicinity of conical
intersections exist a mechanism that transforms a passing-by electric beam into
a field which differs from the original electric field. This situation is
reminiscent of what is encountered in astronomy where Black Holes formed by
massive stars may affect the nature of a near-by beam of light. Thus if the
NonAdiabatic-Coupling-Terms (NACT) with their singular points may affect the
nature of such a beam (see the above two publications) then it would be
interesting to know to what extend NACTs (and consequently also the BH
equation) will be affected by the special theory of relativity as introduced by
Dirac. Indeed while applying the Dirac approach we derived the relativistic
affected NACTs as well as the corresponding BH equation.
"
5941,"  Microorganisms, such as bacteria, are one of the first targets of
nanoparticles in the environment. In this study, we tested the effect of two
nanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the
Gram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based
proteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no
detectable effect on the proteomic pattern, while ZnO NPs and ZnSO4
significantly modified B. subtilis metabolism. These results allowed us to
conclude that the effects of ZnO observed in this work were mainly attributable
to Zn dissolution in the culture media. Proteomic analysis highlighted twelve
modulated proteins related to central metabolism: MetE and MccB (cysteine
metabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,
YqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free
sulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed
central metabolism reorientation and showed that Zn stress induced oxidative
stress, probably as a consequence of thiol chelation stress by Zn ions. The
other patterns affected by ZnO and ZnSO4 were the stringent response and the
general stress response. Nine proteins involved in or controlled by the
stringent response showed a modified expression profile in the presence of ZnO
NPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An
increase in the ppGpp concentration confirmed the involvement of the stringent
response during a Zn stress. All these metabolic reorientations in response to
Zn stress were probably the result of complex regulatory mechanisms including
at least the stringent response via YwaC.
"
15548,"  In recent years, deep learning algorithms have become increasingly more
prominent for their unparalleled ability to automatically learn discriminant
features from large amounts of data. However, within the field of
electromyography-based gesture recognition, deep learning algorithms are seldom
employed as they require an unreasonable amount of effort from a single person,
to generate tens of thousands of examples.
This work's hypothesis is that general, informative features can be learned
from the large amounts of data generated by aggregating the signals of multiple
users, thus reducing the recording burden while enhancing gesture recognition.
Consequently, this paper proposes applying transfer learning on aggregated data
from multiple users, while leveraging the capacity of deep learning algorithms
to learn discriminant features from large datasets. Two datasets comprised of
19 and 17 able-bodied participants respectively (the first one is employed for
pre-training) were recorded for this work, using the Myo Armband. A third Myo
Armband dataset was taken from the NinaPro database and is comprised of 10
able-bodied participants. Three different deep learning networks employing
three different modalities as input (raw EMG, Spectrograms and Continuous
Wavelet Transform (CWT)) are tested on the second and third dataset. The
proposed transfer learning scheme is shown to systematically and significantly
enhance the performance for all three networks on the two datasets, achieving
an offline accuracy of 98.31% for 7 gestures over 17 participants for the
CWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw
EMG-based ConvNet. Finally, a use-case study employing eight able-bodied
participants suggests that real-time feedback allows users to adapt their
muscle activation strategy which reduces the degradation in accuracy normally
experienced over time.
"
19080,"  We consider submanifolds into Riemannian manifold with metallic structures.
We obtain some new results for hypersurfaces in these spaces and we express the
fundamental theorem of submanifolds into products spaces in terms of metallic
structures. Moreover, we define new structures called complex metallic
structures. We show that these structures are linked with complex structures.
Then, we consider submanifolds into Riemannian manifold with such structures
with a focus on invariant submanifolds and hypersurfaces. We also express in
particular the fundamental theorem of submanifolds of complex space form in
terms of complex metallic structures.
"
20454,"  Recent experiments show no statistical impact of seal length on the
performance of long elastomeric seals in relatively smooth test fixtures.
Motivated by these results, we analytically and computationally investigate the
combined effects of seal length and compressibility on the maximum differential
pressure a seal can support. We present a Saint-Venant type analytic shear lag
solution for slightly compressible seals with large aspect ratios, which
compares well with nonlinear finite element simulations in regions far from the
ends of the seal. However, at the high- and low-pressure ends, where fracture
is observed experimentally, the analytic solution is in poor agreement with
detailed finite element calculations. Nevertheless, we show that the analytic
solution provides far-field stress measures that correlate, over a range of
aspect ratios and bulk moduli, the calculated energy release rates for the
growth of small cracks at the two ends of the seal. Thus a single finite
element simulation coupled with the analytic solution can be used to determine
tendencies for fracture at the two ends of the seal over a wide range of
geometry and compressibility. Finally, using a hypothetical critical energy
release rate, predictions for whether a crack on the high-pressure end will
begin to grow before or after a crack on the low-pressure end begins to grow
are made using the analytic solution and compared with finite element
simulations for finite deformation, hyperelastic seals.
"
15869,"  Machine learning algorithms are typically run on large scale, distributed
compute infrastructure that routinely face a number of unavailabilities such as
failures and temporary slowdowns. Adding redundant computations using
coding-theoretic tools called ""codes"" is an emerging technique to alleviate the
adverse effects of such unavailabilities. A code consists of an encoding
function that proactively introduces redundant computation and a decoding
function that reconstructs unavailable outputs using the available ones. Past
work focuses on using codes to provide resilience for linear computations and
specific iterative optimization algorithms. However, computations performed for
a variety of applications including inference on state-of-the-art machine
learning algorithms, such as neural networks, typically fall outside this
realm. In this paper, we propose taking a learning-based approach to designing
codes that can handle non-linear computations. We present carefully designed
neural network architectures and a training methodology for learning encoding
and decoding functions that produce approximate reconstructions of unavailable
computation results. We present extensive experimental results demonstrating
the effectiveness of the proposed approach: we show that the our learned codes
can accurately reconstruct $64 - 98\%$ of the unavailable predictions from
neural-network based image classifiers on the MNIST, Fashion-MNIST, and
CIFAR-10 datasets. To the best of our knowledge, this work proposes the first
learning-based approach for designing codes, and also presents the first
coding-theoretic solution that can provide resilience for any non-linear
(differentiable) computation. Our results show that learning can be an
effective technique for designing codes, and that learned codes are a highly
promising approach for bringing the benefits of coding to non-linear
computations.
"
20371,"  We present and implement a non-destructive detection scheme for the
transition probability readout of an optical lattice clock. The scheme relies
on a differential heterodyne measurement of the dispersive properties of
lattice-trapped atoms enhanced by a high finesse cavity. By design, this scheme
offers a 1st order rejection of the technical noise sources, an enhanced
signal-to-noise ratio, and an homogeneous atom-cavity coupling. We
theoretically show that this scheme is optimal with respect to the photon shot
noise limit. We experimentally realize this detection scheme in an operational
strontium optical lattice clock. The resolution is on the order of a few atoms
with a photon scattering rate low enough to keep the atoms trapped after
detection. This scheme opens the door to various different interrogations
protocols, which reduce the frequency instability, including atom recycling,
zero-dead time clocks with a fast repetition rate, and sub quantum projection
noise frequency stability.
"
11415,"  Chemotaxis is a ubiquitous biological phenomenon in which cells detect a
spatial gradient of chemoattractant, and then move towards the source. Here we
present a position-dependent advection-diffusion model that quantitatively
describes the statistical features of the chemotactic motion of the social
amoeba {\it Dictyostelium discoideum} in a linear gradient of cAMP (cyclic
adenosine monophosphate). We fit the model to experimental trajectories that
are recorded in a microfluidic setup with stationary cAMP gradients and extract
the diffusion and drift coefficients in the gradient direction. Our analysis
shows that for the majority of gradients, both coefficients decrease in time
and become negative as the cells crawl up the gradient. The extracted model
parameters also show that besides the expected drift in the direction of
chemoattractant gradient, we observe a nonlinear dependency of the
corresponding variance in time, which can be explained by the model.
Furthermore, the results of the model show that the non-linear term in the mean
squared displacement of the cell trajectories can dominate the linear term on
large time scales.
"
7847,"  We propose expected policy gradients (EPG), which unify stochastic policy
gradients (SPG) and deterministic policy gradients (DPG) for reinforcement
learning. Inspired by expected sarsa, EPG integrates across the action when
estimating the gradient, instead of relying only on the action in the sampled
trajectory. We establish a new general policy gradient theorem, of which the
stochastic and deterministic policy gradient theorems are special cases. We
also prove that EPG reduces the variance of the gradient estimates without
requiring deterministic policies and, for the Gaussian case, with no
computational overhead. Finally, we show that it is optimal in a certain sense
to explore with a Gaussian policy such that the covariance is proportional to
the exponential of the scaled Hessian of the critic with respect to the
actions. We present empirical results confirming that this new form of
exploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic
in four challenging MuJoCo domains.
"
9952,"  We study the time evolution of a one-dimensional interacting fermion system
described by the Luttinger model starting from a nonequilibrium state defined
by a smooth temperature profile $T(x)$. As a specific example we consider the
case when $T(x)$ is equal to $T_L$ ($T_R$) far to the left (right). Using a
series expansion in $\epsilon = 2(T_{R} - T_{L})/(T_{L}+T_{R})$, we compute the
energy density, the heat current density, and the fermion two-point correlation
function for all times $t \geq 0$. For local (delta-function) interactions, the
first two are computed to all orders, giving simple exact expressions involving
the Schwarzian derivative of the integral of $T(x)$. For nonlocal interactions,
breaking scale invariance, we compute the nonequilibrium steady state (NESS) to
all orders and the evolution to first order in $\epsilon$. The heat current in
the NESS is universal even when conformal invariance is broken by the
interactions, and its dependence on $T_{L,R}$ agrees with numerical results for
the $XXZ$ spin chain. Moreover, our analytical formulas predict peaks at short
times in the transition region between different temperatures and show
dispersion effects that, even if nonuniversal, are qualitatively similar to
ones observed in numerical simulations for related models, such as spin chains
and interacting lattice fermions.
"
16006,"  Mobile computing is one of the main drivers of innovation, yet the future
growth of mobile computing capabilities remains critically threatened by
hardware constraints, such as the already extremely dense transistor packing
and limited battery capacity. The breakdown of Dennard scaling and stagnating
energy storage improvements further amplify these threats. However, the
computational burden we put on our mobile devices is not always justified. In a
myriad of situations the result of a computation is further manipulated,
interpreted, and finally acted upon. This allows for the computation to be
relaxed, so that the result is calculated with ""good enough"", not perfect
accuracy. For example, results of a Web search may be perfectly acceptable even
if the order of the last few listed items is shuffled, as an end user decides
which of the available links to follow. Similarly, the quality of a
voice-over-IP call may be acceptable, despite being imperfect, as long as the
two involved parties can clearly understand each other. This novel way of
thinking about computation is termed Approximate Computing (AC) and promises to
reduce resource usage, while ensuring that satisfactory performance is
delivered to end-users. AC is already experimented with on various levels of
desktop computer architecture, from the hardware level where incorrect adders
have been designed to sacrifice result correctness for reduced energy
consumption, to compiler-level optimisations that omit certain lines of code to
speed up video encoding. AC is yet to be attempted on mobile devices and in
this article we examine the potential benefits of mobile AC and present an
overview of AC techniques applicable in the mobile domain.
"
16812,"  Automatic Music Transcription (AMT) is one of the oldest and most
well-studied problems in the field of music information retrieval. Within this
challenging research field, onset detection and instrument recognition take
important places in transcription systems, as they respectively help to
determine exact onset times of notes and to recognize the corresponding
instrument sources. The aim of this study is to explore the usefulness of
multiscale scattering operators for these two tasks on plucked string
instrument and piano music. After resuming the theoretical background and
illustrating the key features of this sound representation method, we evaluate
its performances comparatively to other classical sound representations. Using
both MIDI-driven datasets with real instrument samples and real musical pieces,
scattering is proved to outperform other sound representations for these AMT
subtasks, putting forward its richer sound representation and invariance
properties.
"
2006,"  Artifical Neural Networks are a particular class of learning systems modeled
after biological neural functions with an interesting penchant for Hebbian
learning, that is ""neurons that wire together, fire together"". However, unlike
their natural counterparts, artificial neural networks have a close and
stringent coupling between the modules of neurons in the network. This coupling
or locking imposes upon the network a strict and inflexible structure that
prevent layers in the network from updating their weights until a full
feed-forward and backward pass has occurred. Such a constraint though may have
sufficed for a while, is now no longer feasible in the era of very-large-scale
machine learning, coupled with the increased desire for parallelization of the
learning process across multiple computing infrastructures. To solve this
problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are
introduced as a viable alternative to the backpropagation algorithm. This paper
performs a speed benchmark to compare the speed and accuracy capabilities of
SG-DNI as opposed to a standard neural interface using multilayer perceptron
MLP. SG-DNI shows good promise, in that it not only captures the learning
problem, it is also over 3-fold faster due to it asynchronous learning
capabilities.
"
4819,"  In this paper, we study a slant submanifold of a complex space form. We also
obtain an integral formula of Simons' type for a Kaehlerian slant submanifold
in a complex space form and apply it to prove our main result.
"
12345,"  Geosciences is a field of great societal relevance that requires solutions to
several urgent problems facing our humanity and the planet. As geosciences
enters the era of big data, machine learning (ML) -- that has been widely
successful in commercial domains -- offers immense potential to contribute to
problems in geosciences. However, problems in geosciences have several unique
challenges that are seldom found in traditional applications, requiring novel
problem formulations and methodologies in machine learning. This article
introduces researchers in the machine learning (ML) community to these
challenges offered by geoscience problems and the opportunities that exist for
advancing both machine learning and geosciences. We first highlight typical
sources of geoscience data and describe their properties that make it
challenging to use traditional machine learning techniques. We then describe
some of the common categories of geoscience problems where machine learning can
play a role, and discuss some of the existing efforts and promising directions
for methodological development in machine learning. We conclude by discussing
some of the emerging research themes in machine learning that are applicable
across all problems in the geosciences, and the importance of a deep
collaboration between machine learning and geosciences for synergistic
advancements in both disciplines.
"
20213,"  Ongoing innovations in recurrent neural network architectures have provided a
steady influx of apparently state-of-the-art results on language modelling
benchmarks. However, these have been evaluated using differing code bases and
limited computational resources, which represent uncontrolled sources of
experimental variation. We reevaluate several popular architectures and
regularisation methods with large-scale automatic black-box hyperparameter
tuning and arrive at the somewhat surprising conclusion that standard LSTM
architectures, when properly regularised, outperform more recent models. We
establish a new state of the art on the Penn Treebank and Wikitext-2 corpora,
as well as strong baselines on the Hutter Prize dataset.
"
649,"  We offer a generalization of a formula of Popov involving the Von Mangoldt
function. Some commentary on its relation to other results in analytic number
theory is mentioned as well as an analogue involving the m$\ddot{o}$bius
function.
"
7261,"  An extremal point of a positive threshold Boolean function $f$ is either a
maximal zero or a minimal one. It is known that if $f$ depends on all its
variables, then the set of its extremal points completely specifies $f$ within
the universe of threshold functions. However, in some cases, $f$ can be
specified by a smaller set. The minimum number of points in such a set is the
specification number of $f$. It was shown in [S.-T. Hu. Threshold Logic, 1965]
that the specification number of a threshold function of $n$ variables is at
least $n+1$. In [M. Anthony, G. Brightwell, and J. Shawe-Taylor. On specifying
Boolean functions by labelled examples. Discrete Applied Mathematics, 1995] it
was proved that this bound is attained for nested functions and conjectured
that for all other threshold functions the specification number is strictly
greater than $n+1$. In the present paper, we resolve this conjecture negatively
by exhibiting threshold Boolean functions of $n$ variables, which are
non-nested and for which the specification number is $n+1$. On the other hand,
we show that the set of extremal points satisfies the statement of the
conjecture, i.e., a positive threshold Boolean function depending on all its
$n$ variables has $n+1$ extremal points if and only if it is nested. To prove
this, we reveal an underlying structure of the set of extremal points.
"
8772,"  Let $G$ be a finite group and let $c(G)$ be the number of cyclic subgroups of
$G$. We study the function $\alpha(G) = c(G)/|G|$. We explore its basic
properties and we point out a connection with the probability of commutation.
For many families $\mathscr{F}$ of groups we characterize the groups $G \in
\mathscr{F}$ for which $\alpha(G)$ is maximal and we classify the groups $G$
for which $\alpha(G) > 3/4$. We also study the number of cyclic subgroups of a
direct power of a given group deducing an asymptotic result and we characterize
the equality $\alpha(G) = \alpha(G/N)$ when $G/N$ is a symmetric group.
"
11318,"  A semiorder is a model of preference relations where each element $x$ is
associated with a utility value $\alpha(x)$, and there is a threshold $t$ such
that $y$ is preferred to $x$ iff $\alpha(y) > \alpha(x)+t$. These are motivated
by the notion that there is some uncertainty in the utility values we assign an
object or that a subject may be unable to distinguish a preference between
objects whose values are close. However, they fail to model the well-known
phenomenon that preferences are not always transitive. Also, if we are
uncertain of the utility values, it is not logical that preference is
determined absolutely by a comparison of them with an exact threshold. We
propose a new model in which there are two thresholds, $t_1$ and $t_2$; if the
difference $\alpha(y) - \alpha(x)$ less than $t_1$, then $y$ is not preferred
to $x$; if the difference is greater than $t_2$ then $y$ is preferred to $x$;
if it is between $t_1$ and $t_2$, then then $y$ may or may not be preferred to
$x$. We call such a relation a double-threshold semiorder, and the
corresponding directed graph $G = (V,E)$ a double threshold digraph. Every
directed acyclic graph is a double threshold graph; bounds on $t_2/t_1$ give a
nested hierarchy of subclasses of the directed acyclic graphs. In this paper we
characterize the subclasses in terms of forbidden subgraphs, and give
algorithms for finding an assignment of of utility values that explains the
relation in terms of a given $(t_1,t_2)$ or else produces a forbidden subgraph,
and finding the minimum value $\lambda$ of $t_2/t_1$ that is satisfiable for a
given directed acyclic graph. We show that $\lambda$ gives a measure of the
complexity of a directed acyclic graph with respect to several optimization
problems that are NP-hard on arbitrary directed acyclic graphs.
"
3762,"  Finding semantically rich and computer-understandable representations for
textual dialogues, utterances and words is crucial for dialogue systems (or
conversational agents), as their performance mostly depends on understanding
the context of conversations. Recent research aims at finding distributed
vector representations (embeddings) for words, such that semantically similar
words are relatively close within the vector-space. Encoding the ""meaning"" of
text into vectors is a current trend, and text can range from words, phrases
and documents to actual human-to-human conversations. In recent research
approaches, responses have been generated utilizing a decoder architecture,
given the vector representation of the current conversation. In this paper, the
utilization of embeddings for answer retrieval is explored by using
Locality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor
(ANN) model, to find similar conversations in a corpus and rank possible
candidates. Experimental results on the well-known Ubuntu Corpus (in English)
and a customer service chat dataset (in Dutch) show that, in combination with a
candidate selection method, retrieval-based approaches outperform generative
ones and reveal promising future research directions towards the usability of
such a system.
"
15066,"  We address the question concerning the birational geometry of the strata of
holomorphic and quadratic differentials. We show strata of holomorphic and
quadratic differentials to be uniruled in small genus by constructing rational
curves via pencils on K3 and del Pezzo surfaces respectively. Restricting to
genus $3\leq g\leq6$, we construct projective bundles over a rational varieties
that dominate the holomorphic strata with length at most $g-1$, hence showing
in addition that these strata are unirational.
"
14754,"  We consider the dynamics of message passing for spatially coupled codes and,
in particular, the set of density evolution equations that tracks the profile
of decoding errors along the spatial direction of coupling. It is known that,
for suitable boundary conditions and after a transient phase, the error profile
exhibits a ""solitonic behavior"". Namely, a uniquely-shaped wavelike solution
develops, that propagates with constant velocity. Under this assumption we
derive an analytical formula for the velocity in the framework of a continuum
limit of the spatially coupled system. The general formalism is developed for
spatially coupled low-density parity-check codes on general binary memoryless
symmetric channels which form the main system of interest in this work. We
apply the formula for special channels and illustrate that it matches the
direct numerical evaluation of the velocity for a wide range of noise values. A
possible application of the velocity formula to the evaluation of finite size
scaling law parameters is also discussed. We conduct a similar analysis for
general scalar systems and illustrate the findings with applications to
compressive sensing and generalized low-density parity-check codes on the
binary erasure or binary symmetric channels.
"
18211,"  We determined the shock-darkening pressure range in ordinary chondrites using
the iSALE shock physics code. We simulated planar shock waves on a mesoscale in
a sample layer at different nominal pressures. Iron and troilite grains were
resolved in a porous olivine matrix in the sample layer. We used equations of
state (Tillotson EoS and ANEOS) and basic strength and thermal properties to
describe the material phases. We used Lagrangian tracers to record peak shock
pressures in each material unit. The post-shock temperatures (and the fractions
of tracers experiencing temperatures above the melting point) for each material
were estimated after the passage of the shock wave and after reflections of the
shock at grain boundaries in the heterogeneous materials. The results showed
that shock-darkening, associated with troilite melt and the onset of olivine
melt, happened between 40 and 50 GPa - with 52 GPa being the pressure at which
all tracers in the troilite material reach the melting point. We demonstrate
the difficulties of shock heating in iron and also the importance of porosity.
Material impedances, grain shapes and the porosity models available in the
iSALE code are discussed. We also discussed possible not-shock-related triggers
for iron melt.
"
15499,"  In this paper, we consider multi-stage stochastic optimization problems with
convex objectives and conic constraints at each stage. We present a new
stochastic first-order method, namely the dynamic stochastic approximation
(DSA) algorithm, for solving these types of stochastic optimization problems.
We show that DSA can achieve an optimal ${\cal O}(1/\epsilon^4)$ rate of
convergence in terms of the total number of required scenarios when applied to
a three-stage stochastic optimization problem. We further show that this rate
of convergence can be improved to ${\cal O}(1/\epsilon^2)$ when the objective
function is strongly convex. We also discuss variants of DSA for solving more
general multi-stage stochastic optimization problems with the number of stages
$T > 3$. The developed DSA algorithms only need to go through the scenario tree
once in order to compute an $\epsilon$-solution of the multi-stage stochastic
optimization problem. To the best of our knowledge, this is the first time that
stochastic approximation type methods are generalized for multi-stage
stochastic optimization with $T \ge 3$.
"
4828,"  The present is a companion paper to ""A contemporary look at Hermann Hankel's
1861 pioneering work on Lagrangian fluid dynamics"" by Frisch, Grimberg and
Villone (2017). Here we present the English translation of the 1861 prize
manuscript from Göttingen University ""Zur allgemeinen Theorie der Bewegung
der Flüssigkeiten"" (On the general theory of the motion of the fluids) of
Hermann Hankel (1839-1873), which was originally submitted in Latin and then
translated into German by the Author for publication. We also provide the
English translation of two important reports on the manuscript, one written by
Bernhard Riemann and the other by Wilhelm Eduard Weber, during the assessment
process for the prize. Finally we give a short biography of Hermann Hankel with
his complete bibliography.
"
19949,"  This paper studies an optimal trading problem that incorporates the trader's
market view on the terminal asset price distribution and uninformative noise
embedded in the asset price dynamics. We model the underlying asset price
evolution by an exponential randomized Brownian bridge (rBb) and consider
various prior distributions for the random endpoint. We solve for the optimal
strategies to sell a stock, call, or put, and analyze the associated delayed
liquidation premia. We solve for the optimal trading strategies numerically and
compare them across different prior beliefs. Among our results, we find that
disconnected continuation/exercise regions arise when the trader prescribe a
two-point discrete distribution and double exponential distribution.
"
11298,"  We first consider the additive Brownian motion process $(X(s_1,s_2),\
(s_1,s_2) \in \mathbb{R}^2)$ defined by $X(s_1,s_2) = Z_1(s_1) - Z_2 (s_2)$,
where $Z_1$ and $Z_2 $ are two independent (two-sided) Brownian motions. We
show that with probability one, the Hausdorff dimension of the boundary of any
connected component of the random set $\{(s_1,s_2)\in \mathbb{R}^2: X(s_1,s_2)
>0\}$ is equal to $$
\frac{1}{4}\left(1 + \sqrt{13 + 4 \sqrt{5}}\right) \simeq 1.421\, . $$ Then
the same result is shown to hold when $X$ is replaced by a standard Brownian
sheet indexed by the nonnegative quadrant.
"
457,"  Fundamental relations between information and estimation have been
established in the literature for the continuous-time Gaussian and Poisson
channels, in a long line of work starting from the classical representation
theorems by Duncan and Kabanov respectively. In this work, we demonstrate that
such relations hold for a much larger family of continuous-time channels. We
introduce the family of semi-martingale channels where the channel output is a
semi-martingale stochastic process, and the channel input modulates the
characteristics of the semi-martingale. For these channels, which includes as a
special case the continuous time Gaussian and Poisson models, we establish new
representations relating the mutual information between the channel input and
output to an optimal causal filtering loss, thereby unifying and considerably
extending results from the Gaussian and Poisson settings. Extensions to the
setting of mismatched estimation are also presented where the relative entropy
between the laws governing the output of the channel under two different input
distributions is equal to the cumulative difference between the estimation loss
incurred by using the mismatched and optimal causal filters respectively. The
main tool underlying these results is the Doob--Meyer decomposition of a class
of likelihood ratio sub-martingales. The results in this work can be viewed as
the continuous-time analogues of recent generalizations for relations between
information and estimation for discrete-time Lévy channels.
"
19496,"  As modern precision cosmological measurements continue to show agreement with
the broad features of the standard $\Lambda$-Cold Dark Matter ($\Lambda$CDM)
cosmological model, we are increasingly motivated to look for small departures
from the standard model's predictions which might not be detected with standard
approaches. While searches for extensions and modifications of $\Lambda$CDM
have to date turned up no convincing evidence of beyond-the-standard-model
cosmology, the list of models compared against $\Lambda$CDM is by no means
complete and is often governed by readily-coded modifications to standard
Boltzmann codes. Also, standard goodness-of-fit methods such as a naive
$\chi^2$ test fail to put strong pressure on the null $\Lambda$CDM hypothesis,
since modern datasets have orders of magnitudes more degrees of freedom than
$\Lambda$CDM. Here we present a method of tuning goodness-of-fit tests to
detect potential sub-dominant extra-$\Lambda$CDM signals present in the data
through compressing observations in a way that maximizes extra-$\Lambda$CDM
signal variation over noise and $\Lambda$CDM variation. This method, based on a
Karhunen-Loève transformation of the data, is tuned to be maximally
sensitive to particular types of variations characteristic of the tuning model;
but, unlike direct model comparison, the test is also sensitive to features
that only partially mimic the tuning model. As an example of its use, we apply
this method in the context of a nonstandard primordial power spectrum compared
against the $2015$ $Planck$ CMB temperature and polarization power spectrum. We
find weak evidence of extra-$\Lambda$CDM physics, conceivably due to known
systematics in the 2015 Planck polarization release.
"
1442,"  We study the key domain wall properties in segmented nanowires loop-based
structures used in domain wall based sensors. The two reasons for device
failure, namely the distribution of domain wall propagation field (depinning)
and the nucleation field are determined with Magneto-Optical Kerr Effect (MOKE)
and Giant Magnetoresistance (GMR) measurements for thousands of elements to
obtain significant statistics. Single layers of Ni$_{81}$Fe$_{19}$, a complete
GMR stack with Co$_{90}$Fe$_{10}$/Ni$_{81}$Fe$_{19}$ as a free layer and a
single layer of Co$_{90}$Fe$_{10}$ are deposited and industrially patterned to
determine the influence of the shape anisotropy, the magnetocrystalline
anisotropy and the fabrication processes. We show that the propagation field is
little influenced by the geometry but significantly by material parameters. The
domain wall nucleation fields can be described by a typical Stoner-Wohlfarth
model related to the measured geometrical parameters of the wires and fitted by
considering the process parameters. The GMR effect is subsequently measured in
a substantial number of devices (3000), in order to accurately gauge the
variation between devices. This reveals a corrected upper limit to the
nucleation fields of the sensors that can be exploited for fast
characterization of working elements.
"
1362,"  The popular Alternating Least Squares (ALS) algorithm for tensor
decomposition is efficient and easy to implement, but often converges to poor
local optima---particularly when the weights of the factors are non-uniform. We
propose a modification of the ALS approach that is as efficient as standard
ALS, but provably recovers the true factors with random initialization under
standard incoherence assumptions on the factors of the tensor. We demonstrate
the significant practical superiority of our approach over traditional ALS for
a variety of tasks on synthetic data---including tensor factorization on exact,
noisy and over-complete tensors, as well as tensor completion---and for
computing word embeddings from a third-order word tri-occurrence tensor.
"
13430,"  We introduce right generating sets, Cayley graphs, growth functions, types
and rates, and isoperimetric constants for left homogeneous spaces equipped
with coordinate systems; characterise right amenable finitely right generated
left homogeneous spaces with finite stabilisers as those whose isoperimetric
constant is $0$; and prove that finitely right generated left homogeneous
spaces with finite stabilisers of sub-exponential growth are right amenable, in
particular, quotient sets of groups of sub-exponential growth by finite
subgroups are right amenable.
"
14641,"  Bayesian hierarchical models are increasingly popular for realistic modelling
and analysis of complex data. This trend is accompanied by the need for
flexible, general, and computationally efficient methods for model criticism
and conflict detection. Usually, a Bayesian hierarchical model incorporates a
grouping of the individual data points, for example individuals in repeated
measurement data. In such cases, the following question arises: Are any of the
groups ""outliers"", or in conflict with the remaining groups? Existing general
approaches aiming to answer such questions tend to be extremely computationally
demanding when model fitting is based on MCMC. We show how group-level model
criticism and conflict detection can be done quickly and accurately through
integrated nested Laplace approximations (INLA). The new method is implemented
as a part of the open source R-INLA package for Bayesian computing
(this http URL).
"
269,"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
"
6390,"  The quasi-two-dimensional organic charge-transfer salt
$\kappa$-(BEDT-TTF)$_2$Cu$_2$(CN)$_3$ is one of the prime candidates for a
quantum spin-liquid due the strong spin frustration of its anisotropic
triangular lattice in combination with its proximity to the Mott transition.
Despite intensive investigations of the material's low-temperature properties,
several important questions remain to be answered. Particularly puzzling are
the 6\,K anomaly and the enigmatic effects observed in magnetic fields. Here we
report on low-temperature measurements of lattice effects which were shown to
be particularly strongly pronounced in this material (R. S. Manna \emph{et
al.}, Phys. Rev. Lett. \textbf{104}, 016403 (2010)). A special focus of our
study lies on sample-to-sample variations of these effects and their
implications on the interpretation of experimental data. By investigating
overall nine single crystals from two different batches, we can state that
there are considerable differences in the size of the second-order phase
transition anomaly around 6\,K, varying within a factor of 3. In addition, we
find field-induced anomalies giving rise to pronounced features in the sample
length for two out of these nine crystals for temperatures $T <$ 9 K. We
tentatively assign the latter effects to $B$-induced magnetic clusters
suspected to nucleate around crystal imperfections. These $B$-induced effects
are absent for the crystals where the 6\,K anomaly is most strongly pronounced.
The large lattice effects observed at 6\,K are consistent with proposed pairing
instabilities of fermionic excitations breaking the lattice symmetry. The
strong sample-to-sample variation in the size of the phase transition anomaly
suggests that the conversion of the fermions to bosons at the instability is
only partial and to some extent influenced by not yet identified
sample-specific parameters.
"
4138,"  Self-adaptive system (SAS) is capable of adjusting its behavior in response
to meaningful changes in the operational context and itself. Due to the
inherent volatility of the open and changeable environment in which SAS is
embedded, the ability of adaptation is highly demanded by many
software-intensive systems. Two concerns, i.e., the requirements uncertainty
and the context uncertainty are most important among others. An essential issue
to be addressed is how to dynamically adapt non-functional requirements (NFRs)
and task configurations of SASs with context uncertainty. In this paper, we
propose a model-based fuzzy control approach that is underpinned by the
feedforward-feedback control mechanism. This approach identifies and represents
NFR uncertainties, task uncertainties and context uncertainties with linguistic
variables, and then designs an inference structure and rules for the fuzzy
controller based on the relations between the requirements model and the
context model. The adaptation of NFRs and task configurations is achieved
through fuzzification, inference, defuzzification and readaptation. Our
approach is demonstrated with a mobile computing application and is evaluated
through a series of simulation experiments.
"
4698,"  Convolutional and Recurrent, deep neural networks have been successful in
machine learning systems for computer vision, reinforcement learning, and other
allied fields. However, the robustness of such neural networks is seldom
apprised, especially after high classification accuracy has been attained. In
this paper, we evaluate the robustness of three recurrent neural networks to
tiny perturbations, on three widely used datasets, to argue that high accuracy
does not always mean a stable and a robust (to bounded perturbations,
adversarial attacks, etc.) system. Especially, normalizing the spectrum of the
discrete recurrent network to bound the spectrum (using power method, Rayleigh
quotient, etc.) on a unit disk produces stable, albeit highly non-robust neural
networks. Furthermore, using the $\epsilon$-pseudo-spectrum, we show that
training of recurrent networks, say using gradient-based methods, often result
in non-normal matrices that may or may not be diagonalizable. Therefore, the
open problem lies in constructing methods that optimize not only for accuracy
but also for the stability and the robustness of the underlying neural network,
a criterion that is distinct from the other.
"
19232,"  We present lifestate rules--an approach for abstracting event-driven object
protocols. Developing applications against event-driven software frameworks is
notoriously difficult. One reason why is that to create functioning
applications, developers must know about and understand the complex protocols
that abstract the internal behavior of the framework. Such protocols intertwine
the proper registering of callbacks to receive control from the framework with
appropriate application programming interface (API) calls to delegate back to
it. Lifestate rules unify lifecycle and typestate constraints in one common
specification language. Our primary contribution is a model of event-driven
systems from which lifestate rules can be derived. We then apply specification
mining techniques to learn lifestate specifications for Android framework
types. In the end, our implementation is able to find several rules that
characterize actual behavior of the Android framework.
"
1126,"  We reported the usage of grating-based X-ray phase-contrast imaging in
nondestructive testing of grating imperfections. It was found that
electroplating flaws could be easily detected by conventional absorption
signal, and in particular, we observed that the grating defects resulting from
uneven ultraviolet exposure could be clearly discriminated with phase-contrast
signal. The experimental results demonstrate that grating-based X-ray
phase-contrast imaging, with a conventional low-brilliance X-ray source, a
large field of view and a reasonable compact setup, which simultaneously yields
phase- and attenuation-contrast signal of the sample, can be ready-to-use in
fast nondestructive testing of various imperfections in gratings and other
similar photoetching products.
"
4331,"  It is well known that the addition of noise in a multistable system can
induce random transitions between stable states. The rate of transition can be
characterised in terms of the noise-free system's dynamics and the added noise:
for potential systems in the presence of asymptotically low noise the
well-known Kramers' escape time gives an expression for the mean escape time.
This paper examines some general properties and examples of transitions between
local steady and oscillatory attractors within networks: the transition rates
at each node may be affected by the dynamics at other nodes. We use first
passage time theory to explain some properties of scalings noted in the
literature for an idealised model of initiation of epileptic seizures in small
systems of coupled bistable systems with both steady and oscillatory
attractors. We focus on the case of sequential escapes where a steady attractor
is only marginally stable but all nodes start in this state. As the nodes
escape to the oscillatory regime, we assume that the transitions back are very
infrequent in comparison. We quantify and characterise the resulting sequences
of noise-induced escapes. For weak enough coupling we show that a master
equation approach gives a good quantitative understanding of sequential
escapes, but for strong coupling this description breaks down.
"
8451,"  We revisit the algebraic description of shape invariance method in
one-dimensional quantum mechanics. In this note we focus on four particular
examples: the Kepler problem in flat space, the Kepler problem in spherical
space, the Kepler problem in hyperbolic space, and the Rosen-Morse potential
problem. Following the prescription given by Gangopadhyaya et al., we first
introduce certain nonlinear algebraic systems. We then show that, if the model
parameters are appropriately quantized, the bound-state problems can be solved
solely by means of representation theory.
"
281,"  The task of calibration is to retrospectively adjust the outputs from a
machine learning model to provide better probability estimates on the target
variable. While calibration has been investigated thoroughly in classification,
it has not yet been well-established for regression tasks. This paper considers
the problem of calibrating a probabilistic regression model to improve the
estimated probability densities over the real-valued targets. We propose to
calibrate a regression model through the cumulative probability density, which
can be derived from calibrating a multi-class classifier. We provide three
non-parametric approaches to solve the problem, two of which provide empirical
estimates and the third providing smooth density estimates. The proposed
approaches are experimentally evaluated to show their ability to improve the
performance of regression models on the predictive likelihood.
"
5201,"  In this paper we present a loss-based approach to change point analysis. In
particular, we look at the problem from two perspectives. The first focuses on
the definition of a prior when the number of change points is known a priori.
The second contribution aims to estimate the number of change points by using a
loss-based approach recently introduced in the literature. The latter considers
change point estimation as a model selection exercise. We show the performance
of the proposed approach on simulated data and real data sets.
"
13565,"  The area of Handwritten Signature Verification has been broadly researched in
the last decades, but remains an open research problem. In offline (static)
signature verification, the dynamic information of the signature writing
process is lost, and it is difficult to design good feature extractors that can
distinguish genuine signatures and skilled forgeries. This verification task is
even harder in writer independent scenarios which is undeniably fiscal for
realistic cases. In this paper, we have proposed an Ensemble model for offline
writer, independent signature verification task with Deep learning. We have
used two CNNs for feature extraction, after that RGBT for classification &
Stacking to generate final prediction vector. We have done extensive
experiments on various datasets from various sources to maintain a variance in
the dataset. We have achieved the state of the art performance on various
datasets.
"
18043,"  We investigate the constraints on the sum of neutrino masses ($\Sigma m_\nu$)
using the most recent cosmological data, which combines the distance
measurement from baryonic acoustic oscillation in the extended Baryon
Oscillation Spectroscopic Survey DR14 quasar sample with the power spectra of
temperature and polarization anisotropies in the cosmic microwave background
from the Planck 2015 data release. We also use other low-redshift observations
including the baryonic acoustic oscillation at relatively low redshifts, the
supernovae of type Ia and the local measurement of Hubble constant. In the
standard cosmological constant $\Lambda$ cold dark matter plus massive neutrino
model, we obtain the $95\%$ \acl{CL} upper limit to be $\Sigma
m_\nu<0.129~\mathrm{eV}$ for the degenerate mass hierarchy, $\Sigma
m_{\nu}<0.159~\mathrm{eV}$ for the normal mass hierarchy, and $\Sigma
m_{\nu}<0.189~\mathrm{eV}$ for the inverted mass hierarchy. Based on Bayesian
evidence, we find that the degenerate hierarchy is positively supported, and
the current data combination can not distinguish normal and inverted
hierarchies. Assuming the degenerate mass hierarchy, we extend our study to
non-standard cosmological models including the generic dark energy, the spatial
curvature, and the extra relativistic degrees of freedom, respectively, but
find these models not favored by the data.
"
19571,"  Complex Finsler vector bundles have been studied mainly by T. Aikou, who
defined complex Finsler structures on holomorphic vector bundles. In this
paper, we consider the more general case of a holomorphic Lie algebroid E and
we introduce Finsler structures, partial and Chern-Finsler connections on it.
First, we recall some basic notions on holomorphic Lie algebroids. Then, using
an idea from E. Martinez, we introduce the concept of complexified prolongation
of such an algebroid. Also, we study nonlinear and linear connections on the
tangent bundle of E and on the prolongation of E and we investigate the
relation between their coefficients. The analogue of the classical
Chern-Finsler connection is defined and studied in the paper for the case of
the holomorphic Lie algebroid.
"
9848,"  Automatic differentiation (AD) is an essential primitive for machine learning
programming systems. Tangent is a new library that performs AD using source
code transformation (SCT) in Python. It takes numeric functions written in a
syntactic subset of Python and NumPy as input, and generates new Python
functions which calculate a derivative. This approach to automatic
differentiation is different from existing packages popular in machine
learning, such as TensorFlow and Autograd. Advantages are that Tangent
generates gradient code in Python which is readable by the user, easy to
understand and debug, and has no runtime overhead. Tangent also introduces
abstractions for easily injecting logic into the generated gradient code,
further improving usability.
"
6217,"  Summarizes recent work on the wakefields and impedances of flat, metallic
plates with small corrugations
"
19000,"  As a living information and communications system, the genome encodes
patterns in single nucleotide polymorphisms (SNPs) reflecting human adaption
that optimizes population survival in differing environments. This paper
mathematically models environmentally induced adaptive forces that quantify
changes in the distribution of SNP frequencies between populations. We make
direct connections between biophysical methods (e.g. minimizing genomic free
energy) and concepts in population genetics. Our unbiased computer program
scanned a large set of SNPs in the major histocompatibility complex region, and
flagged an altitude dependency on a SNP associated with response to oxygen
deprivation. The statistical power of our double-blind approach is demonstrated
in the flagging of mathematical functional correlations of SNP
information-based potentials in multiple populations with specific
environmental parameters. Furthermore, our approach provides insights for new
discoveries on the biology of common variants. This paper demonstrates the
power of biophysical modeling of population diversity for better understanding
genome-environment interactions in biological phenomenon.
"
14338,"  In this paper, we present a number of robust methodologies for an underwater
robot to visually detect, follow, and interact with a diver for collaborative
task execution. We design and develop two autonomous diver-following
algorithms, the first of which utilizes both spatial- and frequency-domain
features pertaining to human swimming patterns in order to visually track a
diver. The second algorithm uses a convolutional neural network-based model for
robust tracking-by-detection. In addition, we propose a hand gesture-based
human-robot communication framework that is syntactically simpler and
computationally more efficient than the existing grammar-based frameworks. In
the proposed interaction framework, deep visual detectors are used to provide
accurate hand gesture recognition; subsequently, a finite-state machine
performs robust and efficient gesture-to-instruction mapping. The
distinguishing feature of this framework is that it can be easily adopted by
divers for communicating with underwater robots without using artificial
markers or requiring memorization of complex language rules. Furthermore, we
validate the performance and effectiveness of the proposed methodologies
through extensive field experiments in closed- and open-water environments.
Finally, we perform a user interaction study to demonstrate the usability
benefits of our proposed interaction framework compared to existing methods.
"
1894,"  This paper introduces the combinatorial Boolean model (CBM), which is defined
as the class of linear combinations of conjunctions of Boolean attributes. This
paper addresses the issue of learning CBM from labeled data. CBM is of high
knowledge interpretability but naïve learning of it requires exponentially
large computation time with respect to data dimension and sample size. To
overcome this computational difficulty, we propose an algorithm GRAB (GRAfting
for Boolean datasets), which efficiently learns CBM within the
$L_1$-regularized loss minimization framework. The key idea of GRAB is to
reduce the loss minimization problem to the weighted frequent itemset mining,
in which frequent patterns are efficiently computable. We employ benchmark
datasets to empirically demonstrate that GRAB is effective in terms of
computational efficiency, prediction accuracy and knowledge discovery.
"
6574,"  We study the estimation of the covariance matrix $\Sigma$ of a
$p$-dimensional normal random vector based on $n$ independent observations
corrupted by additive noise. Only a general nonparametric assumption is imposed
on the distribution of the noise without any sparsity constraint on its
covariance matrix. In this high-dimensional semiparametric deconvolution
problem, we propose spectral thresholding estimators that are adaptive to the
sparsity of $\Sigma$. We establish an oracle inequality for these estimators
under model miss-specification and derive non-asymptotic minimax convergence
rates that are shown to be logarithmic in $n/\log p$. We also discuss the
estimation of low-rank matrices based on indirect observations as well as the
generalization to elliptical distributions. The finite sample performance of
the threshold estimators is illustrated in a numerical example.
"
20329,"  How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid
obstacles? One approach is to use a small dataset collected by human experts:
however, high capacity learning algorithms tend to overfit when trained with
little data. An alternative is to use simulation. But the gap between
simulation and real world remains large especially for perception problems. The
reason most research avoids using large-scale real data is the fear of crashes!
In this paper, we propose to bite the bullet and collect a dataset of crashes
itself! We build a drone whose sole purpose is to crash into objects: it
samples naive trajectories and crashes into random objects. We crash our drone
11,500 times to create one of the biggest UAV crash dataset. This dataset
captures the different ways in which a UAV can crash. We use all this negative
flying data in conjunction with positive data sampled from the same
trajectories to learn a simple yet powerful policy for UAV navigation. We show
that this simple self-supervised model is quite effective in navigating the UAV
even in extremely cluttered environments with dynamic obstacles including
humans. For supplementary video see: this https URL
"
1676,"  In a general linear model, this paper derives a necessary and sufficient
condition under which two general ridge estimators coincide with each other.
The condition is given as a structure of the dispersion matrix of the error
term. Since the class of estimators considered here contains linear unbiased
estimators such as the ordinary least squares estimator and the best linear
unbiased estimator, our result can be viewed as a generalization of the
well-known theorems on the equality between these two estimators, which have
been fully studied in the literature. Two related problems are also considered:
equality between two residual sums of squares, and classification of dispersion
matrices by a perturbation approach.
"
4839,"  We derive the second order rates of joint source-channel coding, whose source
obeys an irreducible and ergodic Markov process when the channel is a discrete
memoryless, while a previous study solved it only in a special case. We also
compare the joint source-channel scheme with the separation scheme in the
second order regime while a previous study made a notable comparison only with
numerical calculation. To make these two notable progress, we introduce two
kinds of new distribution families, switched Gaussian convolution distribution
and *-product distribution, which are defined by modifying the Gaussian
distribution.
"
17941,"  In this paper we consider the Dvali and Gómez assumption that the end state
of a gravitational collapse is a Bose-Einstein condensate of gravitons. We then
construct the two Gross-Pitaevskii equations for a static and spherically
symmetric configuration of the condensate. These two equations correspond to
the constrained minimisation of the gravitational Hamiltonian with respect to
the redshift and the Newtonian potential, per given number of gravitons. We
find that the effective geometry of the condensate is the one of a gravastar (a
DeSitter star) with a sub-Planckian cosmological constant, for masses larger
than the Planck scale. Thus, a condensate corresponding to a semiclassical
black hole, is always quantum and weakly coupled. Finally, we obtain that the
boundary of our gravastar, although it is not the location of a horizon,
corresponds to the Schwarzschild radius.
"
15953,"  Generative modeling of high dimensional data like images is a notoriously
difficult and ill-defined problem. In particular, how to evaluate a learned
generative model is unclear. In this position paper, we argue that adversarial
learning, pioneered with generative adversarial networks (GANs), provides an
interesting framework to implicitly define more meaningful task losses for
generative modeling tasks, such as for generating ""visually realistic"" images.
We refer to those task losses as parametric adversarial divergences and we give
two main reasons why we think parametric divergences are good learning
objectives for generative modeling. Additionally, we unify the processes of
choosing a good structured loss (in structured prediction) and choosing a
discriminator architecture (in generative modeling) using statistical decision
theory; we are then able to formalize and quantify the intuition that ""weaker""
losses are easier to learn from, in a specific setting. Finally, we propose two
new challenging tasks to evaluate parametric and nonparametric divergences: a
qualitative task of generating very high-resolution digits, and a quantitative
task of learning data that satisfies high-level algebraic constraints. We use
two common divergences to train a generator and show that the parametric
divergence outperforms the nonparametric divergence on both the qualitative and
the quantitative task.
"
13436,"  We propose a novel approach to parameter estimation for simulator-based
statistical models with intractable likelihood. Our proposed method involves
recursive application of kernel ABC and kernel herding to the same observed
data. We provide a theoretical explanation regarding why the approach works,
showing (for the population setting) that, under a certain assumption, point
estimates obtained with this method converge to the true parameter, as
recursion proceeds. We have conducted a variety of numerical experiments,
including parameter estimation for a real-world pedestrian flow simulator, and
show that in most cases our method outperforms existing approaches.
"
11488,"  Technological developments call for increasing perception and action
capabilities of robots. Among other skills, vision systems that can adapt to
any possible change in the working conditions are needed. Since these
conditions are unpredictable, we need benchmarks which allow to assess the
generalization and robustness capabilities of our visual recognition
algorithms. In this work we focus on robotic kitting in unconstrained
scenarios. As a first contribution, we present a new visual dataset for the
kitting task. Differently from standard object recognition datasets, we provide
images of the same objects acquired under various conditions where camera,
illumination and background are changed. This novel dataset allows for testing
the robustness of robot visual recognition algorithms to a series of different
domain shifts both in isolation and unified. Our second contribution is a novel
online adaptation algorithm for deep models, based on batch-normalization
layers, which allows to continuously adapt a model to the current working
conditions. Differently from standard domain adaptation algorithms, it does not
require any image from the target domain at training time. We benchmark the
performance of the algorithm on the proposed dataset, showing its capability to
fill the gap between the performances of a standard architecture and its
counterpart adapted offline to the given target domain.
"
10969,"  This article develops a framework for testing general hypothesis in
high-dimensional models where the number of variables may far exceed the number
of observations. Existing literature has considered less than a handful of
hypotheses, such as testing individual coordinates of the model parameter.
However, the problem of testing general and complex hypotheses remains widely
open. We propose a new inference method developed around the hypothesis
adaptive projection pursuit framework, which solves the testing problems in the
most general case. The proposed inference is centered around a new class of
estimators defined as $l_1$ projection of the initial guess of the unknown onto
the space defined by the null. This projection automatically takes into account
the structure of the null hypothesis and allows us to study formal inference
for a number of long-standing problems. For example, we can directly conduct
inference on the sparsity level of the model parameters and the minimum signal
strength. This is especially significant given the fact that the former is a
fundamental condition underlying most of the theoretical development in
high-dimensional statistics, while the latter is a key condition used to
establish variable selection properties. Moreover, the proposed method is
asymptotically exact and has satisfactory power properties for testing very
general functionals of the high-dimensional parameters. The simulation studies
lend further support to our theoretical claims and additionally show excellent
finite-sample size and power properties of the proposed test.
"
4054,"  Deep neural networks (DNN) excel at extracting patterns. Through
representation learning and automated feature engineering on large datasets,
such models have been highly successful in computer vision and natural language
applications. Designing optimal network architectures from a principled or
rational approach however has been less than successful, with the best
successful approaches utilizing an additional machine learning algorithm to
tune the network hyperparameters. However, in many technical fields, there
exist established domain knowledge and understanding about the subject matter.
In this work, we develop a novel furcated neural network architecture that
utilizes domain knowledge as high-level design principles of the network. We
demonstrate proof-of-concept by developing IL-Net, a furcated network for
predicting the properties of ionic liquids, which is a class of complex
multi-chemicals entities. Compared to existing state-of-the-art approaches, we
show that furcated networks can improve model accuracy by approximately 20-35%,
without using additional labeled data. Lastly, we distill two key design
principles for furcated networks that can be adapted to other domains.
"
1715,"  Let $\Omega$ be an unbounded domain in $\mathbb{R}\times\mathbb{R}^{d}.$ A
positive harmonic function $u$ on $\Omega$ that vanishes on the boundary of
$\Omega$ is called a Martin function. In this note, we show that, when $\Omega$
is convex, the superlevel sets of a Martin function are also convex. As a
consequence we obtain that if in addition $\Omega$ is symmetric, then the
maximum of any Martin function along a slice $\Omega\cap
(\{t\}\times\mathbb{R}^d)$ is attained at $(t,0).$
"
2499,"  This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
"
15796,"  User-based Collaborative Filtering (CF) is one of the most popular approaches
to create recommender systems. This approach is based on finding the most
relevant k users from whose rating history we can extract items to recommend.
CF, however, suffers from data sparsity and the cold-start problem since users
often rate only a small fraction of available items. One solution is to
incorporate additional information into the recommendation process such as
explicit trust scores that are assigned by users to others or implicit trust
relationships that result from social connections between users. Such
relationships typically form a very sparse trust network, which can be utilized
to generate recommendations for users based on people they trust. In our work,
we explore the use of a measure from network science, i.e. regular equivalence,
applied to a trust network to generate a similarity matrix that is used to
select the k-nearest neighbors for recommending items. We evaluate our approach
on Epinions and we find that we can outperform related methods for tackling
cold-start users in terms of recommendation accuracy.
"
9008,"  The ANAIS experiment aims at the confirmation of the DAMA/LIBRA signal at the
Canfranc Underground Laboratory (LSC). Several 12.5 kg NaI(Tl) modules produced
by Alpha Spectra Inc. have been operated there during the last years in various
set-ups; an outstanding light collection at the level of 15 photoelectrons per
keV, which allows triggering at 1 keV of visible energy, has been measured for
all of them and a complete characterization of their background has been
achieved. In the first months of 2017, the full ANAIS-112 set-up consisting of
nine Alpha Spectra detectors with a total mass of 112.5 kg was commissioned at
LSC and the first dark matter run started in August, 2017. Here, the latest
results on the detectors performance and measured background from the
commissioning run will be presented and the sensitivity prospects of the
ANAIS-112 experiment will be discussed.
"
11438,"  We investigate the dynamics of a coupled waveguide system with competing
linear and nonlinear loss-gain profiles which can facilitate power saturation.
We show the usefulness of the model in achieving unidirectional beam
propagation. In this regard, the considered type of coupled waveguide system
has two drawbacks, (i) difficulty in achieving perfect isolation of light in a
waveguide and (ii) existence of blow-up type behavior for certain input power
situations. We here show a nonlinear $\cal{PT}$ symmetric coupling that helps
to overcome these two drawbacks. Such a nonlinear coupling has close connection
with the phenomenon of stimulated Raman scattering. In particular, we have
elucidated the role of this nonlinear coupling using an integrable $\cal{PT}$
symmetric situation. In particular, using the integrals of motion, we have
reduced this coupled waveguide problem to the problem of dynamics of a particle
in a potential. With the latter picture, we have clearly illustrated the role
of the considered nonlinear coupling. The above $\cal{PT}$ symmetric case
corresponds to a limiting form of a general equation describing the phenomenon
of stimulated Raman scattering. We also point out the ability to transport
light unidirectionally even in this general case.
"
11620,"  This paper introduces a new free library for the Python programming language,
which provides a collection of structured linear transforms, that are not
represented as explicit two dimensional arrays but in a more efficient way by
exploiting the structural knowledge.
This allows fast and memory savy forward and backward transformations while
also provding a clean but still flexible interface to these effcient
algorithms, thus making code more readable, scable and adaptable.
We first outline the goals of this library, then how they were achieved and
lastly we demonstrate the performance compared to current state of the art
packages available for Python.
This library is released and distributed under a free license.
"
3429,"  A new majority and minority voted redundancy (MMR) scheme is proposed that
can provide the same degree of fault tolerance as N-modular redundancy (NMR)
but with fewer function units and a less sophisticated voting logic. Example
NMR and MMR circuits were implemented using a 32/28nm CMOS process and
compared. The results show that MMR circuits dissipate less power, occupy less
area, and encounter less critical path delay than the corresponding NMR
circuits while providing the same degree of fault tolerance. Hence the MMR is a
promising alternative to the NMR to efficiently implement high levels of
redundancy in safety-critical applications.
"
13086,"  Backscatter of electrons from a beta spectrometer, with incomplete energy
deposition, can lead to undesirable effects in many types of experiments. We
present and discuss the design and operation of a backscatter-suppressed beta
spectrometer that was developed as part of a program to measure the
electron-antineutrino correlation coefficient in neutron beta decay (aCORN). An
array of backscatter veto detectors surrounds a plastic scintillator beta
energy detector. The spectrometer contains an axial magnetic field gradient, so
electrons are efficiently admitted but have a low probability for escaping back
through the entrance after backscattering. The design, construction,
calibration, and performance of the spectrometer are discussed.
"
1114,"  Polarized extinction and emission from dust in the interstellar medium (ISM)
are hard to interpret, as they have a complex dependence on dust optical
properties, grain alignment and magnetic field orientation. This is
particularly true in molecular clouds. The data available today are not yet
used to their full potential.
The combination of emission and extinction, in particular, provides
information not available from either of them alone. We combine data from the
scientific literature on polarized dust extinction with Planck data on
polarized emission and we use them to constrain the possible variations in dust
and environmental conditions inside molecular clouds, and especially
translucent lines of sight, taking into account magnetic field orientation.
We focus on the dependence between \lambda_max -- the wavelength of maximum
polarization in extinction -- and other observables such as the extinction
polarization, the emission polarization and the ratio of the two. We set out to
reproduce these correlations using Monte-Carlo simulations where the relevant
quantities in a dust model -- grain alignment, size distribution and magnetic
field orientation -- vary to mimic the diverse conditions expected inside
molecular clouds.
None of the quantities chosen can explain the observational data on its own:
the best results are obtained when all quantities vary significantly across and
within clouds. However, some of the data -- most notably the stars with low
emission-to-extinction polarization ratio -- are not reproduced by our
simulation. Our results suggest not only that dust evolution is necessary to
explain polarization in molecular clouds, but that a simple change in size
distribution is not sufficient to explain the data, and point the way for
future and more sophisticated models.
"
14614,"  The classification of time series data is a challenge common to all
data-driven fields. However, there is no agreement about which are the most
efficient techniques to group unlabeled time-ordered data. This is because a
successful classification of time series patterns depends on the goal and the
domain of interest, i.e. it is application-dependent.
In this article, we study free-to-play game data. In this domain, clustering
similar time series information is increasingly important due to the large
amount of data collected by current mobile and web applications. We evaluate
which methods cluster accurately time series of mobile games, focusing on
player behavior data. We identify and validate several aspects of the
clustering: the similarity measures and the representation techniques to reduce
the high dimensionality of time series. As a robustness test, we compare
various temporal datasets of player activity from two free-to-play video-games.
With these techniques we extract temporal patterns of player behavior
relevant for the evaluation of game events and game-business diagnosis. Our
experiments provide intuitive visualizations to validate the results of the
clustering and to determine the optimal number of clusters. Additionally, we
assess the common characteristics of the players belonging to the same group.
This study allows us to improve the understanding of player dynamics and churn
behavior.
"
1444,"  We investigate the magnetic properties of the multiferroic quantum-spin
system LiCu$_2$O$_2$ by electron spin resonance (ESR) measurements at $X$- and
$Q$-band frequencies in a wide temperature range $(T_{\rm N1} \leq T \leq
300$\,K). The observed anisotropies of the $g$ tensor and the ESR linewidth in
untwinned single crystals result from the crystal-electric field and from local
exchange geometries acting on the magnetic Cu$^{2+}$ ions in the zigzag-ladder
like structure of LiCu$_2$O$_2$. Supported by a microscopic analysis of the
exchange paths involved, we show that both the symmetric anisotropic exchange
interaction and the antisymmetric Dzyaloshinskii-Moriya interaction provide the
dominant spin-spin relaxation channels in this material.
"
11959,"  This research investigates the implementation mechanism of block-wise ILU(k)
preconditioner on GPU. The block-wise ILU(k) algorithm requires both the level
k and the block size to be designed as variables. A decoupled ILU(k) algorithm
consists of a symbolic phase and a factorization phase. In the symbolic phase,
a ILU(k) nonzero pattern is established from the point-wise structure extracted
from a block-wise matrix. In the factorization phase, the block-wise matrix
with a variable block size is factorized into a block lower triangular matrix
and a block upper triangular matrix. And a further diagonal factorization is
required to perform on the block upper triangular matrix for adapting a
parallel triangular solver on GPU.We also present the numerical experiments to
study the preconditioner actions on different k levels and block sizes.
"
17790,"  This document outlines the approach to supporting cross-node transactions
over a Redis cluster.
"
7406,"  A loop-augmented forest is a labeled rooted forest with loops on some of its
roots. By exploiting an interplay between nilpotent partial functions and
labeled rooted forests, we investigate the permutation action of the symmetric
group on loop-augmented forests. Furthermore, we describe an extension of the
Foulkes' conjecture and prove a special case. Among other important outcomes of
our analysis are a complete description of the stabilizer subgroup of an
idempotent in the semigroup of partial transformations and a generalization of
the (Knuth-Sagan) hook length formula.
"
10891,"  We propose hMDAP, a hybrid framework for large-scale data analytical
processing on Spark, to support multi-paradigm process (incl. OLAP, machine
learning, and graph analysis etc.) in distributed environments. The framework
features a three-layer data process module and a business process module which
controls the former. We will demonstrate the strength of hMDAP by using traffic
scenarios in a real world.
"
7496,"  If the topological insulator Bi$_{2}$Se$_{3}$ is doped with electrons,
superconductivity with $T_{\rm c}\approx3-4\:{\rm K}$ emerges for a low
density of carriers ($n\approx10^{20}{\rm cm}^{-3}$) and with a small ratio of
the superconducting coherence length and Fermi wave length:
$\xi/\lambda_{F}\approx2\cdots4$. These values make fluctuations of the
superconducting order parameter increasingly important, to the extend that the
$T_{c}$-value is surprisingly large. Strong spin-orbit interaction led to the
proposal of an odd-parity pairing state. This begs the question of the nature
of the transition in an unconventional superconductor with strong pairing
fluctuations. We show that for a multi-component order parameter, these
fluctuations give rise to a nematic phase at $T_{\rm nem}>T_{c}$. Below
$T_{c}$ several experiments demonstrated a rotational symmetry breaking where
the Cooper pair wave function is locked to the lattice. Our theory shows that
this rotational symmetry breaking, as vestige of the superconducting state,
already occurs above $T_{c}$. The nematic phase is characterized by vanishing
off-diagonal long range order, yet with anisotropic superconducting
fluctuations. It can be identified through direction-dependent
para-conductivity, lattice softening, and an enhanced Raman response in the
$E_{g}$ symmetry channel. In addition, nematic order partially avoids the usual
fluctuation suppression of $T_{c}$.
"
11314,"  In this paper, we discuss the generalized Hamming weights of a class of
linear codes associated with non-degenerate quadratic forms. In order to do so,
we study the quadratic forms over subspaces of finite field and obtain some
interesting results about subspaces and their dual spaces. On this basis, we
solve all the generalized Hamming weights of these linear codes.
"
16905,"  We show that the smallest non-abelian quotient of $\mathrm{Aut}(F_n)$ is
$\mathrm{PSL}_n(\mathbb{Z}/2\mathbb{Z}) = \mathrm{L}_n(2)$, thus confirming a
conjecture of Mecchia--Zimmermann. In the course of the proof we give an
exponential (in $n$) lower bound for the cardinality of a set on which
$\mathrm{SAut}(F_n)$, the unique index $2$ subgroup of $\mathrm{Aut}(F_n)$, can
act non-trivially. We also offer new results on the representation theory of
$\mathrm{SAut(F_n)}$ in small dimensions over small, positive characteristics,
and on rigidity of maps from $\mathrm{SAut}(F_n)$ to finite groups of Lie type
and algebraic groups in characteristic $2$.
"
12714,"  Consider a quadratic vector field on $\mathbb{C}^2$ having an invariant line
at infinity and isolated singularities only. We define the extended spectra of
singularities to be the collection of the spectra of the linearization matrices
of each of the singular points over the affine part, together with all the
characteristic numbers (i.e. Camacho-Sad indices) at infinity. This collection
consists of 11 complex numbers, and is invariant under affine equivalence of
vector fields.
In this paper we describe all polynomial relations among these numbers. There
are 5 independent polynomial relations; four of them follow from the
Euler-Jacobi, the Baum-Bott and the Camacho-Sad index theorems, and are well
known. The fifth relation was, until now, completely unknown. We provide an
explicit formula for the missing 5th relation, discuss it's meaning and prove
that it cannot be formulated as an index theorem.
"
13963,"  We show that 2-dimensional systolic complexes are quasi-isometric to quadric
complexes with flat intervals. We use this fact along with the weight function
of Brodzki, Campbell, Guentner, Niblo and Wright to prove that 2-dimensional
systolic complexes satisfy Property A.
"
2618,"  We establish four supercongruences between truncated ${}_3F_2$ hypergeometric
series involving $p$-adic Gamma functions, which extend some of the
Rodriguez-Villegas supercongruences.
"
5325,"  Sparsity of the solution of a linear regression model is a common
requirement, and many prior distributions have been designed for this purpose.
A combination of the sparsity requirement with smoothness of the solution is
also common in application, however, with considerably fewer existing prior
models. In this paper, we compare two prior structures, the Bayesian fused
lasso (BFL) and least-squares with adaptive prior covariance matrix (LS-APC).
Since only variational solution was published for the latter, we derive a Gibbs
sampling algorithm for its inference and Bayesian model selection. The method
is designed for high dimensional problems, therefore, we discuss numerical
issues associated with evaluation of the posterior. In simulation, we show that
the LS-APC prior achieves results comparable to that of the Bayesian Fused
Lasso for piecewise constant parameter and outperforms the BFL for parameters
of more general shapes. Another advantage of the LS-APC priors is revealed in
real application to estimation of the release profile of the European Tracer
Experiment (ETEX). Specifically, the LS-APC model provides more conservative
uncertainty bounds when the regressor matrix is not informative.
"
17577,"  Let $q$ be a prime power of a prime $p$, $n$ a positive integer and $\mathbb
F_{q^n}$ the finite field with $q^n$ elements. The $k-$normal elements over
finite fields were introduced and characterized by Huczynska et al (2013).
Under the condition that $n$ is not divisible by $p$, they obtained an
existence result on primitive $1-$normal elements of $\mathbb F_{q^n}$ over
$\mathbb F_q$ for $q>2$. In this note, we extend their result to the excluded
case $q=2$.
"
14517,"  Biological organisms have to cope with stochastic variations in both the
external environment and the internal population dynamics. Theoretical studies
and laboratory experiments suggest that population diversification could be an
effective bet-hedging strategy for adaptation to varying environments. Here we
show that bet-hedging can also be effective against demographic fluctuations
that pose a trade-off between growth and survival for populations even in a
constant environment. A species can maximize its overall abundance in the long
term by diversifying into coexisting subpopulations of both ""fast-growing"" and
""better-surviving"" individuals. Our model generalizes statistical physics
models of birth-death processes to incorporate dispersal, during which new
populations are founded, and can further incorporate variations of local
environments. In this way we unify different bet-hedging strategies against
demographic and environmental variations as a general means of adaptation to
both types of uncertainties in population growth.
"
11994,"  Automatic question-answering is a classical problem in natural language
processing, which aims at designing systems that can automatically answer a
question, in the same way as human does. In this work, we propose a deep
learning based model for automatic question-answering. First the questions and
answers are embedded using neural probabilistic modeling. Then a deep
similarity neural network is trained to find the similarity score of a pair of
answer and question. Then for each question, the best answer is found as the
one with the highest similarity score. We first train this model on a
large-scale public question-answering database, and then fine-tune it to
transfer to the customer-care chat data. We have also tested our framework on a
public question-answering database and achieved very good performance.
"
9862,"  The concept of a $\Gamma$-semigroup has been introduced by Mridul Kanti Sen
in the Int. Symp., New Delhi, 1981. It is well known that the Green's relations
play an essential role in studying the structure of semigroups. In the present
paper we deal with an application of $\Gamma$-semigroups techniques to the
Green's Theorem in an attempt to show the way we pass from semigroups to
$\Gamma$-semigroups.
"
20084,"  The remarkably strong chemical adsorption behaviors of nitric oxide on
magnesia (001) film deposited on metal substrate have been investigated by
employing periodic density functional calculations with Van der Waals
corrections. The molybdenum supported magnesia (001) show significantly
enhanced adsorption properties and the nitric oxide is chemisorbed strongly and
preferably trapped in flat adsorption configuration on metal supported oxide
film, due to the substantially large adsorption energies and transformation
barriers. The analysis of Bader charges, projected density of states,
differential charge densities, electron localization function, highest occupied
orbital and particular orbital with largest Mg-NO-Mg bonding coefficients, are
applied to reveal the electronic adsorption properties and characteristics of
bonding between nitric oxide and surface as well as the bonding within the
hybrid structure. The strong chemical binding of nitric oxide on magnesia
deposited on molybdenum slab offers new opportunities for toxic gas detection
and treatment. We anticipate that hybrid structure promoted remarkable chemical
adsorption of nitric oxide on magnesia in this study will provide versatile
strategy for enhancing chemical reactivity and properties of insulating oxide.
"
14776,"  We prove the superhedging duality for a discrete-time financial market with
proportional transaction costs under portfolio constraints and model
uncertainty. Frictions are modeled through solvency cones as in the original
model of [Kabanov, Y., Hedging and liquidation under transaction costs in
currency markets. Fin. Stoch., 3(2):237-248, 1999] adapted to the quasi-sure
setup of [Bouchard, B. and Nutz, M., Arbitrage and duality in nondominated
discrete-time models. Ann. Appl. Probab., 25(2):823-859, 2015]. Our results
hold under the condition of No Strict Arbitrage and under the efficient
friction hypothesis.
"
7792,"  We present a complete resolution of the Abraham-Minkowski controversy . This
is done by considering several new aspects which invalidate previous
discussions. We show that: 1)For polarized matter the center of mass theorem is
no longer valid in its usual form. A contribution related to microscopic spin
should be considered. 2)The electromagnetic dipolar energy density contributes
to the inertia of matter and should be incorporated covariantly to the the
energy-momentum tensor of matter. Then there is an electromagnetic component in
matter's momentum density whose variation explains the results of the only
experiment which supports Abraham's force. 3)Averaging the microscopic
Lorentz's force results in the unambiguos expression for the force density
exerted by the field. This force density is consistent with all the
experimental evidence. 4)Momentum conservation determines the electromagnetic
energy-momentum tensor. This tensor is different from Abraham's and Minkowski's
tensors, but one recovers Minkowski's expression for the momentum density. The
energy density is different from Poynting's expression but Poynting's vector
remains the same. Our tensor is non-symmetric which allows the field to exert a
distributed torque on matter. We use our results to discuss momentum and
angular momentum exchange in various situations of physical interest. We find
complete consistency of our equations in the description of the systems
considered. We also show that several alternative expressions of the field
energy-momentum tensor and force-density cannot be successfully used in all our
examples. In particular we verify in two of these examples that the center of
mass and spin introduced by us moves with constant velocity, but that the
standard center of mass does not.
"
6095,"  We present the Lyman-$\alpha$ flux power spectrum measurements of the XQ-100
sample of quasar spectra obtained in the context of the European Southern
Observatory Large Programme ""Quasars and their absorption lines: a legacy
survey of the high redshift universe with VLT/XSHOOTER"". Using $100$ quasar
spectra with medium resolution and signal-to-noise ratio we measure the power
spectrum over a range of redshifts $z = 3 - 4.2$ and over a range of scales $k
= 0.003 - 0.06\,\mathrm{s\,km^{-1}}$. The results agree well with the
measurements of the one-dimensional power spectrum found in the literature. The
data analysis used in this paper is based on the Fourier transform and has been
tested on synthetic data. Systematic and statistical uncertainties of our
measurements are estimated, with a total error (statistical and systematic)
comparable to the one of the BOSS data in the overlapping range of scales, and
smaller by more than $50\%$ for higher redshift bins ($z>3.6$) and small scales
($k > 0.01\,\mathrm{s\,km^{-1}}$). The XQ-100 data set has the unique feature
of having signal-to-noise ratios and resolution intermediate between the two
data sets that are typically used to perform cosmological studies, i.e. BOSS
and high-resolution spectra (e.g. UVES/VLT or HIRES). More importantly, the
measured flux power spectra span the high redshift regime which is usually more
constraining for structure formation models.
"
17284,"  We consider the problem of training generative models with deep neural
networks as generators, i.e. to map latent codes to data points. Whereas the
dominant paradigm combines simple priors over codes with complex deterministic
models, we propose instead to use more flexible code distributions. These
distributions are estimated non-parametrically by reversing the generator map
during training. The benefits include: more powerful generative models, better
modeling of latent structure and explicit control of the degree of
generalization.
"
13677,"  Undetected overfitting can occur when there are significant redundancies
between training and validation data. We describe AVE, a new measure of
training-validation redundancy for ligand-based classification problems that
accounts for the similarity amongst inactive molecules as well as active. We
investigated seven widely-used benchmarks for virtual screening and
classification, and show that the amount of AVE bias strongly correlates with
the performance of ligand-based predictive methods irrespective of the
predicted property, chemical fingerprint, similarity measure, or
previously-applied unbiasing techniques. Therefore, it may be that the
previously-reported performance of most ligand-based methods can be explained
by overfitting to benchmarks rather than good prospective accuracy.
"
493,"  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part.
"
4045,"  By formally invoking the Wiener-Hopf method, we explicitly solve a
one-dimensional, singular integral equation for the excitation of a slowly
decaying electromagnetic wave, called surface plasmon-polariton (SPP), of small
wavelength on a semi-infinite, flat conducting sheet irradiated by a plane wave
in two spatial dimensions. This setting is germane to wave diffraction by edges
of large sheets of single-layer graphene. Our analytical approach includes: (i)
formulation of a functional equation in the Fourier domain; (ii) evaluation of
a split function, which is expressed by a contour integral and is a key
ingredient of the Wiener-Hopf factorization; and (iii) extraction of the SPP as
a simple-pole residue of a Fourier integral. Our analytical solution is in good
agreement with a finite-element numerical computation.
"
8011,"  Many current and future exoplanet missions are pushing to infrared (IR)
wavelengths where the flux contrast between the planet and star is more
favorable (Deming et al. 2009), and the impact of stellar magnetic activity is
decreased. Indeed, a recent analysis of starspots and faculae found these forms
of stellar activity do not substantially impact the transit signatures or
science potential for FGKM stars with JWST (Zellem et al. 2017). However, this
is not true in the case of flares, which I demonstrate can be a hinderance to
transit studies in this note.
"
7584,"  The universal homogeneous triangle-free graph, constructed by Henson and
denoted $\mathcal{H}_3$, is the triangle-free analogue of the Rado graph. While
the Ramsey theory of the Rado graph has been completely established, beginning
with Erdős-Hajnal-Posá and culminating in work of Sauer and
Laflamme-Sauer-Vuksanovic, the Ramsey theory of $\mathcal{H}_3$ had only
progressed to bounds for vertex colorings (Komjáth-Rödl) and edge
colorings (Sauer). This was due to a lack of broadscale techniques.
We solve this problem in general: For each finite triangle-free graph $G$,
there is a finite number $T(G)$ such that for any coloring of all copies of $G$
in $\mathcal{H}_3$ into finitely many colors, there is a subgraph of
$\mathcal{H}_3$ which is again universal homogeneous triangle-free in which the
coloring takes no more than $T(G)$ colors. This is the first such result for a
homogeneous structure omitting copies of some non-trivial finite structure. The
proof entails developments of new broadscale techniques, including a flexible
method for constructing trees which code $\mathcal{H}_3$ and the development of
their Ramsey theory.
"
12364,"  Purpose: The goal of this study is to show the advantage of a collaborative
work in the annotation and evaluation of prostate cancer tissues from
T2-weighted MRI compared to the commonly used double blind evaluation.
Methods: The variability of medical findings focused on the prostate gland
(central gland, peripheral and tumoural zones) by two independent experts was
firstly evaluated, and secondly compared with a consensus of these two experts.
Using a prostate MRI database, experts drew regions of interest (ROIs)
corresponding to healthy prostate (peripheral and central zones) and cancer
using a semi-automated tool. One of the experts then drew the ROI with
knowledge of the other expert's ROI.
Results: The surface area of each ROI as the Hausdorff distance and the Dice
coefficient for each contour were evaluated between the different experiments,
taking the drawing of the second expert as the reference. The results showed
that the significant differences between the two experts became non-significant
with a collaborative work.
Conclusions: This study shows that collaborative work with a dedicated tool
allows a better consensus between expertise than using a double blind
evaluation. Although we show this for prostate cancer evaluation in T2-weighted
MRI, the results of this research can be extrapolated to other diseases and
kind of medical images.
"
20748,"  Molecular dynamics is based on solving Newton's equations for many-particle
systems that evolve along complex, highly fluctuating trajectories. The orbital
instability and short-time complexity of Newtonian orbits is in sharp contrast
to the more coherent behavior of collective modes such as density profiles. The
notion of virtual molecular dynamics is introduced here based on temporal
coarse-graining via Pade approximants and the Ito formula for stochastic
processes. It is demonstrated that this framework leads to significant
efficiency over traditional molecular dynamics and avoids the need to introduce
coarse-grained variables and phenomenological equations for their evolution. In
this framework, an all-atom trajectory is represented by a Markov chain of
virtual atomic states at a discrete sequence of timesteps, transitions between
which are determined by an integration of conventional molecular dynamics with
Pade approximants and a microstate energy annealing methodology. The latter is
achieved by a conventional and an MD NVE energy minimization schemes. This
multiscale framework is demonstrated for a pertussis toxin subunit undergoing a
structural transition, a T=1 capsid-like structure of HPV16 L1 protein, and two
coalescing argon droplets.
"
11132,"  Constrained Markov Decision Process (CMDP) is a natural framework for
reinforcement learning tasks with safety constraints, where agents learn a
policy that maximizes the long-term reward while satisfying the constraints on
the long-term cost. A canonical approach for solving CMDPs is the primal-dual
method which updates parameters in primal and dual spaces in turn. Existing
methods for CMDPs only use on-policy data for dual updates, which results in
sample inefficiency and slow convergence. In this paper, we propose a policy
search method for CMDPs called Accelerated Primal-Dual Optimization (APDO),
which incorporates an off-policy trained dual variable in the dual update
procedure while updating the policy in primal space with on-policy likelihood
ratio gradient. Experimental results on a simulated robot locomotion task show
that APDO achieves better sample efficiency and faster convergence than
state-of-the-art approaches for CMDPs.
"
14444,"  The terms ""acoustic/elastic meta-materials"" describe a class of periodic
structures with unit cells exhibiting local resonance. This localized resonant
structure has been shown to result in negative effective stiffness and/or mass
at frequency ranges close to these local resonances. As a result, these
structures present unusual wave propagation properties at wavelengths well
below the regime corresponding to band-gap generation based on spatial
periodicity, (i.e. ""Bragg scattering""). Therefore, acoustic/elastic
meta-materials can lead to applications, especially suitable in the
low-frequency range. However, low frequency range applications of such
meta-materials require very heavy internal moving masses, as well as additional
constraints at the amplitudes of the internally oscillating locally resonating
structures, which may prohibit their practical implementation. In order to
resolve this disadvantage, the K-Damping concept will be analyzed. According to
this concept, the acoustic/elastic meta-materials are designed to include
negative stiffness elements instead or in addition to the internally resonating
added masses. This concept removes the need for the heavy locally added heavy
masses, while it simultaneously exploits the negative stiffness damping
phenomenon. Application of both Bloch's theory and the classical modal analysis
at the one-dimensional mass-in-mass lattice is analyzed and corresponding
dispersion relations are derived. The results indicate significant advantages
over the conventional mass-in-a mass lattice, such as broader band-gaps and
increased damping ratio and reveal significant potential in the proposed
solution. Preliminary feasibility analysis for seismic meta-structures and low
frequency acoustic isolation-damping confirm the strong potential and
applicability of this concept.
"
11477,"  The Discrete Truncated Wigner Approximation (DTWA) is a semi-classical phase
space method useful for the exploration of Many-body quantum dynamics. In this
work we investigate Many-Body Localization (MBL) and thermalization using DTWA
and compare its performance to exact numerical solutions. By taking as a
benchmark case a 1D random field Heisenberg spin chain with short range
interactions, and by comparing to numerically exact techniques, we show that
DTWA is able to reproduce dynamical signatures that characterize both the
thermal and the MBL phases. It exhibits the best quantitative agreement at
short times deep in each of the phases and larger mismatches close to the phase
transition. The DTWA captures the logarithmic growth of entanglement in the MBL
phase, even though a pure classical mean-field analysis would lead to no
dynamics at all. Our results suggest the DTWA can become a useful method to
investigate MBL and thermalization in experimentally relevant settings
intractable with exact numerical techniques, such as systems with long range
interactions and/or systems in higher dimensions.
"
3008,"  In this paper we address the problem of electing a committee among a set of
$m$ candidates and on the basis of the preferences of a set of $n$ voters. We
consider the approval voting method in which each voter can approve as many
candidates as she/he likes by expressing a preference profile (boolean
$m$-vector). In order to elect a committee, a voting rule must be established
to `transform' the $n$ voters' profiles into a winning committee. The problem
is widely studied in voting theory; for a variety of voting rules the problem
was shown to be computationally difficult and approximation algorithms and
heuristic techniques were proposed in the literature. In this paper we follow
an Ordered Weighted Averaging approach and study the $k$-sum approval voting
(optimization) problem in the general case $1 \leq k <n$. For this problem we
provide different mathematical programming formulations that allow us to solve
it in an exact solution framework. We provide computational results showing
that our approach is efficient for medium-size test problems ($n$ up to 200,
$m$ up to 60) since in all tested cases it was able to find the exact optimal
solution in very short computational times.
"
9352,"  We study vortex patterns in a prototype nonlinear optical system:
counterpropagating laser beams in a photorefractive crystal, with or without
the background photonic lattice. The vortices are effectively planar and
described by the winding number and the ""flavor"" index, stemming from the fact
that we have two parallel beams propagating in opposite directions. The problem
is amenable to the methods of statistical field theory and generalizes the
Berezinsky-Kosterlitz-Thouless transition of the XY model to the ""two-flavor""
case. In addition to the familiar conductor and insulator phases, we also have
the perfect conductor (vortex proliferation in both beams/""flavors"") and the
frustrated insulator (energy costs of vortex proliferation and vortex
annihilation balance each other). In the presence of disorder in the background
lattice, a novel phase appears which shows long-range correlations and absence
of long-range order, thus being analogous to spin glasses. An important benefit
of this approach is that qualitative behavior of patterns can be known without
intensive numerical work over large areas of the parameter space. More
generally, we would like to draw attention to connections between the
(classical) pattern-forming systems in photorefractive optics and the methods
of (quantum) condensed matter and field theory: on one hand, we use the
field-theoretical methods (renormalization group, replica formalism) to analyze
the patterns; on the other hand, the observed phases are analogous to those
seen in magnetic systems, and make photorefractive optics a fruitful testing
ground for condensed matter systems. As an example, we map our system to a
doped $O(3)$ antiferromagnet with $\mathbb{Z}_2$ defects, which has the same
structure of the phase diagram.
"
17298,"  Pomsets are a model of concurrent computations introduced by Pratt. They can
provide a syntax-oblivious description of semantics of coordination models
based on asynchronous message-passing, such as Message Sequence Charts (MSCs).
In this paper, we study conditions that ensure a specification expressed as a
set of pomsets can be faithfully realised via communicating automata. Our main
contributions are (i) the definition of a realisability condition accounting
for termination soundness, (ii) conditions for global specifications with
""multi-threaded"" participants, and (iii) the definition of realisability
conditions that can be decided directly over pomsets. A positive by-product of
our approach is the efficiency gain in the verification of the realisability
conditions obtained when restricting to specific classes of choreographies
characterisable in term of behavioural types.
"
14983,"  Contributions of the CODALEMA/EXTASIS experiment to the 35th International
Cosmic Ray Conference, 12-20 July 2017, Busan, South Korea.
"
4877,"  We present the results of a Chandra X-ray survey of the 8 most massive galaxy
clusters at z>1.2 in the South Pole Telescope 2500 deg^2 survey. We combine
this sample with previously-published Chandra observations of 49 massive
X-ray-selected clusters at 0<z<0.1 and 90 SZ-selected clusters at 0.25<z<1.2 to
constrain the evolution of the intracluster medium (ICM) over the past ~10 Gyr.
We find that the bulk of the ICM has evolved self similarly over the full
redshift range probed here, with the ICM density at r>0.2R500 scaling like
E(z)^2. In the centers of clusters (r<0.1R500), we find significant deviations
from self similarity (n_e ~ E(z)^{0.1+/-0.5}), consistent with no redshift
dependence. When we isolate clusters with over-dense cores (i.e., cool cores),
we find that the average over-density profile has not evolved with redshift --
that is, cool cores have not changed in size, density, or total mass over the
past ~9-10 Gyr. We show that the evolving ""cuspiness"" of clusters in the X-ray,
reported by several previous studies, can be understood in the context of a
cool core with fixed properties embedded in a self similarly-evolving cluster.
We find no measurable evolution in the X-ray morphology of massive clusters,
seemingly in tension with the rapidly-rising (with redshift) rate of major
mergers predicted by cosmological simulations. We show that these two results
can be brought into agreement if we assume that the relaxation time after a
merger is proportional to the crossing time, since the latter is proportional
to H(z)^(-1).
"
19970,"  We define various height functions for motives over number fields. We compare
these height functions with classical height functions on algebraic varieties,
and also with analogous height functions for variations of Hodge structures on
curves over C. These comparisons provide new questions on motives over number
fields.
"
17222,"  We use 16 quarters of the \textit{Kepler} mission data to analyze the transit
timing variations (TTVs) of the extrasolar planet Kepler-46b (KOI-872). Our
dynamical fits confirm that the TTVs of this planet (period
$P=33.648^{+0.004}_{-0.005}$ days) are produced by a non-transiting planet
Kepler-46c ($P=57.325^{+0.116}_{-0.098}$ days). The Bayesian inference tool
\texttt{MultiNest} is used to infer the dynamical parameters of Kepler-46b and
Kepler-46c. We find that the two planets have nearly coplanar and circular
orbits, with eccentricities $\simeq 0.03$ somewhat higher than previously
estimated. The masses of the two planets are found to be
$M_{b}=0.885^{+0.374}_{-0.343}$ and $M_{c}=0.362^{+0.016}_{-0.016}$ Jupiter
masses, with $M_{b}$ being determined here from TTVs for the first time. Due to
the precession of its orbital plane, Kepler-46c should start transiting its
host star in a few decades from now.
"
11683,"  We revisit the problem of characterizing the eigenvalue distribution of the
Dirichlet-Laplacian on bounded open sets $\Omega\subset\mathbb{R}$ with fractal
boundaries. It is well-known from the results of Lapidus and Pomerance
\cite{LapPo1} that the asymptotic second term of the eigenvalue counting
function can be described in terms of the Minkowski content of the boundary of
$\Omega$ provided it exists. He and Lapidus \cite{HeLap2} discussed a
remarkable extension of this characterization to sets $\Omega$ with boundaries
that are not necessarily Minkowski measurable. They employed so-called
generalized Minkowski contents given in terms of gauge functions more general
than the usual power functions. The class of valid gauge functions in their
theory is characterized by some technical conditions, the geometric meaning and
necessity of which is not obvious. Therefore, it is not completely clear how
general the approach is and which sets $\Omega$ are covered. Here we revisit
these results and put them in the context of regularly varying functions. Using
Karamata theory, it is possible to get rid of most of the technical conditions
and simplify the proofs given by He and Lapidus, revealing thus even more of
the beauty of their results. Further simplifications arise from
characterization results for Minkowski contents obtained in \cite{RW13}. We
hope our new point of view on these spectral problems will initiate some
further investigations of this beautiful theory.
"
4757,"  This paper aims to bridge the affective gap between image content and the
emotional response of the viewer it elicits by using High-Level Concepts
(HLCs). In contrast to previous work that relied solely on low-level features
or used convolutional neural network (CNN) as a black-box, we use HLCs
generated by pretrained CNNs in an explicit way to investigate the
relations/associations between these HLCs and a (small) set of Ekman's
emotional classes. As a proof-of-concept, we first propose a linear admixture
model for modeling these relations, and the resulting computational framework
allows us to determine the associations between each emotion class and certain
HLCs (objects and places). This linear model is further extended to a nonlinear
model using support vector regression (SVR) that aims to predict the viewer's
emotional response using both low-level image features and HLCs extracted from
images. These class-specific regressors are then assembled into a regressor
ensemble that provide a flexible and effective predictor for predicting
viewer's emotional responses from images. Experimental results have
demonstrated that our results are comparable to existing methods, with a clear
view of the association between HLCs and emotional classes that is ostensibly
missing in most existing work.
"
7983,"  We analyze the evolution of Fe XII coronal plasma upflows from the edges of
ten active regions (ARs) as they cross the solar disk using the Hinode Extreme
Ultraviolet Imaging Spectrometer (EIS). Confirming the results of Demoulin et
al. (2013, Sol. Phys. 283, 341), we find that for each AR there is an observed
long term evolution of the upflows which is largely due to the solar rotation
progressively changing the viewpoint of dominantly stationary upflows. From
this projection effect, we estimate the unprojected upflow velocity and its
inclination to the local vertical. AR upflows typically fan away from the AR
core by 40 deg. to near vertical for the following polarity. The span of
inclination angles is more spread for the leading polarity with flows angled
from -29 deg. (inclined towards the AR center) to 28 deg. (directed away from
the AR). In addition to the limb-to-limb apparent evolution, we identify an
intrinsic evolution of the upflows due to coronal activity which is AR
dependent. Further, line widths are correlated with Doppler velocities only for
the few ARs having the largest velocities. We conclude that for the line widths
to be affected by the solar rotation, the spatial gradient of the upflow
velocities must be large enough such that the line broadening exceeds the
thermal line width of Fe XII. Finally, we find that upflows occurring in pairs
or multiple pairs is a common feature of ARs observed by Hinode/EIS, with up to
four pairs present in AR 11575. This is important for constraining the upflow
driving mechanism as it implies that the mechanism is not a local one occurring
over a single polarity. AR upflows originating from reconnection along
quasi-separatrix layers (QSLs) between over-pressure AR loops and neighboring
under-pressure loops is consistent with upflows occurring in pairs, unlike
other proposed mechanisms acting locally in one polarity.
"
20809,"  In this paper, we characterize several lower separation axioms $C_0, C_D$,
$C_R$, $C_N$, $\lambda$-space, nested, $S_{YS}$, $S_{YY}$, $S_{YS}$, and
$S_{\delta}$ using pre-order. To analyze topological properties of (resp.
dynamical systems) foliations, we introduce notions of topology (resp.
dynamical systems) for foliations. Then proper (resp. compact, minimal,
recurrent) foliations are characterized by separation axioms. Conversely, lower
separation axioms are interpreted into the condition for foliations and several
relations of them are described. Moreover, we introduce some notions for
topologies from dynamical systems and foliation theory.
"
5906,"  Storage and transmission in big data are discussed in this paper, where
message importance is taken into account. Similar to Shannon Entropy and Renyi
Entropy, we define non-parametric message important measure (NMIM) as a measure
for the message importance in the scenario of big data, which can characterize
the uncertainty of random events. It is proved that the proposed NMIM can
sufficiently describe two key characters of big data: rare events finding and
large diversities of events. Based on NMIM, we first propose an effective
compressed encoding mode for data storage, and then discuss the channel
transmission over some typical channel models. Numerical simulation results
show that using our proposed strategy occupies less storage space without
losing too much message importance, and there are growth region and saturation
region for the maximum transmission, which contributes to designing of better
practical communication system.
"
15366,"  A program schema defines a class of programs, all of which have identical
statement structure, but whose functions and predicates may differ. A schema
thus defines an entire class of programs according to how its symbols are
interpreted. A subschema of a schema is obtained from a schema by deleting some
of its statements. We prove that given a schema $S$ which is predicate-linear,
free and liberal, such that the true and false parts of every if predicate
satisfy a simple additional condition, and a slicing criterion defined by the
final value of a given variable after execution of any program defined by $S$,
the minimal subschema of $S$ which respects this slicing criterion contains all
the function and predicate symbols `needed' by the variable according to the
data dependence and control dependence relations used in program slicing, which
is the symbol set given by Weiser's static slicing algorithm. Thus this
algorithm gives predicate-minimal slices for classes of programs represented by
schemas satisfying our set of conditions. We also give an example to show that
the corresponding result with respect to the slicing criterion defined by
termination behaviour is incorrect. This complements a result by the authors in
which $S$ was required to be function-linear, instead of predicate-linear.
"
3487,"  We study Brauer's long-standing $k(B)$-conjecture on the number of characters
in $p$-blocks for finite quasi-simple groups and show that their blocks do not
occur as a minimal counterexample for $p\ge5$ nor in the case of abelian
defect. For $p=3$ we obtain that the principal 3-blocks do not provide minimal
counterexamples. We also determine the precise number of irreducible characters
in unipotent blocks of classical groups for odd primes.
"
64,"  Here we reveal details of the interaction between human lysozyme proteins,
both native and fibrils, and their water environment by intense terahertz time
domain spectroscopy. With the aid of a rigorous dielectric model, we determine
the amplitude and phase of the oscillating dipole induced by the THz field in
the volume containing the protein and its hydration water. At low
concentrations, the amplitude of this induced dipolar response decreases with
increasing concentration. Beyond a certain threshold, marking the onset of the
interactions between the extended hydration shells, the amplitude remains fixed
but the phase of the induced dipolar response, which is initially in phase with
the applied THz field, begins to change. The changes observed in the THz
response reveal protein-protein interactions me-diated by extended hydration
layers, which may control fibril formation and may have an important role in
chemical recognition phenomena.
"
18170,"  Private information retrieval (PIR) protocols make it possible to retrieve a
file from a database without disclosing any information about the identity of
the file being retrieved. These protocols have been rigorously explored from an
information-theoretic perspective in recent years. While existing protocols
strictly impose that no information is leaked on the file's identity, this work
initiates the study of the tradeoffs that can be achieved by relaxing the
requirement of perfect privacy. In case the user is willing to leak some
information on the identity of the retrieved file, we study how the PIR rate,
as well as the upload cost and access complexity, can be improved. For the
particular case of replicated servers, we propose two weakly-private
information retrieval schemes based on two recent PIR protocols and a family of
schemes based on partitioning. Lastly, we compare the performance of the
proposed schemes.
"
4631,"  We report on the optical and mechanical characterization of arrays of
parallel micromechanical membranes. Pairs of high-tensile stress, 100 nm-thick
silicon nitride membranes are assembled parallel with each other with
separations ranging from 8.5 to 200 $\mu$m. Their optical properties are
accurately determined using a combination of broadband and monochromatic
illuminations and the lowest vibrational mode frequencies and mechanical
quality factors are determined interferometrically. The results and techniques
demonstrated are promising for investigations of collective phenomena in
optomechanical arrays.
"
18166,"  This paper considers the scenario that multiple data owners wish to apply a
machine learning method over the combined dataset of all owners to obtain the
best possible learning output but do not want to share the local datasets owing
to privacy concerns. We design systems for the scenario that the stochastic
gradient descent (SGD) algorithm is used as the machine learning method because
SGD (or its variants) is at the heart of recent deep learning techniques over
neural networks. Our systems differ from existing systems in the following
features: {\bf (1)} any activation function can be used, meaning that no
privacy-preserving-friendly approximation is required; {\bf (2)} gradients
computed by SGD are not shared but the weight parameters are shared instead;
and {\bf (3)} robustness against colluding parties even in the extreme case
that only one honest party exists. We prove that our systems, while
privacy-preserving, achieve the same learning accuracy as SGD and hence retain
the merit of deep learning with respect to accuracy. Finally, we conduct
several experiments using benchmark datasets, and show that our systems
outperform previous system in terms of learning accuracies.
"
4931,"  Path planning in robotics often requires finding high-quality solutions to
continuously valued and/or high-dimensional problems. These problems are
challenging and most planning algorithms instead solve simplified
approximations. Popular approximations include graphs and random samples, as
respectively used by informed graph-based searches and anytime sampling-based
planners. Informed graph-based searches, such as A*, traditionally use
heuristics to search a priori graphs in order of potential solution quality.
This makes their search efficient but leaves their performance dependent on the
chosen approximation. If its resolution is too low then they may not find a
(suitable) solution but if it is too high then they may take a prohibitively
long time to do so. Anytime sampling-based planners, such as RRT*,
traditionally use random sampling to approximate the problem domain
incrementally. This allows them to increase resolution until a suitable
solution is found but makes their search dependent on the order of
approximation. Arbitrary sequences of random samples expand the approximation
in every direction and fill the problem domain but may be prohibitively
inefficient at containing a solution. This paper unifies and extends these two
approaches to develop Batch Informed Trees (BIT*), an informed, anytime
sampling-based planner. BIT* solves continuous path planning problems
efficiently by using sampling and heuristics to alternately approximate and
search the problem domain. Its search is ordered by potential solution quality,
as in A*, and its approximation improves indefinitely with additional
computational time, as in RRT*. It is shown analytically to be almost-surely
asymptotically optimal and experimentally to outperform existing sampling-based
planners, especially on high-dimensional planning problems.
"
5536,"  In a standard bifurcation of a dynamical system, the stationary points (or
more generally attractors) change qualitatively when varying a control
parameter. Here we describe a novel unusual effect, when the change of a
parameter, e.g. a growth rate, does not influence the stationary states, but
nevertheless leads to a qualitative change of dynamics. For instance, such a
dynamic transition can be between the convergence to a stationary state and a
strong increase without stationary states, or between the convergence to one
stationary state and that to a different state. This effect is illustrated for
a dynamical system describing two symbiotic populations, one of which exhibits
a growth rate larger than the other one. We show that, although the stationary
states of the dynamical system do not depend on the growth rates, the latter
influence the boundary of the basins of attraction. This change of the basins
of attraction explains this unusual effect of the quantitative change of
dynamics by growth rate variation.
"
8045,"  Generative adversarial networks (GANs) transform latent vectors into visually
plausible images. It is generally thought that the original GAN formulation
gives no out-of-the-box method to reverse the mapping, projecting images back
into latent space. We introduce a simple, gradient-based technique called
stochastic clipping. In experiments, for images generated by the GAN, we
precisely recover their latent vector pre-images 100% of the time. Additional
experiments demonstrate that this method is robust to noise. Finally, we show
that even for unseen images, our method appears to recover unique encodings.
"
4841,"  Modern neural networks tend to be overconfident on unseen, noisy or
incorrectly labelled data and do not produce meaningful uncertainty measures.
Bayesian deep learning aims to address this shortcoming with variational
approximations (such as Bayes by Backprop or Multiplicative Normalising Flows).
However, current approaches have limitations regarding flexibility and
scalability. We introduce Bayes by Hypernet (BbH), a new method of variational
approximation that interprets hypernetworks as implicit distributions. It
naturally uses neural networks to model arbitrarily complex distributions and
scales to modern deep learning architectures. In our experiments, we
demonstrate that our method achieves competitive accuracies and predictive
uncertainties on MNIST and a CIFAR5 task, while being the most robust against
adversarial attacks.
"
16848,"  Data warehouse performance is usually achieved through physical data
structures such as indexes or materialized views. In this context, cost models
can help select a relevant set ofsuch performance optimization structures.
Nevertheless, selection becomes more complex in the cloud. The criterion to
optimize is indeed at least two-dimensional, with monetary cost balancing
overall query response time. This paper introduces new cost models that fit
into the pay-as-you-go paradigm of cloud computing. Based on these cost models,
an optimization problem is defined to discover, among candidate views, those to
be materialized to minimize both the overall cost of using and maintaining the
database in a public cloud and the total response time ofa given query
workload. We experimentally show that maintaining materialized views is always
advantageous, both in terms of performance and cost.
"
10618,"  One of the essential prerequisites for detection of Earth-like extra-solar
planets or direct measurements of the cosmological expansion is the accurate
and precise wavelength calibration of astronomical spectrometers. It has
already been realized that the large number of exactly known optical
frequencies provided by laser frequency combs ('astrocombs') can significantly
surpass conventionally used hollow-cathode lamps as calibration light sources.
A remaining challenge, however, is generation of frequency combs with lines
resolvable by astronomical spectrometers. Here we demonstrate an astrocomb
generated via soliton formation in an on-chip microphotonic resonator
('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is
providing wavelength calibration on the 10 cm/s radial velocity level on the
GIANO-B high-resolution near-infrared spectrometer. As such, microresonator
frequency combs have the potential of providing broadband wavelength
calibration for the next-generation of astronomical instruments in
planet-hunting and cosmological research.
"
14100,"  We make the case for studying the complexity of approximately simulating
(sampling) quantum systems for reasons beyond that of quantum computational
supremacy, such as diagnosing phase transitions. We consider the sampling
complexity as a function of time $t$ due to evolution generated by spatially
local quadratic bosonic Hamiltonians. We obtain an upper bound on the scaling
of $t$ with the number of bosons $n$ for which approximate sampling is
classically efficient. We also obtain a lower bound on the scaling of $t$ with
$n$ for which any instance of the boson sampling problem reduces to this
problem and hence implies that the problem is hard, assuming the conjectures of
Aaronson and Arkhipov [Proc. 43rd Annu. ACM Symp. Theory Comput. STOC '11].
This establishes a dynamical phase transition in sampling complexity. Further,
we show that systems in the Anderson-localized phase are always easy to sample
from at arbitrarily long times. We view these results in the light of
classifying phases of physical systems based on parameters in the Hamiltonian.
In doing so, we combine ideas from mathematical physics and computational
complexity to gain insight into the behavior of condensed matter, atomic,
molecular and optical systems.
"
358,"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
"
16185,"  Many organisms repartition their proteome in a circadian fashion in response
to the daily nutrient changes in their environment. A striking example is
provided by cyanobacteria, which perform photosynthesis during the day to fix
carbon. These organisms not only face the challenge of rewiring their proteome
every 12 hours, but also the necessity of storing the fixed carbon in the form
of glycogen to fuel processes during the night. In this manuscript, we extend
the framework developed by Hwa and coworkers (Scott et al., Science 330, 1099
(2010)) for quantifying the relatinship between growth and proteome composition
to circadian metabolism. We then apply this framework to investigate the
circadian metabolism of the cyanobacterium Cyanothece, which not only fixes
carbon during the day, but also nitrogen during the night, storing it in the
polymer cyanophycin. Our analysis reveals that the need to store carbon and
nitrogen tends to generate an extreme growth strategy, in which the cells
predominantly grow during the day, as observed experimentally. This strategy
maximizes the growth rate over 24 hours, and can be quantitatively understood
by the bacterial growth laws. Our analysis also shows that the slow relaxation
of the proteome, arising from the slow growth rate, puts a severe constraint on
implementing this optimal strategy. Yet, the capacity to estimate the time of
the day, enabled by the circadian clock, makes it possible to anticipate the
daily changes in the environment and mount a response ahead of time. This
significantly enhances the growth rate by counteracting the detrimental effects
of the slow proteome relaxation.
"
20202,"  We study the critical behavior of a general contagion model where nodes are
either active (e.g. with opinion A, or functioning) or inactive (e.g. with
opinion B, or damaged). The transitions between these two states are determined
by (i) spontaneous transitions independent of the neighborhood, (ii)
transitions induced by neighboring nodes and (iii) spontaneous reverse
transitions. The resulting dynamics is extremely rich including limit cycles
and random phase switching. We derive a unifying mean-field theory.
Specifically, we analytically show that the critical behavior of systems whose
dynamics is governed by processes (i-iii) can only exhibit three distinct
regimes: (a) uncorrelated spontaneous transition dynamics (b) contact process
dynamics and (c) cusp catastrophes. This ends a long-standing debate on the
universality classes of complex contagion dynamics in mean-field and
substantially deepens its mathematical understanding.
"
20290,"  Many approaches for testing configurable software systems start from the same
assumption: it is impossible to test all configurations. This motivated the
definition of variability-aware abstractions and sampling techniques to cope
with large configuration spaces. Yet, there is no theoretical barrier that
prevents the exhaustive testing of all configurations by simply enumerating
them, if the effort required to do so remains acceptable. Not only this: we
believe there is lots to be learned by systematically and exhaustively testing
a configurable system. In this case study, we report on the first ever
endeavour to test all possible configurations of an industry-strength, open
source configurable software system, JHipster, a popular code generator for web
applications. We built a testing scaffold for the 26,000+ configurations of
JHipster using a cluster of 80 machines during 4 nights for a total of 4,376
hours (182 days) CPU time. We find that 35.70% configurations fail and we
identify the feature interactions that cause the errors. We show that sampling
strategies (like dissimilarity and 2-wise): (1) are more effective to find
faults than the 12 default configurations used in the JHipster continuous
integration; (2) can be too costly and exceed the available testing budget. We
cross this quantitative analysis with the qualitative assessment of JHipster's
lead developers.
"
3351,"  Complex computer simulators are increasingly used across fields of science as
generative models tying parameters of an underlying theory to experimental
observations. Inference in this setup is often difficult, as simulators rarely
admit a tractable density or likelihood function. We introduce Adversarial
Variational Optimization (AVO), a likelihood-free inference algorithm for
fitting a non-differentiable generative model incorporating ideas from
generative adversarial networks, variational optimization and empirical Bayes.
We adapt the training procedure of generative adversarial networks by replacing
the differentiable generative network with a domain-specific simulator. We
solve the resulting non-differentiable minimax problem by minimizing
variational upper bounds of the two adversarial objectives. Effectively, the
procedure results in learning a proposal distribution over simulator
parameters, such that the JS divergence between the marginal distribution of
the synthetic data and the empirical distribution of observed data is
minimized. We evaluate and compare the method with simulators producing both
discrete and continuous data.
"
6120,"  We study the dispersion of a point set, a notion closely related to the
discrepancy. Given a real $r\in (0,1)$ and an integer $d\geq 2$, let $N(r,d)$
denote the minimum number of points inside the $d$-dimensional unit cube
$[0,1]^d$ such that they intersect every axis-aligned box inside $[0,1]^d$ of
volume greater than $r$. We prove an upper bound on $N(r,d)$, matching a lower
bound of Aistleitner et al. up to a multiplicative constant depending only on
$r$. This fully determines the rate of growth of $N(r,d)$ if $r\in(0,1)$ is
fixed.
"
16047,"  We present a multi-wavelength compilation of new and previously-published
photometry for 55 Galactic field RR Lyrae variables. Individual studies,
spanning a time baseline of up to 30 years, are self-consistently phased to
produce light curves in 10 photometric bands covering the wavelength range from
0.4 to 4.5 microns. Data smoothing via the GLOESS technique is described and
applied to generate high-fidelity light curves, from which mean magnitudes,
amplitudes, rise-times, and times of minimum and maximum light are derived.
60,000 observations were acquired using the new robotic Three-hundred
MilliMeter Telescope (TMMT), which was first deployed at the Carnegie
Observatories in Pasadena, CA, and is now permanently installed and operating
at Las Campanas Observatory in Chile. We provide a full description of the TMMT
hardware, software, and data reduction pipeline. Archival photometry
contributed approximately 31,000 observations. Photometric data are given in
the standard Johnson UBV, Kron-Cousins RI, 2MASS JHK, and Spitzer [3.6] & [4.5]
bandpasses.
"
20600,"  In this paper, we propose a quality enhancement network for Versatile Video
Coding (VVC) compressed videos by jointly exploiting spatial details and
temporal structure (SDTS). The network consists of a temporal structure
prediction subnet and a spatial detail enhancement subnet. The former subnet is
used to estimate and compensate the temporal motion across frames, and the
spatial detail subnet is used to reduce the compression artifacts and enhance
the reconstruction quality of the VVC compressed video. Experimental results
demonstrate the effectiveness of our SDTS-based approach. It offers over
7.82$\%$ BD-rate saving on the common test video sequences and achieves the
state-of-the-art performance.
"
9584,"  We study the notion of consistency between a 3D shape and a 2D observation
and propose a differentiable formulation which allows computing gradients of
the 3D shape given an observation from an arbitrary view. We do so by
reformulating view consistency using a differentiable ray consistency (DRC)
term. We show that this formulation can be incorporated in a learning framework
to leverage different types of multi-view observations e.g. foreground masks,
depth, color images, semantics etc. as supervision for learning single-view 3D
prediction. We present empirical analysis of our technique in a controlled
setting. We also show that this approach allows us to improve over existing
techniques for single-view reconstruction of objects from the PASCAL VOC
dataset.
"
13703,"  We use the ab initio Bethe Ansatz dynamics to predict the dissociation of
one-dimensional cold-atom breathers that are created by a quench from a
fundamental soliton. We find that the dissociation is a robust quantum
many-body effect, while in the mean-field (MF) limit the dissociation is
forbidden by the integrability of the underlying nonlinear Schrödinger
equation. The analysis demonstrates the possibility to observe quantum
many-body effects without leaving the MF range of experimental parameters. We
find that the dissociation time is of the order of a few seconds for a typical
atomic-soliton setting.
"
9655,"  A map $f\colon K\to \mathbb R^d$ of a simplicial complex is an almost
embedding if $f(\sigma)\cap f(\tau)=\emptyset$ whenever $\sigma,\tau$ are
disjoint simplices of $K$.
Theorem. Fix integers $d,k\ge2$ such that $d=\frac{3k}2+1$.
(a) Assume that $P\ne NP$. Then there exists a finite $k$-dimensional complex
$K$ that does not admit an almost embedding in $\mathbb R^d$ but for which
there exists an equivariant map $\tilde K\to S^{d-1}$.
(b) The algorithmic problem of recognition almost embeddability of finite
$k$-dimensional complexes in $\mathbb R^d$ is NP hard.
The proof is based on the technique from the Matoušek-Tancer-Wagner paper
(proving an analogous result for embeddings), and on singular versions of the
higher-dimensional Borromean rings lemma and a generalized van Kampen--Flores
theorem.
"
10977,"  We develop a linear algebraic framework for the shape-from-shading problem,
because tensors arise when scalar (e.g. image) and vector (e.g. surface normal)
fields are differentiated multiple times. The work is in two parts. In this
first part we investigate when image derivatives exhibit invariance to changing
illumination by calculating the statistics of image derivatives under general
distributions on the light source. We computationally validate the hypothesis
that image orientations (derivatives) provide increased invariance to
illumination by showing (for a Lambertian model) that a shape-from-shading
algorithm matching gradients instead of intensities provides more accurate
reconstructions when illumination is incorrectly estimated under a flatness
prior.
"
9973,"  Exploratory testing is neither black nor white, but rather a continuum of
exploration exists. In this research we propose an approach for decision
support helping practitioners to distribute time between different degrees of
exploratory testing on that continuum. To make the continuum manageable, five
levels have been defined: freestyle testing, high, medium and low degrees of
exploration, and scripted testing. The decision support approach is based on
the repertory grid technique. The approach has been used in one company. The
method for data collection was focus groups. The results showed that the
proposed approach aids practitioners in the reflection of what exploratory
testing levels to use, and aligns their understanding for priorities of
decision criteria and the performance of exploratory testing levels in their
contexts. The findings also showed that the participating company, which is
currently conducting mostly scripted testing, should spend more time on testing
using higher degrees of exploration in comparison to scripted testing.
"
3860,"  In this paper we describe two fully mass conservative, energy stable, finite
difference methods on a staggered grid for the quasi-incompressible
Navier-Stokes-Cahn-Hilliard (q-NSCH) system governing a binary incompressible
fluid flow with variable density and viscosity. Both methods, namely the
primitive method (finite difference method in the primitive variable
formulation) and the projection method (finite difference method in a
projection-type formulation), are so designed that the mass of the binary fluid
is preserved, and the energy of the system equations is always non-increasing
in time at the fully discrete level. We also present an efficient, practical
nonlinear multigrid method - comprised of a standard FAS method for the
Cahn-Hilliard equation, and a method based on the Vanka-type smoothing strategy
for the Navier-Stokes equation - for solving these equations. We test the
scheme in the context of Capillary Waves, rising droplets and Rayleigh-Taylor
instability. Quantitative comparisons are made with existing analytical
solutions or previous numerical results that validate the accuracy of our
numerical schemes. Moreover, in all cases, mass of the single component and the
binary fluid was conserved up to 10 to -8 and energy decreases in time.
"
2151,"  Motivated by applications that arise in online social media and collaboration
networks, there has been a lot of work on community-search and team-formation
problems. In the former class of problems, the goal is to find a subgraph that
satisfies a certain connectivity requirement and contains a given collection of
seed nodes. In the latter class of problems, on the other hand, the goal is to
find individuals who collectively have the skills required for a task and form
a connected subgraph with certain properties.
In this paper, we extend both the community-search and the team-formation
problems by associating each individual with a profile. The profile is a
numeric score that quantifies the position of an individual with respect to a
topic. We adopt a model where each individual starts with a latent profile and
arrives to a conformed profile through a dynamic conformation process, which
takes into account the individual's social interaction and the tendency to
conform with one's social environment. In this framework, social tension arises
from the differences between the conformed profiles of neighboring individuals
as well as from differences between individuals' conformed and latent profiles.
Given a network of individuals, their latent profiles and this conformation
process, we extend the community-search and the team-formation problems by
requiring the output subgraphs to have low social tension. From the technical
point of view, we study the complexity of these problems and propose algorithms
for solving them effectively. Our experimental evaluation in a number of social
networks reveals the efficacy and efficiency of our methods.
"
380,"  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed
bands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a
tentative assignment is given to the 2-photon transition from $X$ to the
n=12-13 $[X^{2}{\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate
the IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been
identified as a molecular candidate amenable to laser control. Our work allows
us to identify an efficient method for loading cold SiO$^{+}$ from an ablated
sample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.
"
20512,"  Wireless engineers and business planners commonly raise the question on
where, when, and how millimeter-wave (mmWave) will be used in 5G and beyond.
Since the next generation network is not just a new radio access standard, but
instead an integration of networks for vertical markets with diverse
applications, answers to the question depend on scenarios and use cases to be
deployed. This paper gives four 5G mmWave deployment examples and describes in
chronological order the scenarios and use cases of their probable deployment,
including expected system architectures and hardware prototypes. The paper
starts with 28 GHz outdoor backhauling for fixed wireless access and moving
hotspots, which will be demonstrated at the PyeongChang winter Olympic games in
2018. The second deployment example is a 60 GHz unlicensed indoor access system
at the Tokyo-Narita airport, which is combined with Mobile Edge Computing (MEC)
to enable ultra-high speed content download with low latency. The third example
is mmWave mesh network to be used as a micro Radio Access Network ({\mu}-RAN),
for cost-effective backhauling of small-cell Base Stations (BSs) in dense urban
scenarios. The last example is mmWave based Vehicular-to-Vehicular (V2V) and
Vehicular-to-Everything (V2X) communications system, which enables automated
driving by exchanging High Definition (HD) dynamic map information between cars
and Roadside Units (RSUs). For 5G and beyond, mmWave and MEC will play
important roles for a diverse set of applications that require both ultra-high
data rate and low latency communications.
"
17806,"  Existing urban boundaries are usually defined by government agencies for
administrative, economic, and political purposes. Defining urban boundaries
that consider socio-economic relationships and citizen commute patterns is
important for many aspects of urban and regional planning. In this paper, we
describe a method to delineate urban boundaries based upon human interactions
with physical space inferred from social media. Specifically, we depicted the
urban boundaries of Great Britain using a mobility network of Twitter user
spatial interactions, which was inferred from over 69 million geo-located
tweets. We define the non-administrative anthropographic boundaries in a
hierarchical fashion based on different physical movement ranges of users
derived from the collective mobility patterns of Twitter users in Great
Britain. The results of strongly connected urban regions in the form of
communities in the network space yield geographically cohesive, non-overlapping
urban areas, which provide a clear delineation of the non-administrative
anthropographic urban boundaries of Great Britain. The method was applied to
both national (Great Britain) and municipal scales (the London metropolis).
While our results corresponded well with the administrative boundaries, many
unexpected and interesting boundaries were identified. Importantly, as the
depicted urban boundaries exhibited a strong instance of spatial proximity, we
employed a gravity model to understand the distance decay effects in shaping
the delineated urban boundaries. The model explains how geographical distances
found in the mobility patterns affect the interaction intensity among different
non-administrative anthropographic urban areas, which provides new insights
into human spatial interactions with urban space.
"
2545,"  Distribution of cold gas in the post-reionization era provides an important
link between distribution of galaxies and the process of star formation.
Redshifted 21 cm radiation from the Hyperfine transition of neutral Hydrogen
allows us to probe the neutral component of cold gas, most of which is to be
found in the interstellar medium of galaxies. Existing and upcoming radio
telescopes can probe the large scale distribution of neutral Hydrogen via HI
intensity mapping. In this paper we use an estimate of the HI power spectrum
derived using an ansatz to compute the expected signal from the large scale HI
distribution at z ~ 3. We find that the scale dependence of bias at small
scales makes a significant difference to the expected signal even at large
angular scales. We compare the predicted signal strength with the sensitivity
of radio telescopes that can observe such radiation and calculate the
observation time required for detecting neutral Hydrogen at these redshifts. We
find that OWFA (Ooty Wide Field Array) offers the best possibility to detect
neutral Hydrogen at z ~ 3 before the SKA (Square Kilometer Array) becomes
operational. We find that the OWFA should be able to make a 3 sigma or a more
significant detection in 2000 hours of observations at several angular scales.
Calculations done using the Fisher matrix approach indicate that a 5 sigma
detection of the binned HI power spectrum via measurement of the amplitude of
the HI power spectrum is possible in 1000 hours (Sarkar, Bharadwaj and Ali,
2017).
"
17398,"  Nonparametric kernel density estimation is a very natural procedure which
simply makes use of the smoothing power of the convolution operation. Yet, it
performs poorly when the density of a positive variable is to be estimated
(boundary issues, spurious bumps in the tail). So various extensions of the
basic kernel estimator allegedly suitable for $\mathbb{R}^+$-supported
densities, such as those using Gamma or other asymmetric kernels, abound in the
literature. Those, however, are not based on any valid smoothing operation
analogous to the convolution, which typically leads to inconsistencies. By
contrast, in this paper a kernel estimator for $\mathbb{R}^+$-supported
densities is defined by making use of the Mellin convolution, the natural
analogue of the usual convolution on $\mathbb{R}^+$. From there, a very
transparent theory flows and leads to new type of asymmetric kernels strongly
related to Meijer's $G$-functions. The numerous pleasant properties of this
`Mellin-Meijer-kernel density estimator' are demonstrated in the paper. Its
pointwise and $L_2$-consistency (with optimal rate of convergence) is
established for a large class of densities, including densities unbounded at 0
and showing power-law decay in their right tail. Its practical behaviour is
investigated further through simulations and some real data analyses.
"
14011,"  Time crystals are quantum many-body systems which, due to interactions
between particles, are able to spontaneously self-organize their motion in a
periodic way in time by analogy with the formation of crystalline structures in
space in condensed matter physics. In solid state physics properties of space
crystals are often investigated with the help of external potentials that are
spatially periodic and reflect various crystalline structures. A similar
approach can be applied for time crystals, as periodically driven systems
constitute counterparts of spatially periodic systems, but in the time domain.
Here we show that condensed matter problems ranging from single particles in
potentials of quasi-crystal structure to many-body systems with exotic
long-range interactions can be realized in the time domain with an appropriate
periodic driving. Moreover, it is possible to create molecules where atoms are
bound together due to destructive interference if the atomic scattering length
is modulated in time.
"
16166,"  This paper introduces a new member of the family of Variational Autoencoders
(VAE) that constrains the rate of information transferred by the latent layer.
The latent layer is interpreted as a communication channel, the information
rate of which is bound by imposing a pre-set signal-to-noise ratio. The new
constraint subsumes the mutual information between the input and latent
variables, combining naturally with the likelihood objective of the observed
data as used in a conventional VAE. The resulting Bounded-Information-Rate
Variational Autoencoder (BIR-VAE) provides a meaningful latent representation
with an information resolution that can be specified directly in bits by the
system designer. The rate constraint can be used to prevent overtraining, and
the method naturally facilitates quantisation of the latent variables at the
set rate. Our experiments confirm that the BIR-VAE has a meaningful latent
representation and that its performance is at least as good as state-of-the-art
competing algorithms, but with lower computational complexity.
"
17152,"  Recently, the educational initiative TED-Ed has published a popular brain
teaser coined the 'frog riddle', which illustrates non-intuitive implications
of conditional probabilities. In its intended form, the frog riddle is a
reformulation of the classic boy-girl paradox. However, the authors alter the
narrative of the riddle in a form, that subtly changes the way information is
conveyed. The presented solution, unfortunately, does not take this point into
full account, and as a consequence, lacks consistency in the sense that
different parts of the problem are treated on unequal footing. We here review,
how the mechanism of receiving information matters, and why this is exactly the
reason that such kind of problems challenge intuitive thinking. Subsequently,
we present a generalized solution, that accounts for the above difficulties,
and preserves full logical consistency. Eventually, the relation to the
boy-girl paradox is discussed.
"
12527,"  This paper addresses the problem of decentralized tube-based nonlinear Model
Predictive Control (NMPC) for a class of uncertain nonlinear continuous-time
multi-agent systems with additive and bounded disturbance. In particular, the
problem of robust navigation of a multi-agent system to predefined states of
the workspace while using only local information is addressed, under certain
distance and control input constraints. We propose a decentralized feedback
control protocol that consists of two terms: a nominal control input, which is
computed online and is the outcome of a Decentralized Finite Horizon Optimal
Control Problem (DFHOCP) that each agent solves at every sampling time, for its
nominal system dynamics; and an additive state feedback law which is computed
offline and guarantees that the real trajectories of each agent will belong to
a hyper-tube centered along the nominal trajectory, for all times. The volume
of the hyper-tube depends on the upper bound of the disturbances as well as the
bounds of the derivatives of the dynamics. In addition, by introducing certain
distance constraints, the proposed scheme guarantees that the initially
connected agents remain connected for all times. Under standard assumptions
that arise in nominal NMPC schemes, controllability assumptions as well as
communication capabilities between the agents, we guarantee that the
multi-agent system is ISS (Input to State Stable) with respect to the
disturbances, for all initial conditions satisfying the state constraints.
Simulation results verify the correctness of the proposed framework.
"
2308,"  In this paper, we consider the cubic fourth-order nonlinear Schrödinger
equation (4NLS) under the periodic boundary condition. We prove two results.
One is the local well-posedness in $H^s$ with $-1/3 \le s < 0$ for the Cauchy
problem of the Wick ordered 4NLS. The other one is the non-squeezing property
for the flow map of 4NLS in the symplectic phase space $L^2(\mathbb{T})$. To
prove the former we used the ideas introduced in [Takaoka and Tsutsumi 2004]
and [Nakanish et al 2010], and to prove the latter we used the ideas in
[Colliander et al 2005].
"
16159,"  The complexity of knowledge production on complex systems is well-known, but
there still lacks knowledge framework that would both account for a certain
structure of knowledge production at an epistemological level and be directly
applicable to the study and management of complex systems. We set a basis for
such a framework, by first analyzing in detail a case study of the construction
of a geographical theory of complex territorial systems, through mixed methods,
namely qualitative interview analysis and quantitative citation network
analysis. We can therethrough inductively build a framework that considers
knowledge entreprises as perspectives, with co-evolving components within
complementary knowledge domains. We finally discuss potential applications and
developments.
"
12529,"  In this paper we use refined approximations for Chebyshev's
$\vartheta$-function to establish new explicit estimates for the prime counting
function $\pi(x)$, which improve the current best estimates for large values of
$x$. As an application we find an upper bound for the number $H_0$ which is
defined to be the smallest positive integer so that Ramanujan's prime counting
inequality holds for every $x \geq H_0$.
"
7971,"  This manuscript proposes a novel empirical Bayes technique for regularizing
regression coefficients in predictive models. When predictions from a
previously published model are available, this empirical Bayes method provides
a natural mathematical framework for shrinking coefficients toward the
estimates implied by the body of existing research rather than the shrinkage
toward zero provided by traditional L1 and L2 penalization schemes. The method
is applied to two different prediction problems. The first involves the
construction of a model for predicting whether a single nucleotide polymorphism
(SNP) of the KCNQ1 gene will result in dysfunction of the corresponding voltage
gated ion channel. The second involves the prediction of preoperative serum
creatinine change in patients undergoing cardiac surgery.
"
20930,"  Due to their simplicity and excellent performance, parallel asynchronous
variants of stochastic gradient descent have become popular methods to solve a
wide range of large-scale optimization problems on multi-core architectures.
Yet, despite their practical success, support for nonsmooth objectives is still
lacking, making them unsuitable for many problems of interest in machine
learning, such as the Lasso, group Lasso or empirical risk minimization with
convex constraints.
In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse
method inspired by SAGA, a variance reduced incremental gradient algorithm. The
proposed method is easy to implement and significantly outperforms the state of
the art on several nonsmooth, large-scale problems. We prove that our method
achieves a theoretical linear speedup with respect to the sequential version
under assumptions on the sparsity of gradients and block-separability of the
proximal term. Empirical benchmarks on a multi-core architecture illustrate
practical speedups of up to 12x on a 20-core machine.
"
6474,"  This note deals with certain properties of convex functions. We provide
results on the convexity of the set of minima of these functions, the behaviour
of their subgradient set under restriction, and optimization of these functions
over an affine subspace.
"
1235,"  We study the heavy path decomposition of conditional Galton-Watson trees. In
a standard Galton-Watson tree conditional on its size $n$, we order all
children by their subtree sizes, from large (heavy) to small. A node is marked
if it is among the $k$ heaviest nodes among its siblings. Unmarked nodes and
their subtrees are removed, leaving only a tree of marked nodes, which we call
the $k$-heavy tree. We study various properties of these trees, including their
size and the maximal distance from any original node to the $k$-heavy tree. In
particular, under some moment condition, the $2$-heavy tree is with high
probability larger than $cn$ for some constant $c > 0$, and the maximal
distance from the $k$-heavy tree is $O(n^{1/(k+1)})$ in probability. As a
consequence, for uniformly random Apollonian networks of size $n$, the expected
size of the longest simple path is $\Omega(n)$.
"
3823,"  OpenML is an online machine learning platform where researchers can easily
share data, machine learning tasks and experiments as well as organize them
online to work and collaborate more efficiently. In this paper, we present an R
package to interface with the OpenML platform and illustrate its usage in
combination with the machine learning R package mlr. We show how the OpenML
package allows R users to easily search, download and upload data sets and
machine learning tasks. Furthermore, we also show how to upload results of
experiments, share them with others and download results from other users.
Beyond ensuring reproducibility of results, the OpenML platform automates much
of the drudge work, speeds up research, facilitates collaboration and increases
the users' visibility online.
"
17876,"  In this work we investigate the dynamics of the nonlinear DDE
(delay-differential equation)
x''(t)+x(t-T)+x(t)^3=0
where T is the delay. For T=0 this system is conservative and exhibits no
limit cycles. For T>0, no matter how small, an infinite number of limit cycles
exist, their amplitudes going to infinity in the limit as T approaches zero.
We investigate this situation in three ways: 1) Harmonic Balance, 2)
Melnikov's integral, and 3) Adding damping to regularize the singularity.
"
17280,"  The paper presents two results. First it is shown how the discrete potential
modified KdV equation and its Lax pairs in matrix form arise from the
Hirota-Miwa equation by a 2-periodic reduction. Then Darboux transformations
and binary Darboux transformations are derived for the discrete potential
modified KdV equation and it is shown how these may be used to construct exact
solutions.
"
16759,"  We establish interior Lipschitz estimates at the macroscopic scale for
solutions to systems of linear elasticity with rapidly oscillating periodic
coefficients and mixed boundary conditions in domains periodically perforated
at a microscopic scale $\varepsilon$ by establishing $H^1$-convergence rates
for such solutions. The interior estimates are derived directly without the use
of compactness via an argument presented in [3] that was adapted for elliptic
equations in [2] and [11]. As a consequence, we derive a Liouville type
estimate for solutions to the systems of linear elasticity in unbounded
periodically perforated domains.
"
15907,"  We determine the joint limiting distribution of adjacent spacings around a
central, intermediate, or an extreme order statistic $X_{k:n}$ of a random
sample of size $n$ from a continuous distribution $F$. For central and
intermediate cases, normalized spacings in the left and right neighborhoods are
asymptotically i.i.d. exponential random variables. The associated independent
Poisson arrival processes are independent of $X_{k:n}$. For an extreme
$X_{k:n}$, the asymptotic independence property of spacings fails for $F$ in
the domain of attraction of Fréchet and Weibull ($\alpha \neq 1$)
distributions. This work also provides additional insight into the limiting
distribution for the number of observations around $X_{k:n}$ for all three
cases.
"
1434,"  We provide $L^p$-versus $L^\infty$-bounds for eigenfunctions on a real
spherical space $Z$ of wavefront type. It is shown that these bounds imply a
non-trivial error term estimate for lattice counting on $Z$. The paper also
serves as an introduction to geometric counting on spaces of the mentioned
type. Section 7 on higher rank is new and extends the result from v1 to higher
rank. Final version. To appear in Acta Math. Sinica.
"
18230,"  We investigate the formation of optical localized nonlinear structures,
evolving upon a non-zero background plane wave, in a dispersive quadratic
medium. We show the existence of quadratic Akhmediev breathers and Peregrine
solitary waves, in the regime of cascading second-harmonic generation. This
finding opens a novel path for the excitation of extreme rogue waves and for
the description of modulation instability in quadratic nonlinear optics.
"
8854,"  Android users are now suffering serious threats from various unwanted apps.
The analysis of apps' audit logs is one of the critical methods for some device
manufactures to unveil the underlying malice of apps. We propose and implement
DroidHolmes, a novel system that recovers contextual information around
limited-quantity audit logs. It also can help improving the performance of
existing analysis tools, such as FlowDroid and IccTA. The key module of
DroidHolmes is finding a path matched with the logs on the app's control-flow
graph. The challenge, however, is that the limited-quantity logs may incur high
computational complexity in log matching, where there are a large amount of
candidates caused by the coupling relation of successive logs. To address the
challenge, we propose a divide and conquer algorithm for effectively
positioning each log record individually. In our evaluation, DroidHolmes helps
existing tools to achieve 94.87% and 100% in precision and recall respectively
on 132 apps from open-source test suites. Based on the result of DroidHolmes,
the contextual information in the behaviors of 500 real-world apps is also
recovered. Meanwhile, DroidHolmes incurs negligible performance overhead on the
smartphone.
"
10893,"  We present in detail the convolutional neural network used in our previous
work to detect cosmic strings in cosmic microwave background (CMB) temperature
anisotropy maps. By training this neural network on numerically generated CMB
temperature maps, with and without cosmic strings, the network can produce
prediction maps that locate the position of the cosmic strings and provide a
probabilistic estimate of the value of the string tension $G\mu$. Supplying
noiseless simulations of CMB maps with arcmin resolution to the network
resulted in the accurate determination both of string locations and string
tension for sky maps having strings with string tension as low as
$G\mu=5\times10^{-9}$. The code is publicly available online. Though we trained
the network with a long straight string toy model, we show the network performs
well with realistic Nambu-Goto simulations.
"
20546,"  The crystallographic stacking order in multilayer graphene plays an important
role in determining its electronic structure. In trilayer graphene,
rhombohedral stacking (ABC) is particularly intriguing, exhibiting a flat band
with an electric-field tunable band gap. Such electronic structure is distinct
from simple hexagonal stacking (AAA) or typical Bernal stacking (ABA), and is
promising for nanoscale electronics, optoelectronics applications. So far clean
experimental electronic spectra on the first two stackings are missing because
the samples are usually too small in size (um or nm scale) to be resolved by
conventional angle-resolved photoemission spectroscopy (ARPES). Here by using
ARPES with nanospot beam size (NanoARPES), we provide direct experimental
evidence for the coexistence of three different stackings of trilayer graphene
and reveal their distinctive electronic structures directly. By fitting the
experimental data, we provide important experimental band parameters for
describing the electronic structure of trilayer graphene with different
stackings.
"
2095,"  Accounting fraud is a global concern representing a significant threat to the
financial system stability due to the resulting diminishing of the market
confidence and trust of regulatory authorities. Several tricks can be used to
commit accounting fraud, hence the need for non-static regulatory interventions
that take into account different fraudulent patterns. Accordingly, this study
aims to improve the detection of accounting fraud via the implementation of
several machine learning methods to better differentiate between fraud and
non-fraud companies, and to further assist the task of examination within the
riskier firms by evaluating relevant financial indicators. Out-of-sample
results suggest there is a great potential in detecting falsified financial
statements through statistical modelling and analysis of publicly available
accounting information. The proposed methodology can be of assistance to public
auditors and regulatory agencies as it facilitates auditing processes, and
supports more targeted and effective examinations of accounting reports.
"
19257,"  We introduce features for massive data streams. These stream features can be
thought of as ""ordered moments"" and generalize stream sketches from ""moments of
order one"" to ""ordered moments of arbitrary order"". In analogy to classic
moments, they have theoretical guarantees such as universality that are
important for learning algorithms.
"
6072,"  We introduce a model for the short-term dynamics of financial assets based on
an application to finance of quantum gauge theory, developing ideas of Ilinski.
We present a numerical algorithm for the computation of the probability
distribution of prices and compare the results with APPLE stocks prices and the
S&P500 index.
"
14612,"  Schmidt's game, and other similar intersection games have played an important
role in recent years in applications to number theory, dynamics, and
Diophantine approximation theory. These games are real games, that is, games in
which the players make moves from a complete separable metric space. The
determinacy of these games trivially follows from the axiom of determinacy for
real games, $\mathsf{AD}_\mathbb{R}$, which is a much stronger axiom than that
asserting all integer games are determined, $\mathsf{AD}$. One of our main
results is a general theorem which under the hypothesis $\mathsf{AD}$ implies
the determinacy of intersection games which have a property allowing strategies
to be simplified. In particular, we show that Schmidt's $(\alpha,\beta,\rho)$
game on $\mathbb{R}$ is determined from $\mathsf{AD}$ alone, but on
$\mathbb{R}^n$ for $n \geq 3$ we show that $\mathsf{AD}$ does not imply the
determinacy of this game. We also prove several other results specifically
related to the determinacy of Schmidt's game. These results highlight the
obstacles in obtaining the determinacy of Schmidt's game from $\mathsf{AD}$.
"
16955,"  The classical Weisfeiler-Lehman method WL[2] uses edge colors to produce a
powerful graph invariant. It is at least as powerful in its ability to
distinguish non-isomorphic graphs as the most prominent algebraic graph
invariants. It determines not only the spectrum of a graph, and the angles
between standard basis vectors and the eigenspaces, but even the angles between
projections of standard basis vectors into the eigenspaces. Here, we
investigate the combinatorial power of WL[2]. For sufficiently large k, WL[k]
determines all combinatorial properties of a graph. Many traditionally used
combinatorial invariants are determined by WL[k] for small k. We focus on two
fundamental invariants, the num- ber of cycles Cp of length p, and the number
of cliques Kp of size p. We show that WL[2] determines the number of cycles of
lengths up to 6, but not those of length 8. Also, WL[2] does not determine the
number of 4-cliques.
"
16451,"  This is an expository survey on recent sum-product results in finite fields.
We present a number of sum-product or ""expander"" results that say that if
$|A| > p^{2/3}$ then some set determined by sums and product of elements of $A$
is nearly as large as possible, and if $|A|<p^{2/3}$ then the set in question
is significantly larger that $A$. These results are based on a point-plane
incidence bound of Rudnev, and are quantitatively stronger than a wave of
earlier results following Bourgain, Katz, and Tao's breakthrough sum-product
result.
In addition, we present two geometric results: an incidence bound due to
Stevens and de Zeeuw, and bound on collinear triples, and an example of an
expander that breaks the threshold of $p^{2/3}$ required by the other results.
We have simplified proofs wherever possible, and hope that this survey may
serve as a compact guide to recent advances in arithmetic combinatorics over
finite fields. We do not claim originality for any of the results.
"
9816,"  We introduce and solve a new type of quadratic backward stochastic
differential equation systems defined in an infinite time horizon, called
\emph{ergodic BSDE systems}. Such systems arise naturally as candidate
solutions to characterize forward performance processes and their associated
optimal trading strategies in a regime switching market. In addition, we
develop a connection between the solution of the ergodic BSDE system and the
long-term growth rate of classical utility maximization problems, and use the
ergodic BSDE system to study the large time behavior of PDE systems with
quadratic growth Hamiltonians.
"
15850,"  We consider asymptotic normality of linear rank statistics under various
randomization rules met in clinical trials and designed for patients'
allocation into treatment and placebo arms. Exposition relies on some general
limit theorem due to McLeish (1974) which appears to be well suited for the
problem considered and may be employed for other similar rules undis- cussed in
the paper. Examples of applications include well known results as well as
several new ones.
"
11436,"  We introduce and describe the results of a novel shared task on bandit
learning for machine translation. The task was organized jointly by Amazon and
Heidelberg University for the first time at the Second Conference on Machine
Translation (WMT 2017). The goal of the task is to encourage research on
learning machine translation from weak user feedback instead of human
references or post-edits. On each of a sequence of rounds, a machine
translation system is required to propose a translation for an input, and
receives a real-valued estimate of the quality of the proposed translation for
learning. This paper describes the shared task's learning and evaluation setup,
using services hosted on Amazon Web Services (AWS), the data and evaluation
metrics, and the results of various machine translation architectures and
learning protocols.
"
15492,"  The 7th Symposium on Educational Advances in Artificial Intelligence
(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and
Future AI Educator Program to support the training of early-career university
faculty, secondary school faculty, and future educators (PhD candidates or
postdocs who intend a career in academia). As part of the program, awardees
were asked to address one of the following ""blue sky"" questions:
* How could/should Artificial Intelligence (AI) courses incorporate ethics
into the curriculum?
* How could we teach AI topics at an early undergraduate or a secondary
school level?
* AI has the potential for broad impact to numerous disciplines. How could we
make AI education more interdisciplinary, specifically to benefit
non-engineering fields?
This paper is a collection of their responses, intended to help motivate
discussion around these issues in AI education.
"
9957,"  Fractons are emergent particles which are immobile in isolation, but which
can move together in dipolar pairs or other small clusters. These exotic
excitations naturally occur in certain quantum phases of matter described by
tensor gauge theories. Previous research has focused on the properties of small
numbers of fractons and their interactions, effectively mapping out the
""Standard Model"" of fractons. In the present work, however, we consider systems
with a finite density of either fractons or their dipolar bound states, with a
focus on the $U(1)$ fracton models. We study some of the phases in which
emergent fractonic matter can exist, thereby initiating the study of the
""condensed matter"" of fractons. We begin by considering a system with a finite
density of fractons, which we show can exhibit microemulsion physics, in which
fractons form small-scale clusters emulsed in a phase dominated by long-range
repulsion. We then move on to study systems with a finite density of mobile
dipoles, which have phases analogous to many conventional condensed matter
phases. We focus on two major examples: Fermi liquids and quantum Hall phases.
A finite density of fermionic dipoles will form a Fermi surface and enter a
Fermi liquid phase. Interestingly, this dipolar Fermi liquid exhibits a
finite-temperature phase transition, corresponding to an unbinding transition
of fractons. Finally, we study chiral two-dimensional phases corresponding to
dipoles in ""quantum Hall"" states of their emergent magnetic field. We study
numerous aspects of these generalized quantum Hall systems, such as their edge
theories and ground state degeneracies.
"
18037,"  We investigate the terminal-pairibility problem in the case when the base
graph is a complete bipartite graph, and the demand graph is also bipartite
with the same color classes. We improve the lower bound on maximum value of
$\Delta(D)$ which still guarantees that the demand graph $D$ is
terminal-pairable in this setting. We also prove a sharp theorem on the maximum
number of edges such a demand graph can have.
"
1216,"  Publishing reproducible analyses is a long-standing and widespread challenge
for the scientific community, funding bodies and publishers. Although a
definitive solution is still elusive, the problem is recognized to affect all
disciplines and lead to a critical system inefficiency. Here, we propose a
blockchain-based approach to enhance scientific reproducibility, with a focus
on life science studies and precision medicine. While the interest of encoding
permanently into an immutable ledger all the study key information-including
endpoints, data and metadata, protocols, analytical methods and all
findings-has been already highlighted, here we apply the blockchain approach to
solve the issue of rewarding time and expertise of scientists that commit to
verify reproducibility. Our mechanism builds a trustless ecosystem of
researchers, funding bodies and publishers cooperating to guarantee digital and
permanent access to information and reproducible results. As a natural
byproduct, a procedure to quantify scientists' and institutions' reputation for
ranking purposes is obtained.
"
3692,"  The resonances associated with a fractional damped oscillator which is driven
by an oscillatory external force are studied. It is shown that such resonances
can be manipulated by tuning up either the coefficient of the fractional
damping or the order of the corresponding fractional derivatives.
"
6936,"  The moving sofa problem, posed by L. Moser in 1966, asks for the planar shape
of maximal area that can move around a right-angled corner in a hallway of unit
width. It is known that a maximal area shape exists, and that its area is at
least 2.2195... - the area of an explicit construction found by Gerver in 1992
- and at most $2\sqrt{2}=2.82...$, with the lower bound being conjectured as
the true value. We prove a new and improved upper bound of 2.37. The method
involves a computer-assisted proof scheme that can be used to rigorously derive
further improved upper bounds that converge to the correct value.
"
901,"  We have used soft x-ray photoemission electron microscopy to image the
magnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands
arranged in geometrically frustrated configurations such as square ice and
kagome ice geometries. Upon thermal randomization, ensembles of nano-islands
with strong inter-island magnetic coupling relax towards low-energy
configurations. Statistical analysis shows that the likelihood of ensembles
falling into low-energy configurations depends strongly on the annealing
temperature. Annealing to just below the Curie temperature of the ferromagnetic
film (T$_{C}$ = 338 K) allows for a much greater probability of achieving low
energy configurations as compared to annealing above the Curie temperature. At
this thermally active temperature of 325 K, the ensemble of ferromagnetic
nano-islands explore their energy landscape over time and eventually transition
to lower energy states as compared to the frozen-in configurations obtained
upon cooling from above the Curie temperature. Thus, this materials system
allows for a facile method to systematically study thermal evolution of
artificial spin ice arrays of nano-islands at temperatures modestly above room
temperature.
"
14613,"  Kernel $k$-means clustering can correctly identify and extract a far more
varied collection of cluster structures than the linear $k$-means clustering
algorithm. However, kernel $k$-means clustering is computationally expensive
when the non-linear feature map is high-dimensional and there are many input
points. Kernel approximation, e.g., the Nyström method, has been applied in
previous works to approximately solve kernel learning problems when both of the
above conditions are present. This work analyzes the application of this
paradigm to kernel $k$-means clustering, and shows that applying the linear
$k$-means clustering algorithm to $\frac{k}{\epsilon} (1 + o(1))$ features
constructed using a so-called rank-restricted Nyström approximation results
in cluster assignments that satisfy a $1 + \epsilon$ approximation ratio in
terms of the kernel $k$-means cost function, relative to the guarantee provided
by the same algorithm without the use of the Nyström method. As part of the
analysis, this work establishes a novel $1 + \epsilon$ relative-error trace
norm guarantee for low-rank approximation using the rank-restricted Nyström
approximation. Empirical evaluations on the $8.1$ million instance MNIST8M
dataset demonstrate the scalability and usefulness of kernel $k$-means
clustering with Nyström approximation. This work argues that spectral
clustering using Nyström approximation---a popular and computationally
efficient, but theoretically unsound approach to non-linear clustering---should
be replaced with the efficient and theoretically sound combination of kernel
$k$-means clustering with Nyström approximation. The superior performance of
the latter approach is empirically verified.
"
18633,"  Halide perovskite (HaP) semiconductors are revolutionizing photovoltaic (PV)
solar energy conversion by showing remarkable performance of solar cells made
with esp. tetragonal methylammonium lead tri-iodide (MAPbI3). In particular,
the low voltage loss of these cells implies a remarkably low recombination rate
of photogenerated carriers. It was suggested that low recombination can be due
to spatial separation of electrons and holes, a possibility if MAPbI3 is a
semiconducting ferroelectric, which, however, requires clear experimental
evidence. As a first step we show that, in operando, MAPbI3 (unlike MAPbBr3) is
pyroelectric, which implies it can be ferroelectric. The next step, proving it
is (not) ferroelectric, is challenging, because of the material s relatively
high electrical conductance (a consequence of an optical band gap suitable for
PV conversion!) and low stability under high applied bias voltage. This
excludes normal measurements of a ferroelectric hysteresis loop to prove
ferroelctricity s hallmark for switchable polarization. By adopting an approach
suitable for electrically leaky materials as MAPbI3, we show here ferroelectric
hysteresis from well-characterized single crystals at low temperature (still
within the tetragonal phase, which is the room temperature stable phase). Using
chemical etching, we also image polar domains, the structural fingerprint for
ferroelectricity, periodically stacked along the polar axis of the crystal,
which, as predicted by theory, scale with the overall crystal size. We also
succeeded in detecting clear second-harmonic generation, direct evidence for
the material s non-centrosymmetry. We note that the material s ferroelectric
nature, can, but not obviously need to be important in a PV cell, operating
around room temperature.
"
14274,"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
"
20044,"  In this study, the gravitational octree code originally optimized for the
Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta
architecture. The Volta architecture introduces independent thread scheduling
requiring either the insertion of the explicit synchronizations at appropriate
locations or the enforcement of the same implicit synchronizations as do the
Pascal or earlier architectures by specifying \texttt{-gencode
arch=compute\_60,code=sm\_70}. The performance measurements on Tesla V100, the
current flagship GPU by NVIDIA, revealed that the $N$-body simulations of the
Andromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \times
10^{-2}$~s or $3.3 \times 10^{-2}$~s per step for each case. Tesla V100
achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the
flagship GPU in the previous generation. The observed speed-up of 2.2 is
greater than 1.5, which is the ratio of the theoretical peak performance of the
two GPUs. The independence of the units for integer operations from those for
floating-point number operations enables the overlapped execution of integer
and floating-point number operations. It hides the execution time of the
integer operations leading to the speed-up rate above the theoretical peak
performance ratio. Tesla V100 can execute $N$-body simulation with up to $25
\times 2^{20} = 26214400$ particles, and it took $2.0 \times 10^{-1}$~s per
step. It corresponds to $3.5$~TFlop/s, which is 22\% of the single-precision
theoretical peak performance.
"
10252,"  Among recently introduced new notions in real algebraic geometry is that of
regulous functions. Such functions form a foundation for the development of
regulous geometry. Several interesting results on regulous varieties and
regulous sheaves are already available. In this paper, we define and
investigate regulous vector bundles. We establish algebraic and geometric
properties of such vector bundles, and identify them with stratified-algebraic
vector bundles. Furthermore, using new results on curve-rational functions, we
characterize regulous vector bundles among families of vector spaces
parametrized by an affine regulous variety. We also study relationships between
regulous and topological vector bundles.
"
5930,"  Using polarization-resolved transient reflection spectroscopy, we investigate
the ultrafast modulation of light interacting with a metasurface consisting of
coherently vibrating nanophotonic meta-atoms in the form of U-shaped split-ring
resonators, that exhibit co-localized optical and mechanical resonances. With a
two-dimensional square-lattice array of these resonators formed of gold on a
glass substrate, we monitor the visible-pump-pulse induced gigahertz
oscillations in intensity of reflected linearly-polarized infrared probe light
pulses, modulated by the resonators effectively acting as miniature tuning
forks. A multimodal vibrational response involving the opening and closing
motion of the split rings is detected in this way. Numerical simulations of the
associated transient deformations and strain fields elucidate the complex
nanomechanical dynamics contributing to the ultrafast optical modulation, and
point to the role of acousto-plasmonic interactions through the opening and
closing motion of the SRR gaps as the dominant effect. Applications include
ultrafast acoustooptic modulator design and sensing.
"
3481,"  We examine the growth and evolution of quenched galaxies in the Mufasa
cosmological hydrodynamic simulations that include an evolving halo mass-based
quenching prescription, with galaxy colours computed accounting for
line-of-sight extinction to individual star particles. Mufasa reproduces the
observed present-day red sequence reasonably well, including its slope,
amplitude, and scatter. In Mufasa, the red sequence slope is driven entirely by
the steep stellar mass-stellar metallicity relation, which independently agrees
with observations. High-mass star-forming galaxies blend smoothly onto the red
sequence, indicating the lack of a well-defined green valley at M*>10^10.5 Mo.
The most massive galaxies quench the earliest and then grow very little in mass
via dry merging; they attain their high masses at earlier epochs when cold
inflows more effectively penetrate hot halos. To higher redshifts, the red
sequence becomes increasingly contaminated with massive dusty star-forming
galaxies; UVJ selection subtly but effectively separates these populations. We
then examine the evolution of the mass functions of central and satellite
galaxies split into passive and star-forming via UVJ. Massive quenched systems
show good agreement with observations out to z~2, despite not including a rapid
early quenching mode associated with mergers. However, low-mass quenched
galaxies are far too numerous at z<1 in Mufasa, indicating that Mufasa strongly
over-quenches satellites. A challenge for hydrodynamic simulations is to devise
a quenching model that produces enough early massive quenched galaxies and
keeps them quenched to z=0, while not being so strong as to over-quench
satellites; Mufasa's current scheme fails at the latter.
"
7857,"  We consider the problem of provably optimal exploration in reinforcement
learning for finite horizon MDPs. We show that an optimistic modification to
value iteration achieves a regret bound of $\tilde{O}( \sqrt{HSAT} +
H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states,
$A$ the number of actions and $T$ the number of time-steps. This result
improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved
by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new
results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of
$\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of
$\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key
insights. We use careful application of concentration inequalities to the
optimal value function as a whole, rather than to the transitions probabilities
(to improve scaling in $S$), and we define Bernstein-based ""exploration
bonuses"" that use the empirical variance of the estimated values at the next
states (to improve scaling in $H$).
"
5624,"  We establish effective mean-value estimates for a wide class of
multiplicative arithmetic functions, thereby providing (essentially optimal)
quantitative versions of Wirsing's classical estimates and extending those of
Halász. Several applications are derived, including: estimates for the
difference of mean-values of so-called pretentious functions, local laws for
the distribution of prime factors in an arbitrary set, and weighted
distribution of additive functions.
"
14919,"  In 1992 a puzzling transition was discovered in simulations of randomly
coupled limit-cycle oscillators. This so-called volcano transition has resisted
analysis ever since. It was originally conjectured to mark the emergence of an
oscillator glass, but here we show it need not. We introduce and solve a
simpler model with a qualitatively identical volcano transition and find,
unexpectedly, that its supercritical state is not glassy. We discuss the
implications for the original model and suggest experimental systems in which a
volcano transition and oscillator glass may appear.
"
1130,"  We propose a new localized inference algorithm for answering marginalization
queries in large graphical models with the correlation decay property. Given a
query variable and a large graphical model, we define a much smaller model in a
local region around the query variable in the target model so that the marginal
distribution of the query variable can be accurately approximated. We introduce
two approximation error bounds based on the Dobrushin's comparison theorem and
apply our bounds to derive a greedy expansion algorithm that efficiently guides
the selection of neighbor nodes for localized inference. We verify our
theoretical bounds on various datasets and demonstrate that our localized
inference algorithm can provide fast and accurate approximation for large
graphical models.
"
20391,"  We consider a two-node tandem queueing network in which the upstream queue is
M/G/1 and each job reuses its upstream service requirement when moving to the
downstream queue. Both servers employ the first-in-first-out policy. We
investigate the amount of work in the second queue at certain embedded arrival
time points, namely when the upstream queue has just emptied. We focus on the
case of infinite-variance service times and obtain a heavy traffic process
limit for the embedded Markov chain.
"
8853,"  Domain shift refers to the well known problem that a model trained in one
source domain performs poorly when applied to a target domain with different
statistics. {Domain Generalization} (DG) techniques attempt to alleviate this
issue by producing models which by design generalize well to novel testing
domains. We propose a novel {meta-learning} method for domain generalization.
Rather than designing a specific model that is robust to domain shift as in
most previous DG work, we propose a model agnostic training procedure for DG.
Our algorithm simulates train/test domain shift during training by synthesizing
virtual testing domains within each mini-batch. The meta-optimization objective
requires that steps to improve training domain performance should also improve
testing domain performance. This meta-learning procedure trains models with
good generalization ability to novel domains. We evaluate our method and
achieve state of the art results on a recent cross-domain image classification
benchmark, as well demonstrating its potential on two classic reinforcement
learning tasks.
"
16196,"  Alternating minimization, or Fienup methods, have a long history in phase
retrieval. We provide new insights related to the empirical and theoretical
analysis of these algorithms when used with Fourier measurements and combined
with convex priors. In particular, we show that Fienup methods can be viewed as
performing alternating minimization on a regularized nonconvex least-squares
problem with respect to amplitude measurements. We then prove that under mild
additional structural assumptions on the prior (semi-algebraicity), the
sequence of signal estimates has a smooth convergent behaviour towards a
critical point of the nonconvex regularized least-squares objective. Finally,
we propose an extension to Fienup techniques, based on a projected gradient
descent interpretation and acceleration using inertial terms. We demonstrate
experimentally that this modification combined with an $\ell_1$ prior
constitutes a competitive approach for sparse phase retrieval.
"
16099,"  We give an elementary proof for the fact that an irreducible hyperbolic
polynomial has only one pair of hyperbolicity cones.
"
14115,"  The declination is a quantitative method for identifying possible partisan
gerrymanders by analyzing vote distributions. In this expository note we
explain and motivate the definition of the declination. The minimal computer
code required for computing the declination is included. We end by computing
its value on several recent elections.
"
2589,"  The concept of a C-class of differential equations goes back to E. Cartan
with the upshot that generic equations in a C-class can be solved without
integration. While Cartan's definition was in terms of differential invariants
being first integrals, all results exhibiting C-classes that we are aware of
are based on the fact that a canonical Cartan geometry associated to the
equations in the class descends to the space of solutions. For sufficiently low
orders, these geometries belong to the class of parabolic geometries and the
results follow from the general characterization of geometries descending to a
twistor space.
In this article we answer the question of whether a canonical Cartan geometry
descends to the space of solutions in the case of scalar ODEs of order at least
four and of systems of ODEs of order at least three. As in the lower order
cases, this is characterized by the vanishing of the generalized Wilczynski
invariants, which are defined via the linearization at a solution. The
canonical Cartan geometries (which are not parabolic geometries) are a slight
variation of those available in the literature based on a recent general
construction. All the verifications needed to apply this construction for the
classes of ODEs we study are carried out in the article, which thus also
provides a complete alternative proof for the existence of canonical Cartan
connections associated to higher order (systems of) ODEs.
"
11313,"  We demonstrate the existence of the excited state of an exciton-polariton in
a semiconductor microcavity. The strong coupling of the quantum well heavy-hole
exciton in an excited 2s state to the cavity photon is observed in non-zero
magnetic field due to surprisingly fast increase of Rabi energy of the 2s
exciton-polariton in magnetic field. This effect is explained by a strong
modification of the wave-function of the relative electron-hole motion for the
2s exciton state.
"
13002,"  We show that in $\text{O}(D)$ invariant matrix theories containing a large
number $D$ of complex or Hermitian matrices, one can define a
$D\rightarrow\infty$ limit for which the sum over planar diagrams truncates to
a tractable, yet non-trivial, sum over melon diagrams. In particular, results
obtained recently in SYK and tensor models can be generalized to traditional,
string-inspired matrix quantum mechanical models of black holes.
"
14765,"  Mahlmann and Schindelhauer (2005) defined a Markov chain which they called
$k$-Flipper, and showed that it is irreducible on the set of all connected
regular graphs of a given degree (at least 3). We study the 1-Flipper chain,
which we call the flip chain, and prove that the flip chain converges rapidly
to the uniform distribution over connected $2r$-regular graphs with $n$
vertices, where $n\geq 8$ and $r = r(n)\geq 2$. Formally, we prove that the
distribution of the flip chain will be within $\varepsilon$ of uniform in total
variation distance after $\text{poly}(n,r,\log(\varepsilon^{-1}))$ steps. This
polynomial upper bound on the mixing time is given explicitly, and improves
markedly on a previous bound given by Feder et al.(2006). We achieve this
improvement by using a direct two-stage canonical path construction, which we
define in a general setting.
This work has applications to decentralised networks based on random regular
connected graphs of even degree, as a self-stabilising protocol in which nodes
spontaneously perform random flips in order to repair the network.
"
11448,"  In this paper, we present a novel approach for broadcasting information based
on a Bluetooth Low Energy (BLE) ibeacon technology. We propose a dynamic method
that uses a combination of Wi-Fi and BLE technology where every technology
plays a part in a user discovery and broadcasting process. In such system, a
specific ibeacon device broadcasts the information when a user is in proximity.
Using experiments, we conduct a scenario where the system discovers users,
disseminates information, and later we use collected data to examine the system
performance and capability. The results show that our proposed approach has a
promising potential to become a powerful tool in the discovery and broadcasting
concept that can be easily implemented and used in business environments.
"
8374,"  In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)
to counter the otherwise lethal intracellular formation of ice. Structures and
sequences of various AFPs exhibit a high degree of heterogeneity, consequently
the prediction of the AFPs is considered to be a challenging task. In this
research, we propose to handle this arduous manifold learning task using the
notion of localized processing. In particular an AFP sequence is segmented into
two sub-segments each of which is analyzed for amino acid and di-peptide
compositions. We propose to use only the most significant features using the
concept of information gain (IG) followed by a random forest classification
approach. The proposed RAFP-Pred achieved an excellent performance on a number
of standard datasets. We report a high Youden's index
(sensitivity+specificity-1) value of 0.75 on the standard independent test data
set outperforming the AFP-PseAAC, AFP\_PSSM, AFP-Pred and iAFP by a margin of
0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB
dataset is found to be 83.19\% which is substantially superior to the 57.18\%
reported for the iAFP method.
"
2710,"  This activity has been developed as a resource for the ""EU Space Awareness""
educational programme. As part of the suite ""Our Fragile Planet"" together with
the ""Climate Box"" it addresses aspects of weather phenomena, the Earth's
climate and climate change as well as Earth observation efforts like in the
European ""Copernicus"" programme. This resource consists of three parts that
illustrate the power of the Sun driving a global air circulation system that is
also responsible for tropical and subtropical climate zones. Through
experiments, students learn how heated air rises above cool air and how a
continuous heat source produces air convection streams that can even drive a
propeller. Students then apply what they have learnt to complete a worksheet
that presents the big picture of the global air circulation system of the
equator region by transferring the knowledge from the previous activities in to
a larger scale.
"
15963,"  A novel algorithm is proposed for CANDECOMP/PARAFAC tensor decomposition to
exploit best rank-1 tensor approximation. Different from the existing
algorithms, our algorithm updates rank-1 tensors simultaneously in parallel. In
order to achieve this, we develop new all-at-once algorithms for best rank-1
tensor approximation based on the Levenberg-Marquardt method and the rotational
update. We show that the LM algorithm has the same complexity of first-order
optimisation algorithms, while the rotational method leads to solving the best
rank-1 approximation of tensors of size $2 \times 2 \times \cdots \times 2$. We
derive a closed-form expression of the best rank-1 tensor of $2\times 2 \times
2$ tensors and present an ALS algorithm which updates 3 component at a time for
higher order tensors. The proposed algorithm is illustrated in decomposition of
difficult tensors which are associated with multiplication of two matrices.
"
113,"  The class of stochastically self-similar sets contains many famous examples
of random sets, e.g. Mandelbrot percolation and general fractal percolation.
Under the assumption of the uniform open set condition and some mild
assumptions on the iterated function systems used, we show that the
quasi-Assouad dimension of self-similar random recursive sets is almost surely
equal to the almost sure Hausdorff dimension of the set. We further comment on
random homogeneous and $V$-variable sets and the removal of overlap conditions.
"
12288,"  Let $F$ be a non-Archimedan local field, $G$ a connected reductive group
defined and split over $F$, and $T$ a maximal $F$-split torus in $G$. Let
$\chi_0$ be a depth zero character of the maximal compact subgroup
$\mathcal{T}$ of $T(F)$. It gives by inflation a character $\rho$ of an Iwahori
subgroup $\mathcal{I}$ of $G(F)$ containing $\mathcal{T}$. From Roche, $\chi_0$
defines a split endoscopic group $G'$ of $G$, and there is an injective
morphism of ${\Bbb C}$-algebras $\mathcal{H}(G(F),\rho) \rightarrow
\mathcal{H}(G'(F),1_{\mathcal{I}'})$ where $\mathcal{H}(G(F),\rho)$ is the
Hecke algebra of compactly supported $\rho^{-1}$-spherical functions on $G(F)$
and $\mathcal{I}'$ is an Iwahori subgroup of $G'(F)$. This morphism restricts
to an injective morphism $\zeta: \mathcal{Z}(G(F),\rho)\rightarrow
\mathcal{Z}(G'(F),1_{\mathcal{I}'})$ between the centers of the Hecke algebras.
We prove here that a certain linear combination of morphisms analogous to
$\zeta$ realizes the transfer (matching of strongly $G$-regular semisimple
orbital integrals). If ${\rm char}(F)=p>0$, our result is unconditional only if
$p$ is large enough.
"
14174,"  In their work on a sharp compactness theorem for the Yamabe problem, Khuri,
Marques and Schoen apply a refined blow-up analysis (what we call `second order
blow-up argument' in this article) to obtain highly accurate approximate
solutions for the Yamabe equation. As for the conformal scalar curvature
equation on S^n with n > 3, we examine the second order blow-up argument and
obtain refined estimate for a blow-up sequence near a simple blow-up point. The
estimate involves local effect from the Taylor expansion of the scalar
curvature function, global effect from other blow-up points, and the balance
formula as expressed in the Pohozaev identity in an essential way.
"
19547,"  We study loss functions that measure the accuracy of a prediction based on
multiple data points simultaneously. To our knowledge, such loss functions have
not been studied before in the area of property elicitation or in machine
learning more broadly. As compared to traditional loss functions that take only
a single data point, these multi-observation loss functions can in some cases
drastically reduce the dimensionality of the hypothesis required. In
elicitation, this corresponds to requiring many fewer reports; in empirical
risk minimization, it corresponds to algorithms on a hypothesis space of much
smaller dimension. We explore some examples of the tradeoff between
dimensionality and number of observations, give some geometric
characterizations and intuition for relating loss functions and the properties
that they elicit, and discuss some implications for both elicitation and
machine-learning contexts.
"
18546,"  The Linked Data principles provide a decentral approach for publishing
structured data in the RDF format on the Web. In contrast to structured data
published in relational databases where a key is often provided explicitly,
finding a set of properties that allows identifying a resource uniquely is a
non-trivial task. Still, finding keys is of central importance for manifold
applications such as resource deduplication, link discovery, logical data
compression and data integration. In this paper, we address this research gap
by specifying a refinement operator, dubbed ROCKER, which we prove to be
finite, proper and non-redundant. We combine the theoretical characteristics of
this operator with two monotonicities of keys to obtain a time-efficient
approach for detecting keys, i.e., sets of properties that describe resources
uniquely. We then utilize a hash index to compute the discriminability score
efficiently. Therewith, we ensure that our approach can scale to very large
knowledge bases. Results show that ROCKER yields more accurate results, has a
comparable runtime, and consumes less memory w.r.t. existing state-of-the-art
techniques.
"
11460,"  Discovering automatically the semantic structure of tagged visual data (e.g.
web videos and images) is important for visual data analysis and
interpretation, enabling the machine intelligence for effectively processing
the fast-growing amount of multi-media data. However, this is non-trivial due
to the need for jointly learning underlying correlations between heterogeneous
visual and tag data. The task is made more challenging by inherently sparse and
incomplete tags. In this work, we develop a method for modelling the inherent
visual data concept structures based on a novel Hierarchical-Multi-Label Random
Forest model capable of correlating structured visual and tag information so as
to more accurately interpret the visual semantics, e.g. disclosing meaningful
visual groups with similar high-level concepts, and recovering missing tags for
individual visual data samples. Specifically, our model exploits hierarchically
structured tags of different semantic abstractness and multiple tag statistical
correlations in addition to modelling visual and tag interactions. As a result,
our model is able to discover more accurate semantic correlation between
textual tags and visual features, and finally providing favourable visual
semantics interpretation even with highly sparse and incomplete tags. We
demonstrate the advantages of our proposed approach in two fundamental
applications, visual data clustering and missing tag completion, on
benchmarking video (i.e. TRECVID MED 2011) and image (i.e. NUS-WIDE) datasets.
"
15494,"  We introduce several new constructions for perfect periodic autocorrelation
sequences and arrays over the unit quaternions. This paper uses both
mathematical proofs and com- puter experiments to prove the (bounded) array
constructions have perfect periodic auto- correlation. Furthermore, the first
sequence construction generates odd-perfect sequences of unbounded lengths,
with good ZCZ.
"
10549,"  The ordering of a multilayer consisting of DSPC bilayers on a silica sol
substrate is studied within the model-independent approach to the
reconstruction of profiles of the electron density from X-ray reflectometry
data. It is found that the electroporation of bilayers in the field of anion
silica nanoparticles significantly accelerates the process of their saturation
with Na+ and H2O, which explains both a relatively small time of formation of
the structure of the multilayer of 10^5 - 7x10^5 s and ~13 % excess of the
electron density in it.
"
11838,"  Macroscopic models for systems involving diffusion, short-range repulsion,
and long-range attraction have been studied extensively in the last decades. In
this paper we extend the analysis to a system for two species interacting with
each other according to different inner- and intra-species attractions. Under
suitable conditions on this self- and crosswise attraction an interesting
effect can be observed, namely phase separation into neighbouring regions, each
of which contains only one of the species. We prove that the intersection of
the support of the stationary solutions of the continuum model for the two
species has zero Lebesgue measure, while the support of the sum of the two
densities is simply connected.
Preliminary results indicate the existence of phase separation, i.e. spatial
sorting of the different species. A detailed analysis in one spatial dimension
follows. The existence and shape of segregated stationary solutions is shown
via the Krein-Rutman theorem. Moreover, for small repulsion/nonlinear
diffusion, also uniqueness of these stationary states is proved.
"
15527,"  Recent Einstein-Podolsky-Rosen-Bohm experiments [M. Giustina et al. Phys.
Rev. Lett. 115, 250401 (2015); L. K. Shalm et al. Phys. Rev. Lett. 115, 250402
(2015)] that claim to be loophole free are scrutinized and are shown to suffer
a photon identification loophole. The combination of a digital computer and
discrete-event simulation is used to construct a minimal but faithful model of
the most perfected realization of these laboratory experiments. In contrast to
prior simulations, all photon selections are strictly made, as they are in the
actual experiments, at the local station and no other ""post-selection"" is
involved. The simulation results demonstrate that a manifestly non-quantum
model that identifies photons in the same local manner as in these experiments
can produce correlations that are in excellent agreement with those of the
quantum theoretical description of the corresponding thought experiment, in
conflict with Bell's theorem. The failure of Bell's theorem is possible because
of our recognition of the photon identification loophole. Such identification
measurement-procedures are necessarily included in all actual experiments but
are not included in the theory of Bell and his followers.
"
17861,"  The interaction that occurs between a light solid object and a horizontal
soap film of a bamboo foam contained in a cylindrical tube is simulated in 3D.
We vary the shape of the falling object from a sphere to a cube by changing a
single shape parameter as well as varying the initial orientation and position
of the object. We investigate in detail how the soap film deforms in all these
cases, and determine the network and pressure forces that a foam exerts on a
falling object, due to surface tension and bubble pressure respectively. We
show that a cubic particle in a particular orientation experiences the largest
drag force, and that this orientation is also the most likely outcome of
dropping a cube from an arbitrary orientation through a bamboo foam.
"
19791,"  One-dimensional systems obtained as low-energy limits of hybrid
superconductor-topological insulator devices provide means of production,
transport, and destruction of Majorana bound states (MBSs) by variations of the
magnetic flux. When two or more pairs of MBSs are present in the intermediate
state, there is a possibility of a Landau-Zener transition, wherein even a slow
variation of the flux leads to production of a quasiparticle pair. We study
numerically a version of this process, with four MBSs produced and subsequently
destroyed, and find that, quite universally, the probability of quasiparticle
production in it is 50%. This implies that the effect may be a limiting factor
in applications requiring a high degree of quantum coherence.
"
698,"  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms
for uncoordinated spectrum access. The number of users is assumed to be unknown
to each user. A stochastic setting is first considered, where the rewards on a
channel are the same for each user. In contrast to prior work, it is assumed
that the number of users can possibly exceed the number of channels, and that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
in particular, even when the number of users is less than the number of
channels. Next, an adversarial multi-user MAB framework is considered, where
the rewards on the channels are user-dependent. It is assumed that the number
of users is less than the number of channels, and that the users receive zero
reward on collision. The proposed algorithm combines the Exp3.P algorithm
developed in prior work for single user adversarial bandits with a collision
resolution mechanism to achieve sub-linear regret. It is shown that if every
user employs the proposed algorithm, the system wide regret is of the order
$O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both
stochastic and adversarial scenarios are extended to the dynamic case where the
number of users in the system evolves over time and are shown to lead to
sub-linear regret.
"
4936,"  Predicting the next activity of a running process is an important aspect of
process management. Recently, artificial neural networks, so called
deep-learning approaches, have been proposed to address this challenge. This
demo paper describes a software application that applies the Tensorflow
deep-learning framework to process prediction. The software application reads
industry-standard XES files for training and presents the user with an
easy-to-use graphical user interface for both training and prediction. The
system provides several improvements over earlier work. This demo paper focuses
on the software implementation and describes the architecture and user
interface.
"
10986,"  It is well known that the Euler vortex patch in $\mathbb{R}^{2}$ will remain
regular if it is regular enough initially. In bounded domains, the regularity
theory for patch solutions is less complete. We study here the Euler vortex
patch in a disk. We prove global in time regularity by providing the upper
bound of the growth of curvature of the patch boundary. For a special symmetric
scenario, we construct an example of double exponential curvature growth,
showing that such upper bound is qualitatively sharp.
"
19314,"  In this work we introduce malware detection from raw byte sequences as a
fruitful research area to the larger machine learning community. Building a
neural network for such a problem presents a number of interesting challenges
that have not occurred in tasks such as image processing or NLP. In particular,
we note that detection from raw bytes presents a sequence problem with over two
million time steps and a problem where batch normalization appear to hinder the
learning process. We present our initial work in building a solution to tackle
this problem, which has linear complexity dependence on the sequence length,
and allows for interpretable sub-regions of the binary to be identified. In
doing so we will discuss the many challenges in building a neural network to
process data at this scale, and the methods we used to work around them.
"
4635,"  This paper proposes a novel semi-distributed and practical ICIC scheme based
on the Almost Blank SubFrame (ABSF) approach specified by 3GPP. We define two
mathematical programming problems for the cases of guaranteed and best-effort
traffic, and use game theory to study the properties of the derived ICIC
distributed schemes, which are compared in detail against unaffordable
centralized schemes. Based on the analysis of the proposed models, we define
Distributed Multi-traffic Scheduling (DMS), a unified distributed framework for
adaptive interference-aware scheduling of base stations in future cellular
networks which accounts for both guaranteed and best-effort traffic. DMS
follows a two-tier approach, consisting of local ABSF schedulers, which perform
the resource distribution between guaranteed and best effort traffic, and a
lightweight local supervisor, which coordinates ABSF local decisions. As a
result of such a two-tier design, DMS requires very light signaling to drive
the local schedulers to globally efficient operating points. As shown by means
of numerical results, DMS allows to (i) maximize radio resources reuse, (ii)
provide requested quality for guaranteed traffic, (iii) minimize the time
dedicated to guaranteed traffic to leave room for best-effort traffic, and (iv)
maximize resource utilization efficiency for best-effort traffic.
"
9752,"  Starting with a graph, two players take turns in either deleting an edge or
deleting a vertex and all incident edges. The player removing the last vertex
wins. We review the known results for this game and extend the computation of
nim-values to new families of graphs. A conjecture of Khandhawit and Ye on the
nim-values of graphs with one odd cycle is proved. We also see that, for wheels
and their subgraphs, this game exhibits a surprising amount of unexplained
regularity.
"
3928,"  When the residents of Flint learned that lead had contaminated their water
system, the local government made water-testing kits available to them free of
charge. The city government published the results of these tests, creating a
valuable dataset that is key to understanding the causes and extent of the lead
contamination event in Flint. This is the nation's largest dataset on lead in a
municipal water system.
In this paper, we predict the lead contamination for each household's water
supply, and we study several related aspects of Flint's water troubles, many of
which generalize well beyond this one city. For example, we show that elevated
lead risks can be (weakly) predicted from observable home attributes. Then we
explore the factors associated with elevated lead. These risk assessments were
developed in part via a crowd sourced prediction challenge at the University of
Michigan. To inform Flint residents of these assessments, they have been
incorporated into a web and mobile application funded by \texttt{Google.org}.
We also explore questions of self-selection in the residential testing program,
examining which factors are linked to when and how frequently residents
voluntarily sample their water.
"
7121,"  In this paper a new long-term survival distribution is proposed. The so
called long term Fréchet distribution allows us to fit data where a part of
the population is not susceptible to the event of interest. This model may be
used, for example, in clinical studies where a portion of the population can be
cured during a treatment. It is shown an account of mathematical properties of
the new distribution such as its moments and survival properties. As well is
presented the maximum likelihood estimators (MLEs) for the parameters. A
numerical simulation is carried out in order to verify the performance of the
MLEs. Finally, an important application related to the leukemia free-survival
times for transplant patients are discussed to illustrates our proposed
distribution
"
11724,"  Given a geometric path, the Time-Optimal Path Tracking problem consists in
finding the control strategy to traverse the path time-optimally while
regulating tracking errors. A simple yet effective approach to this problem is
to decompose the controller into two components: (i)~a path controller, which
modulates the parameterization of the desired path in an online manner,
yielding a reference trajectory; and (ii)~a tracking controller, which takes
the reference trajectory and outputs joint torques for tracking. However, there
is one major difficulty: the path controller might not find any feasible
reference trajectory that can be tracked by the tracking controller because of
torque bounds. In turn, this results in degraded tracking performances. Here,
we propose a new path controller that is guaranteed to find feasible reference
trajectories by accounting for possible future perturbations. The main
technical tool underlying the proposed controller is Reachability Analysis, a
new method for analyzing path parameterization problems. Simulations show that
the proposed controller outperforms existing methods.
"
11307,"  In order to alleviate data sparsity and overfitting problems in maximum
likelihood estimation (MLE) for sequence prediction tasks, we propose the
Generative Bridging Network (GBN), in which a novel bridge module is introduced
to assist the training of the sequence prediction model (the generator
network). Unlike MLE directly maximizing the conditional likelihood, the bridge
extends the point-wise ground truth to a bridge distribution conditioned on it,
and the generator is optimized to minimize their KL-divergence. Three different
GBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to
penalize confidence, enhance language smoothness and relieve learning burden.
Experiments conducted on two recognized sequence prediction tasks (machine
translation and abstractive text summarization) show that our proposed GBNs can
yield significant improvements over strong baselines. Furthermore, by analyzing
samples drawn from different bridges, expected influences on the generator are
verified.
"
2319,"  Convolutional Neural Networks have been a subject of great importance over
the past decade and great strides have been made in their utility for producing
state of the art performance in many computer vision problems. However, the
behavior of deep networks is yet to be fully understood and is still an active
area of research. In this work, we present an intriguing behavior: pre-trained
CNNs can be made to improve their predictions by structurally perturbing the
input. We observe that these perturbations - referred as Guided Perturbations -
enable a trained network to improve its prediction performance without any
learning or change in network weights. We perform various ablative experiments
to understand how these perturbations affect the local context and feature
representations. Furthermore, we demonstrate that this idea can improve
performance of several existing approaches on semantic segmentation and scene
labeling tasks on the PASCAL VOC dataset and supervised classification tasks on
MNIST and CIFAR10 datasets.
"
7275,"  T-310 is a cipher that was used for encryption of governmental communications
in East Germany during the final years of the Cold War. Due to its complexity
and the encryption process,there was no published attack for a period of more
than 40 years until 2018 by Nicolas T. Courtois et al. in [10]. In this thesis
we study the so called 'long term keys' that were used in the cipher, in order
to expose weaknesses which will assist the design of various attacks on T-310.
"
2535,"  One of the challenges in model-based control of stochastic dynamical systems
is that the state transition dynamics are involved, and it is not easy or
efficient to make good-quality predictions of the states. Moreover, there are
not many representational models for the majority of autonomous systems, as it
is not easy to build a compact model that captures the entire dynamical
subtleties and uncertainties. In this work, we present a hierarchical Bayesian
linear regression model with local features to learn the dynamics of a
micro-robotic system as well as two simpler examples, consisting of a
stochastic mass-spring damper and a stochastic double inverted pendulum on a
cart. The model is hierarchical since we assume non-stationary priors for the
model parameters. These non-stationary priors make the model more flexible by
imposing priors on the priors of the model. To solve the maximum likelihood
(ML) problem for this hierarchical model, we use the variational expectation
maximization (EM) algorithm, and enhance the procedure by introducing hidden
target variables. The algorithm yields parsimonious model structures, and
consistently provides fast and accurate predictions for all our examples
involving large training and test sets. This demonstrates the effectiveness of
the method in learning stochastic dynamics, which makes it suitable for future
use in a paradigm, such as model-based reinforcement learning, to compute
optimal control policies in real time.
"
10372,"  In this work the issue of Bayesian inference for stationary data is
addressed. Therefor a parametrization of a statistically suitable subspace of
the the shift-ergodic probability measures on a Cartesian product of some
finite state space is given using an inverse limit construction. Moreover, an
explicit model for the prior is given by taking into account an additional step
in the usual stepwise sampling scheme of data. An update to the posterior is
defined by exploiting this augmented sample scheme. Thereby, its model-step is
updated using a measurement of the empirical distances between the model
classes.
"
2283,"  Designing decentralized policies for wireless communication networks is a
crucial problem, which has only been partially solved in the literature so far.
In this paper, we propose the Decentralized Markov Decision Process (Dec-MDP)
framework to analyze a wireless sensor network with multiple users which access
a common wireless channel. We consider devices with energy harvesting
capabilities, so that they aim at balancing the energy arrivals with the data
departures and with the probability of colliding with other nodes. Randomly
over time, an access point triggers a SYNC slot, wherein it recomputes the
optimal transmission parameters of the whole network, and distributes this
information. Every node receives its own policy, which specifies how it should
access the channel in the future, and, thereafter, proceeds in a fully
decentralized fashion, without interacting with other entities in the network.
We propose a multi-layer Markov model, where an external MDP manages the jumps
between SYNC slots, and an internal Dec-MDP computes the optimal policy in the
near future. We numerically show that, because of the harvesting, a fully
orthogonal scheme (e.g., TDMA-like) is suboptimal in energy harvesting
scenarios, and the optimal trade-off lies between an orthogonal and a random
access system.
"
7125,"  We present a family of self-consistent axisymmetric rotating globular cluster
models which are fitted to spectroscopic data for NGC 362, NGC 1851, NGC 2808,
NGC 4372, NGC 5927 and NGC 6752 to provide constraints on their physical and
kinematic properties, including their rotation signals. They are constructed by
flattening Modified Plummer profiles, which have the same asymptotic behaviour
as classical Plummer models, but can provide better fits to young clusters due
to a slower turnover in the density profile. The models are in dynamical
equilibrium as they depend solely on the action variables. We employ a fully
Bayesian scheme to investigate the uncertainty in our model parameters
(including mass-to-light ratios and inclination angles) and evaluate the
Bayesian evidence ratio for rotating to non-rotating models. We find convincing
levels of rotation only in NGC 2808. In the other clusters, there is just a
hint of rotation (in particular, NGC 4372 and NGC 5927), as the data quality
does not allow us to draw strong conclusions. Where rotation is present, we
find that it is confined to the central regions, within radii of $R \leq 2
r_h$. As part of this work, we have developed a novel q-Gaussian basis
expansion of the line-of-sight velocity distributions, from which general
models can be constructed via interpolation on the basis coefficients.
"
3201,"  For high-dimensional sparse linear models, how to construct confidence
intervals for coefficients remains a difficult question. The main reason is the
complicated limiting distributions of common estimators such as the Lasso.
Several confidence interval construction methods have been developed, and
Bootstrap Lasso+OLS is notable for its simple technicality, good
interpretability, and comparable performance with other more complicated
methods. However, Bootstrap Lasso+OLS depends on the beta-min assumption, a
theoretic criterion that is often violated in practice. In this paper, we
introduce a new method called Bootstrap Lasso+Partial Ridge (LPR) to relax this
assumption. LPR is a two-stage estimator: first using Lasso to select features
and subsequently using Partial Ridge to refit the coefficients. Simulation
results show that Bootstrap LPR outperforms Bootstrap Lasso+OLS when there
exist small but non-zero coefficients, a common situation violating the
beta-min assumption. For such coefficients, compared to Bootstrap Lasso+OLS,
confidence intervals constructed by Bootstrap LPR have on average 50% larger
coverage probabilities. Bootstrap LPR also has on average 35% shorter
confidence interval lengths than the de-sparsified Lasso methods, regardless of
whether linear models are misspecified. Additionally, we provide theoretical
guarantees of Bootstrap LPR under appropriate conditions and implement it in
the R package ""HDCI.""
"
4582,"  We study the adjoint of the double layer potential associated with the
Laplacian (the adjoint of the Neumann-Poincaré operator), as a map on the
boundary surface $\Gamma$ of a domain in $\mathbb{R}^3$ with conical points.
The spectrum of this operator directly reflects the well-posedness of related
transmission problems across $\Gamma$. In particular, if the domain is
understood as an inclusion with complex permittivity $\epsilon$, embedded in a
background medium with unit permittivity, then the polarizability tensor of the
domain is well-defined when $(\epsilon+1)/(\epsilon-1)$ belongs to the
resolvent set in energy norm. We study surfaces $\Gamma$ that have a finite
number of conical points featuring rotational symmetry. On the energy space, we
show that the essential spectrum consists of an interval. On $L^2(\Gamma)$,
i.e. for square-integrable boundary data, we show that the essential spectrum
consists of a countable union of curves, outside of which the Fredholm index
can be computed as a winding number with respect to the essential spectrum. We
provide explicit formulas, depending on the opening angles of the conical
points. We reinforce our study with very precise numerical experiments,
computing the energy space spectrum and the spectral measures of the
polarizability tensor in two different examples. Our results indicate that the
densities of the spectral measures may approach zero extremely rapidly in the
continuous part of the energy space spectrum.
"
17804,"  We show how an ensemble of $Q^*$-functions can be leveraged for more
effective exploration in deep reinforcement learning. We build on well
established algorithms from the bandit setting, and adapt them to the
$Q$-learning setting. We propose an exploration strategy based on
upper-confidence bounds (UCB). Our experiments show significant gains on the
Atari benchmark.
"
4246,"  The recognition of actions from video sequences has many applications in
health monitoring, assisted living, surveillance, and smart homes. Despite
advances in sensing, in particular related to 3D video, the methodologies to
process the data are still subject to research. We demonstrate superior results
by a system which combines recurrent neural networks with convolutional neural
networks in a voting approach. The gated-recurrent-unit-based neural networks
are particularly well-suited to distinguish actions based on long-term
information from optical tracking data; the 3D-CNNs focus more on detailed,
recent information from video data. The resulting features are merged in an SVM
which then classifies the movement. In this architecture, our method improves
recognition rates of state-of-the-art methods by 14% on standard data sets.
"
2797,"  Leakage of polarized Galactic diffuse emission into total intensity can
potentially mimic the 21-cm signal coming from the epoch of reionization (EoR),
as both of them might have fluctuating spectral structure. Although we are
sensitive to the EoR signal only in small fields of view, chromatic sidelobes
from further away can contaminate the inner region. Here, we explore the
effects of leakage into the 'EoR window' of the cylindrically averaged power
spectra (PS) within wide fields of view using both observation and simulation
of the 3C196 and NCP fields, two observing fields of the LOFAR-EoR project. We
present the polarization PS of two one-night observations of the two fields and
find that the NCP field has higher fluctuations along frequency, and
consequently exhibits more power at high-$k_\parallel$ that could potentially
leak to Stokes $I$. Subsequently, we simulate LOFAR observations of Galactic
diffuse polarized emission based on a model to assess what fraction of
polarized power leaks into Stokes $I$ because of the primary beam. We find that
the rms fractional leakage over the instrumental $k$-space is $0.35\%$ in the
3C196 field and $0.27\%$ in the NCP field, and it does not change significantly
within the diameters of $15^\circ$, $9^\circ$ and $4^\circ$. Based on the
observed PS and simulated fractional leakage, we show that a similar level of
leakage into Stokes $I$ is expected in the 3C196 and NCP fields, and the
leakage can be considered to be a bias in the PS.
"
2048,"  Linear regression models contaminated by Gaussian noise (inlier) and possibly
unbounded sparse outliers are common in many signal processing applications.
Sparse recovery inspired robust regression (SRIRR) techniques are shown to
deliver high quality estimation performance in such regression models.
Unfortunately, most SRIRR techniques assume \textit{a priori} knowledge of
noise statistics like inlier noise variance or outlier statistics like number
of outliers. Both inlier and outlier noise statistics are rarely known
\textit{a priori} and this limits the efficient operation of many SRIRR
algorithms. This article proposes a novel noise statistics oblivious algorithm
called residual ratio thresholding GARD (RRT-GARD) for robust regression in the
presence of sparse outliers. RRT-GARD is developed by modifying the recently
proposed noise statistics dependent greedy algorithm for robust de-noising
(GARD). Both finite sample and asymptotic analytical results indicate that
RRT-GARD performs nearly similar to GARD with \textit{a priori} knowledge of
noise statistics. Numerical simulations in real and synthetic data sets also
point to the highly competitive performance of RRT-GARD.
"
1996,"  We consider learning a predictor which is non-discriminatory with respect to
a ""protected attribute"" according to the notion of ""equalized odds"" proposed by
Hardt et al. [2016]. We study the problem of learning such a non-discriminatory
predictor from a finite training set, both statistically and computationally.
We show that a post-hoc correction approach, as suggested by Hardt et al, can
be highly suboptimal, present a nearly-optimal statistical procedure, argue
that the associated computational problem is intractable, and suggest a second
moment relaxation of the non-discrimination definition for which learning is
tractable.
"
3290,"  We describe a parallel, adaptive, multi-block algorithm for explicit
integration of time dependent partial differential equations on two-dimensional
Cartesian grids. The grid layout we consider consists of a nested hierarchy of
fixed size, non-overlapping, logically Cartesian grids stored as leaves in a
quadtree. Dynamic grid refinement and parallel partitioning of the grids is
done through the use of the highly scalable quadtree/octree library p4est.
Because our concept is multi-block, we are able to easily solve on a variety of
geometries including the cubed sphere. In this paper, we pay special attention
to providing details of the parallel ghost-filling algorithm needed to ensure
that both corner and edge ghost regions around each grid hold valid values.
We have implemented this algorithm in the ForestClaw code using single-grid
solvers from ClawPack, a software package for solving hyperbolic PDEs using
finite volumes methods. We show weak and strong scalability results for scalar
advection problems on two-dimensional manifold domains on 1 to 64Ki MPI
processes, demonstrating neglible regridding overhead.
"
20278,"  We explore the notion of the quantum auxiliary linear problem and the
associated problem of quantum Backlund transformations (BT). In this context we
systematically construct the analogue of the classical formula that provides
the whole hierarchy of the time components of Lax pairs at the quantum level
for both closed and open integrable lattice models. The generic time evolution
operator formula is particularly interesting and novel at the quantum level
when dealing with systems with open boundary conditions. In the same frame we
show that the reflection K-matrix can also be viewed as a particular type of
BT, fixed at the boundaries of the system. The q-oscillator (q-boson) model, a
variant of the Ablowitz-Ladik model, is then employed as a paradigm to
illustrate the method. Particular emphasis is given to the time part of the
quantum BT as possible connections and applications to the problem of quantum
quenches as well as the time evolution of local quantum impurities are evident.
A discussion on the use of Bethe states as well as coherent states for the
study of the time evolution is also presented.
"
11370,"  We study the effect of constant shifts on the zeros of rational harmomic
functions $f(z) = r(z) - \conj{z}$. In particular, we characterize how shifting
through the caustics of $f$ changes the number of zeros and their respective
orientations. This also yields insight into the nature of the singular zeros of
$f$. Our results have applications in gravitational lensing theory, where
certain such functions $f$ represent gravitational point-mass lenses, and a
constant shift can be interpreted as the position of the light source of the
lens.
"
9206,"  We calculate the scrambling rate $\lambda_L$ and the butterfly velocity $v_B$
associated with the growth of quantum chaos for a solvable large-$N$
electron-phonon system. We study a temperature regime in which the electrical
resistivity of this system exceeds the Mott-Ioffe-Regel limit and increases
linearly with temperature - a sign that there are no long-lived charged
quasiparticles - although the phonons remain well-defined quasiparticles. The
long-lived phonons determine $\lambda_L$, rendering it parametrically smaller
than the theoretical upper-bound $\lambda_L \ll \lambda_{max}=2\pi T/\hbar$.
Significantly, the chaos properties seem to be intrinsic - $\lambda_L$ and
$v_B$ are the same for electronic and phononic operators. We consider two
models - one in which the phonons are dispersive, and one in which they are
dispersionless. In either case, we find that $\lambda_L$ is proportional to the
inverse phonon lifetime, and $v_B$ is proportional to the effective phonon
velocity. The thermal and chaos diffusion constants, $D_E$ and $D_L\equiv
v_B^2/\lambda_L$, are always comparable, $D_E \sim D_L$. In the dispersive
phonon case, the charge diffusion constant $D_C$ satisfies $D_L\gg D_C$, while
in the dispersionless case $D_L \ll D_C$.
"
7487,"  HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
"
13718,"  We discuss various characterizations of synthetic upper Ricci bounds for
metric measure spaces in terms of heat flow, entropy and optimal transport. In
particular, we present a characterization in terms of semiconcavity of the
entropy along certain Wasserstein geodesics which is stable under convergence
of mm-spaces. And we prove that a related characterization is equivalent to an
asymptotic lower bound on the growth of the Wasseretein distance between heat
flows. For weighted Riemannian manifolds, the crucial result will be a precise
uniform two-sided bound for \begin{eqnarray*}\frac{d}{dt}\Big|_{t=0}W\big(\hat
P_t\delta_x,\hat P_t\delta_y\big)\end{eqnarray*} in terms of the mean value of
the Bakry-Emery Ricci tensor $\mathrm{Ric}+\mathrm{Hess}\, f$ along the
minimizing geodesic from $x$ to $y$ and an explicit correction term depending
on the bound for the curvature along this curve.
"
3643,"  The ambitious goals of precision cosmology with wide-field optical surveys
such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope
(LSST) demand, as their foundation, precision CCD astronomy. This in turn
requires an understanding of previously uncharacterized sources of systematic
error in CCD sensors, many of which manifest themselves as static effective
variations in pixel area. Such variation renders a critical assumption behind
the traditional procedure of flat fielding--that a sensor's pixels comprise a
uniform grid--invalid. In this work, we present a method to infer a curl-free
model of a sensor's underlying pixel grid from flat field images, incorporating
the superposition of all electrostatic sensor effects--both known and
unknown--present in flat field data. We use these pixel grid models to estimate
the overall impact of sensor systematics on photometry, astrometry, and PSF
shape measurements in a representative sensor from the Dark Energy Camera
(DECam) and a prototype LSST sensor. Applying the method to DECam data recovers
known significant sensor effects for which corrections are currently being
developed within DES. For an LSST prototype CCD with pixel-response
non-uniformity (PRNU) of 0.4%, we find the impact of ""improper"" flat-fielding
on these observables is negligible in nominal .7"" seeing conditions. These
errors scale linearly with the PRNU, so for future LSST production sensors,
which may have larger PRNU, our method provides a way to assess whether
pixel-level calibration beyond flat fielding will be required.
"
2680,"  A new prior is proposed for representation learning, which can be combined
with other priors in order to help disentangling abstract factors from each
other. It is inspired by the phenomenon of consciousness seen as the formation
of a low-dimensional combination of a few concepts constituting a conscious
thought, i.e., consciousness as awareness at a particular time instant. This
provides a powerful constraint on the representation in that such
low-dimensional thought vectors can correspond to statements about reality
which are true, highly probable, or very useful for taking decisions. The fact
that a few elements of the current state can be combined into such a predictive
or useful statement is a strong constraint and deviates considerably from the
maximum likelihood approaches to modelling data and how states unfold in the
future based on an agent's actions. Instead of making predictions in the
sensory (e.g. pixel) space, the consciousness prior allows the agent to make
predictions in the abstract space, with only a few dimensions of that space
being involved in each of these predictions. The consciousness prior also makes
it natural to map conscious states to natural language utterances or to express
classical AI knowledge in the form of facts and rules, although the conscious
states may be richer than what can be expressed easily in the form of a
sentence, a fact or a rule.
"
20683,"  In this work, we present a parallel, fully-distributed finite element
numerical framework to simulate the low-frequency electromagnetic response of
superconducting devices, which allows to efficiently exploit HPC platforms. We
select the so-called H-formulation, which uses the magnetic field as a state
variable. Nédélec elements (of arbitrary order) are required for an
accurate approximation of the H-formulation for modelling electromagnetic
fields along interfaces between regions with high contrast medium properties.
An h-adaptive mesh refinement technique customized for Nédélec elements
leads to a structured fine mesh in areas of interest whereas a smart coarsening
is obtained in other regions. The composition of a tailored, robust, parallel
nonlinear solver completes the exposition of the developed tools to tackle the
problem. First, a comparison against experimental data is performed to show the
availability of the finite element approximation to model the physical
phenomena. Then, a selected state-of-the-art 3D benchmark is reproduced,
focusing on the parallel performance of the algorithms.
"
1921,"  In the framework of matrix valued observables with low rank means, Stein's
unbiased risk estimate (SURE) can be useful for risk estimation and for tuning
the amount of shrinkage towards low rank matrices. This was demonstrated by
Candès et al. (2013) for singular value soft thresholding, which is a
Lipschitz continuous estimator. SURE provides an unbiased risk estimate for an
estimator whenever the differentiability requirements for Stein's lemma are
satisfied. Lipschitz continuity of the estimator is sufficient, but it is
emphasized that differentiability Lebesgue almost everywhere isn't. The reduced
rank estimator, which gives the best approximation of the observation with a
fixed rank, is an example of a discontinuous estimator for which Stein's lemma
actually applies. This was observed by Mukherjee et al. (2015), but the proof
was incomplete. This brief note gives a sufficient condition for Stein's lemma
to hold for estimators with discontinuities, which is then shown to be
fulfilled for a class of spectral function estimators including the reduced
rank estimator. Singular value hard thresholding does, however, not satisfy the
condition, and Stein's lemma does not apply to this estimator.
"
19210,"  We propose a new active learning strategy designed for deep neural networks.
The goal is to minimize the number of data annotation queried from an oracle
during training. Previous active learning strategies scalable for deep networks
were mostly based on uncertain sample selection. In this work, we focus on
examples lying close to the decision boundary. Based on theoretical works on
margin theory for active learning, we know that such examples may help to
considerably decrease the number of annotations. While measuring the exact
distance to the decision boundaries is intractable, we propose to rely on
adversarial examples. We do not consider anymore them as a threat instead we
exploit the information they provide on the distribution of the input space in
order to approximate the distance to decision boundaries. We demonstrate
empirically that adversarial active queries yield faster convergence of CNNs
trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.
"
7362,"  For over twenty years, the term 'cosmic web' has guided our understanding of
the large-scale arrangement of matter in the cosmos, accurately evoking the
concept of a network of galaxies linked by filaments. But the physical
correspondence between the cosmic web and structural-engineering or textile
'spiderwebs' is even deeper than previously known, and extends to origami
tessellations as well. Here we explain that in a good structure-formation
approximation known as the adhesion model, threads of the cosmic web form a
spiderweb, i.e. can be strung up to be entirely in tension. The correspondence
is exact if nodes sampling voids are included, and if structure is excluded
within collapsed regions (walls, filaments and haloes), where dark-matter
multistreaming and baryonic physics affect the structure. We also suggest how
concepts arising from this link might be used to test cosmological models: for
example, to test for large-scale anisotropy and rotational flows in the cosmos.
"
16845,"  Although proportional hazard rate model is a very popular model to analyze
failure time data, sometimes it becomes important to study the additive hazard
rate model. Again, sometimes the concept of the hazard rate function is
abstract, in comparison to the concept of mean residual life function. A new
model called `dynamic additive mean residual life model' where the covariates
are time-dependent has been defined in the literature. Here we study the
closure properties of the model for different positive and negative ageing
classes under certain condition(s). Quite a few examples are presented to
illustrate different properties of the model.
"
2932,"  When participating in electricity markets, owners of battery energy storage
systems must bid in such a way that their revenues will at least cover their
true cost of operation. Since cycle aging of battery cells represents a
substantial part of this operating cost, the cost of battery degradation must
be factored in these bids. However, existing models of battery degradation
either do not fit market clearing software or do not reflect the actual battery
aging mechanism. In this paper we model battery cycle aging using a piecewise
linear cost function, an approach that provides a close approximation of the
cycle aging mechanism of electrochemical batteries and can be incorporated
easily into existing market dispatch programs. By defining the marginal aging
cost of each battery cycle, we can assess the actual operating profitability of
batteries. A case study demonstrates the effectiveness of the proposed model in
maximizing the operating profit of a battery energy storage system taking part
in the ISO New England energy and reserve markets.
"
13396,"  Stochastic Gradient Descent (SGD) is widely used in machine learning problems
to efficiently perform empirical risk minimization, yet, in practice, SGD is
known to stall before reaching the actual minimizer of the empirical risk. SGD
stalling has often been attributed to its sensitivity to the conditioning of
the problem; however, as we demonstrate, SGD will stall even when applied to a
simple linear regression problem with unity condition number for standard
learning rates. Thus, in this work, we numerically demonstrate and
mathematically argue that stalling is a crippling and generic limitation of SGD
and its variants in practice. Once we have established the problem of stalling,
we generalize an existing framework for hedging against its effects, which (1)
deters SGD and its variants from stalling, (2) still provides convergence
guarantees, and (3) makes SGD and its variants more practical methods for
minimization.
"
7607,"  Given a 2-crossing minimal chart $\Gamma$, a minimal chart with two
crossings, set $\alpha=\min\{~i~|~$there exists an edge of label $i$ containing
a white vertex$\}$, and $\beta=\max\{~i~|~$there exists an edge of label $i$
containing a white vertex$\}$. In this paper we study the structure of a
neighbourhood of $\Gamma_\alpha\cup\Gamma_\beta$, and propose a normal form for
2-crossing minimal $n$-charts, here $\Gamma_\alpha$ and $\Gamma_\beta$ mean the
union of all the edges of label $\alpha$ and $\beta$ respectively.
"
20544,"  The recovery of approximately sparse or compressible coefficients in a
Polynomial Chaos Expansion is a common goal in modern parametric uncertainty
quantification (UQ). However, relatively little effort in UQ has been directed
toward theoretical and computational strategies for addressing the sparse
corruptions problem, where a small number of measurements are highly corrupted.
Such a situation has become pertinent today since modern computational
frameworks are sufficiently complex with many interdependent components that
may introduce hardware and software failures, some of which can be difficult to
detect and result in a highly polluted simulation result.
In this paper we present a novel compressive sampling-based theoretical
analysis for a regularized $\ell^1$ minimization algorithm that aims to recover
sparse expansion coefficients in the presence of measurement corruptions. Our
recovery results are uniform, and prescribe algorithmic regularization
parameters in terms of a user-defined a priori estimate on the ratio of
measurements that are believed to be corrupted. We also propose an iteratively
reweighted optimization algorithm that automatically refines the value of the
regularization parameter, and empirically produces superior results. Our
numerical results test our framework on several medium-to-high dimensional
examples of solutions to parameterized differential equations, and demonstrate
the effectiveness of our approach.
"
7685,"  One of the most challenging tasks for a flying robot is to autonomously
navigate between target locations quickly and reliably while avoiding obstacles
in its path, and with little to no a-priori knowledge of the operating
environment. This challenge is addressed in the present paper. We describe the
system design and software architecture of our proposed solution, and showcase
how all the distinct components can be integrated to enable smooth robot
operation. We provide critical insight on hardware and software component
selection and development, and present results from extensive experimental
testing in real-world warehouse environments. Experimental testing reveals that
our proposed solution can deliver fast and robust aerial robot autonomous
navigation in cluttered, GPS-denied environments.
"
8199,"  We present a chemical abundance analysis of the tidally disrupted globular
cluster (GC) Palomar 5. By co-adding high-resolution spectra of 15 member stars
from the cluster's main body, taken at low signal-to-noise with the Keck/HIRES
spectrograph, we were able to measure integrated abundance ratios of 24 species
of 20 elements including all major nucleosynthetic channels (namely the light
element Na; $\alpha$-elements Mg, Si, Ca, Ti; Fe-peak and heavy elements Sc, V,
Cr, Mn, Co, Ni, Cu, Zn; and the neutron-capture elements Y, Zr, Ba, La, Nd, Sm,
Eu). The mean metallicity of $-1.56\pm0.02\pm0.06$ dex (statistical and
systematic errors) agrees well with the values from individual, low-resolution
measurements of individual stars, but it is lower than previous high-resolution
results of a small number of stars in the literature. Comparison with Galactic
halo stars and other disrupted and unperturbed GCs renders Pal~5 a typical
representative of the Milky Way halo population, as has been noted before,
emphasizing that the early chemical evolution of such clusters is decoupled
from their later dynamical history. We also performed a test as to the
detectability of light element variations in this co-added abundance analysis
technique and found that this approach is not sensitive even in the presence of
a broad range in sodium of $\sim$0.6 dex, a value typically found in the old
halo GCs. Thus, while methods of determining the global abundance patterns of
such objects are well suited to study their overall enrichment histories,
chemical distinctions of their multiple stellar populations is still best
obtained from measurements of individual stars.
"
2955,"  We present a deep learning approach to the ISIC 2017 Skin Lesion
Classification Challenge using a multi-scale convolutional neural network. Our
approach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,
which is fine-tuned for skin lesion classification using two different scales
of input images.
"
7832,"  Predicting the ground state of alloy systems is challenging due to the large
number of possible configurations. We identify an easily computed descriptor
for the stability of binary surface alloys, the effective coordination number
$\mathscr{E}$. We show that $\mathscr{E}(M)$ correlates well with the enthalpy
of mixing, from density functional theory (DFT) calculations on
$M_x$Au$_{1-x}$/Ru [$M$ = Mn or Fe]. At each $x$, the most favored structure
has the highest [lowest] value of $\mathscr{E}(M)$ if the system is
non-magnetic [ferromagnetic]. Importantly, little accuracy is lost upon
replacing $\mathscr{E}(M)$ by $\mathscr{E}^*(M)$, which can be quickly computed
without performing a DFT calculation, possibly offering a simple alternative to
the frequently used cluster expansion method.
"
17517,"  Tests of gravity at the galaxy scale are in their infancy. As a first step to
systematically uncovering the gravitational significance of galaxies, we map
three fundamental gravitational variables -- the Newtonian potential,
acceleration and curvature -- over the galaxy environments of the local
universe to a distance of approximately 200 Mpc. Our method combines the
contributions from galaxies in an all-sky redshift survey, halos from an N-body
simulation hosting low-luminosity objects, and linear and quasi-linear modes of
the density field. We use the ranges of these variables to determine the extent
to which galaxies expand the scope of generic tests of gravity and are capable
of constraining specific classes of model for which they have special
significance. Finally, we investigate the improvements afforded by upcoming
galaxy surveys.
"
13126,"  This work is concerned with Al/Al-oxide(AlO$_{x}$)/Al-layer systems which are
important for Josephson-junction-based superconducting devices such as quantum
bits. The device performance is limited by noise, which has been to a large
degree assigned to the presence and properties of two-level tunneling systems
in the amorphous AlO$_{x}$ tunnel barrier. The study is focused on the
correlation of the fabrication conditions, nanostructural and nanochemical
properties and the occurrence of two-level tunneling systems with particular
emphasis on the AlO$_{x}$-layer. Electron-beam evaporation with two different
processes and sputter deposition were used for structure fabrication, and the
effect of illumination by ultraviolet light during Al-oxide formation is
elucidated. Characterization was performed by analytical transmission electron
microscopy and low-temperature dielectric measurements. We show that the
fabrication conditions have a strong impact on the nanostructural and
nanochemical properties of the layer systems and the properties of two-level
tunneling systems. Based on the understanding of the observed structural
characteristics, routes are derived towards the fabrication of
Al/AlO$_{x}$/Al-layers systems with improved properties.
"
